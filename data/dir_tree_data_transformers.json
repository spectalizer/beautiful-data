{
  "data": {
    "directed": true,
    "multigraph": false,
    "graph": {},
    "nodes": [
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 46465.739,
        "id": "src"
      },
      {
        "type": "dir",
        "size": 32768,
        "file_type": "dir",
        "inclusive_size": 46465.739,
        "id": "src\\transformers"
      },
      {
        "type": "file",
        "size": 13.245,
        "file_type": "py",
        "inclusive_size": 13.245,
        "id": "src\\transformers\\activations.py"
      },
      {
        "type": "file",
        "size": 54.547,
        "file_type": "py",
        "inclusive_size": 54.547,
        "id": "src\\transformers\\audio_utils.py"
      },
      {
        "type": "file",
        "size": 60.432,
        "file_type": "py",
        "inclusive_size": 60.432,
        "id": "src\\transformers\\cache_utils.py"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 169.25300000000001,
        "id": "src\\transformers\\cli"
      },
      {
        "type": "file",
        "size": 57.602,
        "file_type": "py",
        "inclusive_size": 57.602,
        "id": "src\\transformers\\configuration_utils.py"
      },
      {
        "type": "file",
        "size": 15.713,
        "file_type": "py",
        "inclusive_size": 15.713,
        "id": "src\\transformers\\conversion_mapping.py"
      },
      {
        "type": "file",
        "size": 75.372,
        "file_type": "py",
        "inclusive_size": 75.372,
        "id": "src\\transformers\\convert_slow_tokenizer.py"
      },
      {
        "type": "file",
        "size": 5.79,
        "file_type": "py",
        "inclusive_size": 5.79,
        "id": "src\\transformers\\convert_slow_tokenizers_checkpoints_to_fast.py"
      },
      {
        "type": "file",
        "size": 56.368,
        "file_type": "py",
        "inclusive_size": 56.368,
        "id": "src\\transformers\\core_model_loading.py"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 188.03199999999998,
        "id": "src\\transformers\\data"
      },
      {
        "type": "file",
        "size": 12.891,
        "file_type": "py",
        "inclusive_size": 12.891,
        "id": "src\\transformers\\debug_utils.py"
      },
      {
        "type": "file",
        "size": 2.099,
        "file_type": "py",
        "inclusive_size": 2.099,
        "id": "src\\transformers\\dependency_versions_check.py"
      },
      {
        "type": "file",
        "size": 3.324,
        "file_type": "py",
        "inclusive_size": 3.324,
        "id": "src\\transformers\\dependency_versions_table.py"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 5.364,
        "id": "src\\transformers\\distributed"
      },
      {
        "type": "file",
        "size": 34.746,
        "file_type": "py",
        "inclusive_size": 34.746,
        "id": "src\\transformers\\dynamic_module_utils.py"
      },
      {
        "type": "file",
        "size": 18.846,
        "file_type": "py",
        "inclusive_size": 18.846,
        "id": "src\\transformers\\feature_extraction_sequence_utils.py"
      },
      {
        "type": "file",
        "size": 29.967,
        "file_type": "py",
        "inclusive_size": 29.967,
        "id": "src\\transformers\\feature_extraction_utils.py"
      },
      {
        "type": "file",
        "size": 2.958,
        "file_type": "py",
        "inclusive_size": 2.958,
        "id": "src\\transformers\\file_utils.py"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 749.659,
        "id": "src\\transformers\\generation"
      },
      {
        "type": "file",
        "size": 19.734,
        "file_type": "py",
        "inclusive_size": 19.734,
        "id": "src\\transformers\\hf_argparser.py"
      },
      {
        "type": "file",
        "size": 3.684,
        "file_type": "py",
        "inclusive_size": 3.684,
        "id": "src\\transformers\\hyperparameter_search.py"
      },
      {
        "type": "file",
        "size": 22.57,
        "file_type": "py",
        "inclusive_size": 22.57,
        "id": "src\\transformers\\image_processing_base.py"
      },
      {
        "type": "file",
        "size": 13.746,
        "file_type": "py",
        "inclusive_size": 13.746,
        "id": "src\\transformers\\image_processing_utils.py"
      },
      {
        "type": "file",
        "size": 38.0,
        "file_type": "py",
        "inclusive_size": 38.0,
        "id": "src\\transformers\\image_processing_utils_fast.py"
      },
      {
        "type": "file",
        "size": 43.982,
        "file_type": "py",
        "inclusive_size": 43.982,
        "id": "src\\transformers\\image_transforms.py"
      },
      {
        "type": "file",
        "size": 37.242,
        "file_type": "py",
        "inclusive_size": 37.242,
        "id": "src\\transformers\\image_utils.py"
      },
      {
        "type": "file",
        "size": 9.858,
        "file_type": "py",
        "inclusive_size": 9.858,
        "id": "src\\transformers\\initialization.py"
      },
      {
        "type": "dir",
        "size": 8192,
        "file_type": "dir",
        "inclusive_size": 624.441,
        "id": "src\\transformers\\integrations"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 103.406,
        "id": "src\\transformers\\loss"
      },
      {
        "type": "file",
        "size": 72.814,
        "file_type": "py",
        "inclusive_size": 72.814,
        "id": "src\\transformers\\masking_utils.py"
      },
      {
        "type": "file",
        "size": 22.372,
        "file_type": "py",
        "inclusive_size": 22.372,
        "id": "src\\transformers\\modelcard.py"
      },
      {
        "type": "file",
        "size": 21.228,
        "file_type": "py",
        "inclusive_size": 21.228,
        "id": "src\\transformers\\modeling_attn_mask_utils.py"
      },
      {
        "type": "file",
        "size": 31.632,
        "file_type": "py",
        "inclusive_size": 31.632,
        "id": "src\\transformers\\modeling_flash_attention_utils.py"
      },
      {
        "type": "file",
        "size": 24.799,
        "file_type": "py",
        "inclusive_size": 24.799,
        "id": "src\\transformers\\modeling_gguf_pytorch_utils.py"
      },
      {
        "type": "file",
        "size": 11.475,
        "file_type": "py",
        "inclusive_size": 11.475,
        "id": "src\\transformers\\modeling_layers.py"
      },
      {
        "type": "file",
        "size": 108.678,
        "file_type": "py",
        "inclusive_size": 108.678,
        "id": "src\\transformers\\modeling_outputs.py"
      },
      {
        "type": "file",
        "size": 52.206,
        "file_type": "py",
        "inclusive_size": 52.206,
        "id": "src\\transformers\\modeling_rope_utils.py"
      },
      {
        "type": "file",
        "size": 244.329,
        "file_type": "py",
        "inclusive_size": 244.329,
        "id": "src\\transformers\\modeling_utils.py"
      },
      {
        "type": "dir",
        "size": 98304,
        "file_type": "dir",
        "inclusive_size": 40685.674,
        "id": "src\\transformers\\models"
      },
      {
        "type": "file",
        "size": 17.044,
        "file_type": "py",
        "inclusive_size": 17.044,
        "id": "src\\transformers\\model_debugging_utils.py"
      },
      {
        "type": "file",
        "size": 39.891,
        "file_type": "py",
        "inclusive_size": 39.891,
        "id": "src\\transformers\\optimization.py"
      },
      {
        "type": "dir",
        "size": 12288,
        "file_type": "dir",
        "inclusive_size": 527.226,
        "id": "src\\transformers\\pipelines"
      },
      {
        "type": "file",
        "size": 95.547,
        "file_type": "py",
        "inclusive_size": 95.547,
        "id": "src\\transformers\\processing_utils.py"
      },
      {
        "type": "file",
        "size": 0.0,
        "file_type": "typed",
        "inclusive_size": 0.0,
        "id": "src\\transformers\\py.typed"
      },
      {
        "type": "file",
        "size": 11.213,
        "file_type": "py",
        "inclusive_size": 11.213,
        "id": "src\\transformers\\pytorch_utils.py"
      },
      {
        "type": "dir",
        "size": 8192,
        "file_type": "dir",
        "inclusive_size": 146.919,
        "id": "src\\transformers\\quantizers"
      },
      {
        "type": "file",
        "size": 4.495,
        "file_type": "py",
        "inclusive_size": 4.495,
        "id": "src\\transformers\\safetensors_conversion.py"
      },
      {
        "type": "file",
        "size": 157.292,
        "file_type": "py",
        "inclusive_size": 157.292,
        "id": "src\\transformers\\testing_utils.py"
      },
      {
        "type": "file",
        "size": 7.48,
        "file_type": "py",
        "inclusive_size": 7.48,
        "id": "src\\transformers\\time_series_utils.py"
      },
      {
        "type": "file",
        "size": 78.703,
        "file_type": "py",
        "inclusive_size": 78.703,
        "id": "src\\transformers\\tokenization_mistral_common.py"
      },
      {
        "type": "file",
        "size": 60.725,
        "file_type": "py",
        "inclusive_size": 60.725,
        "id": "src\\transformers\\tokenization_python.py"
      },
      {
        "type": "file",
        "size": 173.392,
        "file_type": "py",
        "inclusive_size": 173.392,
        "id": "src\\transformers\\tokenization_utils_base.py"
      },
      {
        "type": "file",
        "size": 13.649,
        "file_type": "py",
        "inclusive_size": 13.649,
        "id": "src\\transformers\\tokenization_utils_sentencepiece.py"
      },
      {
        "type": "file",
        "size": 59.569,
        "file_type": "py",
        "inclusive_size": 59.569,
        "id": "src\\transformers\\tokenization_utils_tokenizers.py"
      },
      {
        "type": "file",
        "size": 258.688,
        "file_type": "py",
        "inclusive_size": 258.688,
        "id": "src\\transformers\\trainer.py"
      },
      {
        "type": "file",
        "size": 33.642,
        "file_type": "py",
        "inclusive_size": 33.642,
        "id": "src\\transformers\\trainer_callback.py"
      },
      {
        "type": "file",
        "size": 5.274,
        "file_type": "py",
        "inclusive_size": 5.274,
        "id": "src\\transformers\\trainer_jit_checkpoint.py"
      },
      {
        "type": "file",
        "size": 53.963,
        "file_type": "py",
        "inclusive_size": 53.963,
        "id": "src\\transformers\\trainer_pt_utils.py"
      },
      {
        "type": "file",
        "size": 18.138,
        "file_type": "py",
        "inclusive_size": 18.138,
        "id": "src\\transformers\\trainer_seq2seq.py"
      },
      {
        "type": "file",
        "size": 36.806,
        "file_type": "py",
        "inclusive_size": 36.806,
        "id": "src\\transformers\\trainer_utils.py"
      },
      {
        "type": "file",
        "size": 145.17,
        "file_type": "py",
        "inclusive_size": 145.17,
        "id": "src\\transformers\\training_args.py"
      },
      {
        "type": "file",
        "size": 3.847,
        "file_type": "py",
        "inclusive_size": 3.847,
        "id": "src\\transformers\\training_args_seq2seq.py"
      },
      {
        "type": "dir",
        "size": 12288,
        "file_type": "dir",
        "inclusive_size": 655.478,
        "id": "src\\transformers\\utils"
      },
      {
        "type": "file",
        "size": 40.925,
        "file_type": "py",
        "inclusive_size": 40.925,
        "id": "src\\transformers\\video_processing_utils.py"
      },
      {
        "type": "file",
        "size": 34.395,
        "file_type": "py",
        "inclusive_size": 34.395,
        "id": "src\\transformers\\video_utils.py"
      },
      {
        "type": "file",
        "size": 38.19,
        "file_type": "py",
        "inclusive_size": 38.19,
        "id": "src\\transformers\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.984,
        "file_type": "py",
        "inclusive_size": 9.984,
        "id": "src\\transformers\\utils\\attention_visualizer.py"
      },
      {
        "type": "file",
        "size": 105.585,
        "file_type": "py",
        "inclusive_size": 105.585,
        "id": "src\\transformers\\utils\\auto_docstring.py"
      },
      {
        "type": "file",
        "size": 17.646,
        "file_type": "py",
        "inclusive_size": 17.646,
        "id": "src\\transformers\\utils\\backbone_utils.py"
      },
      {
        "type": "file",
        "size": 11.277,
        "file_type": "py",
        "inclusive_size": 11.277,
        "id": "src\\transformers\\utils\\chat_parsing_utils.py"
      },
      {
        "type": "file",
        "size": 23.63,
        "file_type": "py",
        "inclusive_size": 23.63,
        "id": "src\\transformers\\utils\\chat_template_utils.py"
      },
      {
        "type": "file",
        "size": 0.282,
        "file_type": "py",
        "inclusive_size": 0.282,
        "id": "src\\transformers\\utils\\constants.py"
      },
      {
        "type": "file",
        "size": 8.031,
        "file_type": "py",
        "inclusive_size": 8.031,
        "id": "src\\transformers\\utils\\deprecation.py"
      },
      {
        "type": "file",
        "size": 37.221,
        "file_type": "py",
        "inclusive_size": 37.221,
        "id": "src\\transformers\\utils\\doc.py"
      },
      {
        "type": "file",
        "size": 0.34,
        "file_type": "py",
        "inclusive_size": 0.34,
        "id": "src\\transformers\\utils\\dummy_detectron2_objects.py"
      },
      {
        "type": "file",
        "size": 0.902,
        "file_type": "py",
        "inclusive_size": 0.902,
        "id": "src\\transformers\\utils\\dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py"
      },
      {
        "type": "file",
        "size": 0.309,
        "file_type": "py",
        "inclusive_size": 0.309,
        "id": "src\\transformers\\utils\\dummy_mistral_common_objects.py"
      },
      {
        "type": "file",
        "size": 0.458,
        "file_type": "py",
        "inclusive_size": 0.458,
        "id": "src\\transformers\\utils\\dummy_music_objects.py"
      },
      {
        "type": "file",
        "size": 12.894,
        "file_type": "py",
        "inclusive_size": 12.894,
        "id": "src\\transformers\\utils\\dummy_pt_objects.py"
      },
      {
        "type": "file",
        "size": 0.286,
        "file_type": "py",
        "inclusive_size": 0.286,
        "id": "src\\transformers\\utils\\dummy_sentencepiece_and_tokenizers_objects.py"
      },
      {
        "type": "file",
        "size": 0.465,
        "file_type": "py",
        "inclusive_size": 0.465,
        "id": "src\\transformers\\utils\\dummy_speech_objects.py"
      },
      {
        "type": "file",
        "size": 0.324,
        "file_type": "py",
        "inclusive_size": 0.324,
        "id": "src\\transformers\\utils\\dummy_timm_and_torchvision_objects.py"
      },
      {
        "type": "file",
        "size": 0.304,
        "file_type": "py",
        "inclusive_size": 0.304,
        "id": "src\\transformers\\utils\\dummy_tokenizers_objects.py"
      },
      {
        "type": "file",
        "size": 0.847,
        "file_type": "py",
        "inclusive_size": 0.847,
        "id": "src\\transformers\\utils\\dummy_torchaudio_objects.py"
      },
      {
        "type": "file",
        "size": 0.479,
        "file_type": "py",
        "inclusive_size": 0.479,
        "id": "src\\transformers\\utils\\dummy_torchvision_objects.py"
      },
      {
        "type": "file",
        "size": 0.63,
        "file_type": "py",
        "inclusive_size": 0.63,
        "id": "src\\transformers\\utils\\dummy_vision_objects.py"
      },
      {
        "type": "file",
        "size": 40.546,
        "file_type": "py",
        "inclusive_size": 40.546,
        "id": "src\\transformers\\utils\\generic.py"
      },
      {
        "type": "file",
        "size": 4.979,
        "file_type": "py",
        "inclusive_size": 4.979,
        "id": "src\\transformers\\utils\\hp_naming.py"
      },
      {
        "type": "file",
        "size": 38.955,
        "file_type": "py",
        "inclusive_size": 38.955,
        "id": "src\\transformers\\utils\\hub.py"
      },
      {
        "type": "file",
        "size": 105.419,
        "file_type": "py",
        "inclusive_size": 105.419,
        "id": "src\\transformers\\utils\\import_utils.py"
      },
      {
        "type": "file",
        "size": 10.288,
        "file_type": "py",
        "inclusive_size": 10.288,
        "id": "src\\transformers\\utils\\kernel_config.py"
      },
      {
        "type": "file",
        "size": 8.802,
        "file_type": "py",
        "inclusive_size": 8.802,
        "id": "src\\transformers\\utils\\loading_report.py"
      },
      {
        "type": "file",
        "size": 12.185,
        "file_type": "py",
        "inclusive_size": 12.185,
        "id": "src\\transformers\\utils\\logging.py"
      },
      {
        "type": "file",
        "size": 15.583,
        "file_type": "py",
        "inclusive_size": 15.583,
        "id": "src\\transformers\\utils\\metrics.py"
      },
      {
        "type": "file",
        "size": 15.711,
        "file_type": "py",
        "inclusive_size": 15.711,
        "id": "src\\transformers\\utils\\notebook.py"
      },
      {
        "type": "file",
        "size": 4.815,
        "file_type": "py",
        "inclusive_size": 4.815,
        "id": "src\\transformers\\utils\\peft_utils.py"
      },
      {
        "type": "file",
        "size": 3.571,
        "file_type": "py",
        "inclusive_size": 3.571,
        "id": "src\\transformers\\utils\\pytest_helpers.py"
      },
      {
        "type": "file",
        "size": 86.229,
        "file_type": "py",
        "inclusive_size": 86.229,
        "id": "src\\transformers\\utils\\quantization_config.py"
      },
      {
        "type": "file",
        "size": 50.663,
        "file_type": "py",
        "inclusive_size": 50.663,
        "id": "src\\transformers\\utils\\sentencepiece_model_pb2.py"
      },
      {
        "type": "file",
        "size": 6.598,
        "file_type": "py",
        "inclusive_size": 6.598,
        "id": "src\\transformers\\utils\\sentencepiece_model_pb2_new.py"
      },
      {
        "type": "file",
        "size": 5.526,
        "file_type": "py",
        "inclusive_size": 5.526,
        "id": "src\\transformers\\utils\\type_validators.py"
      },
      {
        "type": "file",
        "size": 4.306,
        "file_type": "py",
        "inclusive_size": 4.306,
        "id": "src\\transformers\\utils\\versions.py"
      },
      {
        "type": "file",
        "size": 9.408,
        "file_type": "py",
        "inclusive_size": 9.408,
        "id": "src\\transformers\\utils\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.788,
        "file_type": "py",
        "inclusive_size": 13.788,
        "id": "src\\transformers\\quantizers\\auto.py"
      },
      {
        "type": "file",
        "size": 12.727,
        "file_type": "py",
        "inclusive_size": 12.727,
        "id": "src\\transformers\\quantizers\\base.py"
      },
      {
        "type": "file",
        "size": 1.717,
        "file_type": "py",
        "inclusive_size": 1.717,
        "id": "src\\transformers\\quantizers\\quantizers_utils.py"
      },
      {
        "type": "file",
        "size": 2.567,
        "file_type": "py",
        "inclusive_size": 2.567,
        "id": "src\\transformers\\quantizers\\quantizer_aqlm.py"
      },
      {
        "type": "file",
        "size": 2.686,
        "file_type": "py",
        "inclusive_size": 2.686,
        "id": "src\\transformers\\quantizers\\quantizer_auto_round.py"
      },
      {
        "type": "file",
        "size": 3.68,
        "file_type": "py",
        "inclusive_size": 3.68,
        "id": "src\\transformers\\quantizers\\quantizer_awq.py"
      },
      {
        "type": "file",
        "size": 3.846,
        "file_type": "py",
        "inclusive_size": 3.846,
        "id": "src\\transformers\\quantizers\\quantizer_bitnet.py"
      },
      {
        "type": "file",
        "size": 7.087,
        "file_type": "py",
        "inclusive_size": 7.087,
        "id": "src\\transformers\\quantizers\\quantizer_bnb_4bit.py"
      },
      {
        "type": "file",
        "size": 6.751,
        "file_type": "py",
        "inclusive_size": 6.751,
        "id": "src\\transformers\\quantizers\\quantizer_bnb_8bit.py"
      },
      {
        "type": "file",
        "size": 4.781,
        "file_type": "py",
        "inclusive_size": 4.781,
        "id": "src\\transformers\\quantizers\\quantizer_compressed_tensors.py"
      },
      {
        "type": "file",
        "size": 3.841,
        "file_type": "py",
        "inclusive_size": 3.841,
        "id": "src\\transformers\\quantizers\\quantizer_eetq.py"
      },
      {
        "type": "file",
        "size": 8.537,
        "file_type": "py",
        "inclusive_size": 8.537,
        "id": "src\\transformers\\quantizers\\quantizer_fbgemm_fp8.py"
      },
      {
        "type": "file",
        "size": 6.829,
        "file_type": "py",
        "inclusive_size": 6.829,
        "id": "src\\transformers\\quantizers\\quantizer_finegrained_fp8.py"
      },
      {
        "type": "file",
        "size": 6.197,
        "file_type": "py",
        "inclusive_size": 6.197,
        "id": "src\\transformers\\quantizers\\quantizer_fp_quant.py"
      },
      {
        "type": "file",
        "size": 4.334,
        "file_type": "py",
        "inclusive_size": 4.334,
        "id": "src\\transformers\\quantizers\\quantizer_gptq.py"
      },
      {
        "type": "file",
        "size": 7.39,
        "file_type": "py",
        "inclusive_size": 7.39,
        "id": "src\\transformers\\quantizers\\quantizer_higgs.py"
      },
      {
        "type": "file",
        "size": 11.059,
        "file_type": "py",
        "inclusive_size": 11.059,
        "id": "src\\transformers\\quantizers\\quantizer_hqq.py"
      },
      {
        "type": "file",
        "size": 11.772,
        "file_type": "py",
        "inclusive_size": 11.772,
        "id": "src\\transformers\\quantizers\\quantizer_mxfp4.py"
      },
      {
        "type": "file",
        "size": 4.761,
        "file_type": "py",
        "inclusive_size": 4.761,
        "id": "src\\transformers\\quantizers\\quantizer_quanto.py"
      },
      {
        "type": "file",
        "size": 4.519,
        "file_type": "py",
        "inclusive_size": 4.519,
        "id": "src\\transformers\\quantizers\\quantizer_quark.py"
      },
      {
        "type": "file",
        "size": 2.688,
        "file_type": "py",
        "inclusive_size": 2.688,
        "id": "src\\transformers\\quantizers\\quantizer_spqr.py"
      },
      {
        "type": "file",
        "size": 12.146,
        "file_type": "py",
        "inclusive_size": 12.146,
        "id": "src\\transformers\\quantizers\\quantizer_torchao.py"
      },
      {
        "type": "file",
        "size": 2.416,
        "file_type": "py",
        "inclusive_size": 2.416,
        "id": "src\\transformers\\quantizers\\quantizer_vptq.py"
      },
      {
        "type": "file",
        "size": 0.8,
        "file_type": "py",
        "inclusive_size": 0.8,
        "id": "src\\transformers\\quantizers\\__init__.py"
      },
      {
        "type": "file",
        "size": 24.68,
        "file_type": "py",
        "inclusive_size": 24.68,
        "id": "src\\transformers\\pipelines\\any_to_any.py"
      },
      {
        "type": "file",
        "size": 11.074,
        "file_type": "py",
        "inclusive_size": 11.074,
        "id": "src\\transformers\\pipelines\\audio_classification.py"
      },
      {
        "type": "file",
        "size": 12.191,
        "file_type": "py",
        "inclusive_size": 12.191,
        "id": "src\\transformers\\pipelines\\audio_utils.py"
      },
      {
        "type": "file",
        "size": 33.635,
        "file_type": "py",
        "inclusive_size": 33.635,
        "id": "src\\transformers\\pipelines\\automatic_speech_recognition.py"
      },
      {
        "type": "file",
        "size": 59.155,
        "file_type": "py",
        "inclusive_size": 59.155,
        "id": "src\\transformers\\pipelines\\base.py"
      },
      {
        "type": "file",
        "size": 6.115,
        "file_type": "py",
        "inclusive_size": 6.115,
        "id": "src\\transformers\\pipelines\\depth_estimation.py"
      },
      {
        "type": "file",
        "size": 25.269,
        "file_type": "py",
        "inclusive_size": 25.269,
        "id": "src\\transformers\\pipelines\\document_question_answering.py"
      },
      {
        "type": "file",
        "size": 3.406,
        "file_type": "py",
        "inclusive_size": 3.406,
        "id": "src\\transformers\\pipelines\\feature_extraction.py"
      },
      {
        "type": "file",
        "size": 11.064,
        "file_type": "py",
        "inclusive_size": 11.064,
        "id": "src\\transformers\\pipelines\\fill_mask.py"
      },
      {
        "type": "file",
        "size": 9.88,
        "file_type": "py",
        "inclusive_size": 9.88,
        "id": "src\\transformers\\pipelines\\image_classification.py"
      },
      {
        "type": "file",
        "size": 4.794,
        "file_type": "py",
        "inclusive_size": 4.794,
        "id": "src\\transformers\\pipelines\\image_feature_extraction.py"
      },
      {
        "type": "file",
        "size": 9.748,
        "file_type": "py",
        "inclusive_size": 9.748,
        "id": "src\\transformers\\pipelines\\image_segmentation.py"
      },
      {
        "type": "file",
        "size": 21.331,
        "file_type": "py",
        "inclusive_size": 21.331,
        "id": "src\\transformers\\pipelines\\image_text_to_text.py"
      },
      {
        "type": "file",
        "size": 5.261,
        "file_type": "py",
        "inclusive_size": 5.261,
        "id": "src\\transformers\\pipelines\\image_to_image.py"
      },
      {
        "type": "file",
        "size": 7.056,
        "file_type": "py",
        "inclusive_size": 7.056,
        "id": "src\\transformers\\pipelines\\keypoint_matching.py"
      },
      {
        "type": "file",
        "size": 15.328,
        "file_type": "py",
        "inclusive_size": 15.328,
        "id": "src\\transformers\\pipelines\\mask_generation.py"
      },
      {
        "type": "file",
        "size": 8.346,
        "file_type": "py",
        "inclusive_size": 8.346,
        "id": "src\\transformers\\pipelines\\object_detection.py"
      },
      {
        "type": "file",
        "size": 12.816,
        "file_type": "py",
        "inclusive_size": 12.816,
        "id": "src\\transformers\\pipelines\\pt_utils.py"
      },
      {
        "type": "file",
        "size": 28.793,
        "file_type": "py",
        "inclusive_size": 28.793,
        "id": "src\\transformers\\pipelines\\question_answering.py"
      },
      {
        "type": "file",
        "size": 17.171,
        "file_type": "py",
        "inclusive_size": 17.171,
        "id": "src\\transformers\\pipelines\\table_question_answering.py"
      },
      {
        "type": "file",
        "size": 10.166,
        "file_type": "py",
        "inclusive_size": 10.166,
        "id": "src\\transformers\\pipelines\\text_classification.py"
      },
      {
        "type": "file",
        "size": 24.92,
        "file_type": "py",
        "inclusive_size": 24.92,
        "id": "src\\transformers\\pipelines\\text_generation.py"
      },
      {
        "type": "file",
        "size": 13.103,
        "file_type": "py",
        "inclusive_size": 13.103,
        "id": "src\\transformers\\pipelines\\text_to_audio.py"
      },
      {
        "type": "file",
        "size": 28.882,
        "file_type": "py",
        "inclusive_size": 28.882,
        "id": "src\\transformers\\pipelines\\token_classification.py"
      },
      {
        "type": "file",
        "size": 7.628,
        "file_type": "py",
        "inclusive_size": 7.628,
        "id": "src\\transformers\\pipelines\\video_classification.py"
      },
      {
        "type": "file",
        "size": 9.599,
        "file_type": "py",
        "inclusive_size": 9.599,
        "id": "src\\transformers\\pipelines\\visual_question_answering.py"
      },
      {
        "type": "file",
        "size": 6.687,
        "file_type": "py",
        "inclusive_size": 6.687,
        "id": "src\\transformers\\pipelines\\zero_shot_audio_classification.py"
      },
      {
        "type": "file",
        "size": 11.976,
        "file_type": "py",
        "inclusive_size": 11.976,
        "id": "src\\transformers\\pipelines\\zero_shot_classification.py"
      },
      {
        "type": "file",
        "size": 7.644,
        "file_type": "py",
        "inclusive_size": 7.644,
        "id": "src\\transformers\\pipelines\\zero_shot_image_classification.py"
      },
      {
        "type": "file",
        "size": 10.393,
        "file_type": "py",
        "inclusive_size": 10.393,
        "id": "src\\transformers\\pipelines\\zero_shot_object_detection.py"
      },
      {
        "type": "file",
        "size": 69.115,
        "file_type": "py",
        "inclusive_size": 69.115,
        "id": "src\\transformers\\pipelines\\__init__.py"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 60.117,
        "id": "src\\transformers\\models\\afmoe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 81.81700000000001,
        "id": "src\\transformers\\models\\aimv2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 56.311,
        "id": "src\\transformers\\models\\albert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 81.961,
        "id": "src\\transformers\\models\\align"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 75.575,
        "id": "src\\transformers\\models\\altclip"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 46.842999999999996,
        "id": "src\\transformers\\models\\apertus"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 41.265,
        "id": "src\\transformers\\models\\arcee"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 171.32,
        "id": "src\\transformers\\models\\aria"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 73.229,
        "id": "src\\transformers\\models\\audioflamingo3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.814,
        "id": "src\\transformers\\models\\audio_spectrogram_transformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 324.428,
        "id": "src\\transformers\\models\\auto"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 113.169,
        "id": "src\\transformers\\models\\autoformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 47.24,
        "id": "src\\transformers\\models\\aya_vision"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 145.212,
        "id": "src\\transformers\\models\\bamba"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 120.582,
        "id": "src\\transformers\\models\\bark"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 86.925,
        "id": "src\\transformers\\models\\bart"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 7.069,
        "id": "src\\transformers\\models\\barthez"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 15.213000000000001,
        "id": "src\\transformers\\models\\bartpho"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 120.392,
        "id": "src\\transformers\\models\\beit"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 110.452,
        "id": "src\\transformers\\models\\bert"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 25.346,
        "id": "src\\transformers\\models\\bertweet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 42.474,
        "id": "src\\transformers\\models\\bert_generation"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 36.286,
        "id": "src\\transformers\\models\\bert_japanese"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 141.47400000000002,
        "id": "src\\transformers\\models\\bigbird_pegasus"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 145.238,
        "id": "src\\transformers\\models\\big_bird"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 89.67099999999999,
        "id": "src\\transformers\\models\\biogpt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 58.533,
        "id": "src\\transformers\\models\\bit"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 36.376,
        "id": "src\\transformers\\models\\bitnet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 75.188,
        "id": "src\\transformers\\models\\blenderbot"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 72.24900000000001,
        "id": "src\\transformers\\models\\blenderbot_small"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 132.99699999999999,
        "id": "src\\transformers\\models\\blip"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 125.03800000000001,
        "id": "src\\transformers\\models\\blip_2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 66.411,
        "id": "src\\transformers\\models\\bloom"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 150.857,
        "id": "src\\transformers\\models\\blt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 134.1,
        "id": "src\\transformers\\models\\bridgetower"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 58.416,
        "id": "src\\transformers\\models\\bros"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 17.628,
        "id": "src\\transformers\\models\\byt5"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 92.066,
        "id": "src\\transformers\\models\\camembert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 78.406,
        "id": "src\\transformers\\models\\canine"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 113.56299999999999,
        "id": "src\\transformers\\models\\chameleon"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 91.668,
        "id": "src\\transformers\\models\\chinese_clip"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 122.62,
        "id": "src\\transformers\\models\\clap"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 92.274,
        "id": "src\\transformers\\models\\clip"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 93.028,
        "id": "src\\transformers\\models\\clipseg"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 146.33100000000002,
        "id": "src\\transformers\\models\\clvp"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 43.325,
        "id": "src\\transformers\\models\\codegen"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 16.076999999999998,
        "id": "src\\transformers\\models\\code_llama"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 70.129,
        "id": "src\\transformers\\models\\cohere"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 54.279,
        "id": "src\\transformers\\models\\cohere2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 57.875,
        "id": "src\\transformers\\models\\cohere2_vision"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 51.25,
        "id": "src\\transformers\\models\\colpali"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 56.998000000000005,
        "id": "src\\transformers\\models\\colqwen2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 249.31400000000002,
        "id": "src\\transformers\\models\\conditional_detr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 65.654,
        "id": "src\\transformers\\models\\convbert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 55.701,
        "id": "src\\transformers\\models\\convnext"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 36.744,
        "id": "src\\transformers\\models\\convnextv2"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 24.692999999999998,
        "id": "src\\transformers\\models\\cpm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 46.702000000000005,
        "id": "src\\transformers\\models\\cpmant"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 159.068,
        "id": "src\\transformers\\models\\csm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 40.466,
        "id": "src\\transformers\\models\\ctrl"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.325,
        "id": "src\\transformers\\models\\cvt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.452,
        "id": "src\\transformers\\models\\cwm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 101.532,
        "id": "src\\transformers\\models\\dab_detr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 52.535000000000004,
        "id": "src\\transformers\\models\\dac"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 261.357,
        "id": "src\\transformers\\models\\data2vec"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 66.428,
        "id": "src\\transformers\\models\\dbrx"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 63.919999999999995,
        "id": "src\\transformers\\models\\deberta"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 70.789,
        "id": "src\\transformers\\models\\deberta_v2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 43.96,
        "id": "src\\transformers\\models\\decision_transformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 63.04,
        "id": "src\\transformers\\models\\deepseek_v2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 60.492000000000004,
        "id": "src\\transformers\\models\\deepseek_v3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 79.826,
        "id": "src\\transformers\\models\\deepseek_vl"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 144.851,
        "id": "src\\transformers\\models\\deepseek_vl_hybrid"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 212.87,
        "id": "src\\transformers\\models\\deformable_detr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 62.575,
        "id": "src\\transformers\\models\\deit"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 0.988,
        "id": "src\\transformers\\models\\deprecated"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 52.422000000000004,
        "id": "src\\transformers\\models\\depth_anything"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 90.058,
        "id": "src\\transformers\\models\\depth_pro"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 240.99,
        "id": "src\\transformers\\models\\detr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 149.131,
        "id": "src\\transformers\\models\\dia"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 1.556,
        "id": "src\\transformers\\models\\dialogpt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 64.428,
        "id": "src\\transformers\\models\\diffllama"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 39.774,
        "id": "src\\transformers\\models\\dinat"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.961999999999996,
        "id": "src\\transformers\\models\\dinov2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 68.709,
        "id": "src\\transformers\\models\\dinov2_with_registers"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 26.977,
        "id": "src\\transformers\\models\\dinov3_convnext"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 70.829,
        "id": "src\\transformers\\models\\dinov3_vit"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 48.322,
        "id": "src\\transformers\\models\\distilbert"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 9.128,
        "id": "src\\transformers\\models\\dit"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 88.224,
        "id": "src\\transformers\\models\\doge"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 95.25,
        "id": "src\\transformers\\models\\donut"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.786,
        "id": "src\\transformers\\models\\dots1"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 67.714,
        "id": "src\\transformers\\models\\dpr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 192.932,
        "id": "src\\transformers\\models\\dpt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 224.404,
        "id": "src\\transformers\\models\\d_fine"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 95.744,
        "id": "src\\transformers\\models\\edgetam"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 256.615,
        "id": "src\\transformers\\models\\edgetam_video"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 121.592,
        "id": "src\\transformers\\models\\efficientloftr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 67.008,
        "id": "src\\transformers\\models\\efficientnet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 74.057,
        "id": "src\\transformers\\models\\electra"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 185.277,
        "id": "src\\transformers\\models\\emu3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 69.97,
        "id": "src\\transformers\\models\\encodec"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 28.545,
        "id": "src\\transformers\\models\\encoder_decoder"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 164.747,
        "id": "src\\transformers\\models\\eomt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 111.01299999999999,
        "id": "src\\transformers\\models\\ernie"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 39.234,
        "id": "src\\transformers\\models\\ernie4_5"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 57.345,
        "id": "src\\transformers\\models\\ernie4_5_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 281.661,
        "id": "src\\transformers\\models\\ernie4_5_vl_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 289.0,
        "id": "src\\transformers\\models\\esm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 124.365,
        "id": "src\\transformers\\models\\evolla"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 57.164,
        "id": "src\\transformers\\models\\exaone4"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 76.026,
        "id": "src\\transformers\\models\\falcon"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 155.093,
        "id": "src\\transformers\\models\\falcon_h1"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 78.547,
        "id": "src\\transformers\\models\\falcon_mamba"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 120.256,
        "id": "src\\transformers\\models\\fastspeech2_conformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 53.951,
        "id": "src\\transformers\\models\\fast_vlm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 110.456,
        "id": "src\\transformers\\models\\flaubert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 191.03199999999998,
        "id": "src\\transformers\\models\\flava"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 58.230000000000004,
        "id": "src\\transformers\\models\\flex_olmo"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 183.633,
        "id": "src\\transformers\\models\\florence2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 59.431999999999995,
        "id": "src\\transformers\\models\\fnet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 55.194,
        "id": "src\\transformers\\models\\focalnet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 87.241,
        "id": "src\\transformers\\models\\fsmt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 79.55799999999999,
        "id": "src\\transformers\\models\\funnel"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 118.158,
        "id": "src\\transformers\\models\\fuyu"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 57.516000000000005,
        "id": "src\\transformers\\models\\gemma"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 69.396,
        "id": "src\\transformers\\models\\gemma2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 195.151,
        "id": "src\\transformers\\models\\gemma3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 336.89,
        "id": "src\\transformers\\models\\gemma3n"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 94.057,
        "id": "src\\transformers\\models\\git"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 44.26,
        "id": "src\\transformers\\models\\glm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.034,
        "id": "src\\transformers\\models\\glm4"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 113.26299999999999,
        "id": "src\\transformers\\models\\glm46v"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 258.488,
        "id": "src\\transformers\\models\\glm4v"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 167.48000000000002,
        "id": "src\\transformers\\models\\glm4v_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 54.484,
        "id": "src\\transformers\\models\\glm4_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 59.999,
        "id": "src\\transformers\\models\\glm4_moe_lite"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 72.783,
        "id": "src\\transformers\\models\\glmasr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 216.491,
        "id": "src\\transformers\\models\\glm_image"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 110.132,
        "id": "src\\transformers\\models\\glm_ocr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 60.315,
        "id": "src\\transformers\\models\\glpn"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 122.515,
        "id": "src\\transformers\\models\\got_ocr2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 80.833,
        "id": "src\\transformers\\models\\gpt2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 50.594,
        "id": "src\\transformers\\models\\gptj"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.904999999999994,
        "id": "src\\transformers\\models\\gpt_bigcode"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 62.371,
        "id": "src\\transformers\\models\\gpt_neo"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 77.807,
        "id": "src\\transformers\\models\\gpt_neox"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 57.243,
        "id": "src\\transformers\\models\\gpt_neox_japanese"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 90.98,
        "id": "src\\transformers\\models\\gpt_oss"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 19.146,
        "id": "src\\transformers\\models\\gpt_sw3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 47.451,
        "id": "src\\transformers\\models\\granite"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 55.702,
        "id": "src\\transformers\\models\\granitemoe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 99.198,
        "id": "src\\transformers\\models\\granitemoehybrid"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 51.531,
        "id": "src\\transformers\\models\\granitemoeshared"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 47.754000000000005,
        "id": "src\\transformers\\models\\granite_speech"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 286.17699999999996,
        "id": "src\\transformers\\models\\grounding_dino"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 88.17399999999999,
        "id": "src\\transformers\\models\\groupvit"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 36.16,
        "id": "src\\transformers\\models\\helium"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 4.875,
        "id": "src\\transformers\\models\\herbert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 53.161,
        "id": "src\\transformers\\models\\hgnet_v2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 86.546,
        "id": "src\\transformers\\models\\hiera"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 101.87,
        "id": "src\\transformers\\models\\hubert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 37.905,
        "id": "src\\transformers\\models\\hunyuan_v1_dense"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 47.067,
        "id": "src\\transformers\\models\\hunyuan_v1_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 85.664,
        "id": "src\\transformers\\models\\ibert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 130.715,
        "id": "src\\transformers\\models\\idefics"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 117.322,
        "id": "src\\transformers\\models\\idefics2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 141.179,
        "id": "src\\transformers\\models\\idefics3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 44.203,
        "id": "src\\transformers\\models\\ijepa"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 74.94800000000001,
        "id": "src\\transformers\\models\\imagegpt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 150.209,
        "id": "src\\transformers\\models\\informer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 98.827,
        "id": "src\\transformers\\models\\instructblip"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 135.411,
        "id": "src\\transformers\\models\\instructblipvideo"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 112.64,
        "id": "src\\transformers\\models\\internvl"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 36.505,
        "id": "src\\transformers\\models\\jais2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 96.47,
        "id": "src\\transformers\\models\\jamba"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 216.178,
        "id": "src\\transformers\\models\\janus"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 68.401,
        "id": "src\\transformers\\models\\jetmoe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 126.462,
        "id": "src\\transformers\\models\\kosmos2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 130.01,
        "id": "src\\transformers\\models\\kosmos2_5"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 125.685,
        "id": "src\\transformers\\models\\kyutai_speech_to_text"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 92.965,
        "id": "src\\transformers\\models\\lasr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 51.365,
        "id": "src\\transformers\\models\\layoutlm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 139.629,
        "id": "src\\transformers\\models\\layoutlmv2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 134.105,
        "id": "src\\transformers\\models\\layoutlmv3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 70.714,
        "id": "src\\transformers\\models\\layoutxlm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 116.56200000000001,
        "id": "src\\transformers\\models\\led"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 57.854,
        "id": "src\\transformers\\models\\levit"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 61.919,
        "id": "src\\transformers\\models\\lfm2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 55.24,
        "id": "src\\transformers\\models\\lfm2_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 76.054,
        "id": "src\\transformers\\models\\lfm2_vl"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 145.159,
        "id": "src\\transformers\\models\\lightglue"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 54.782000000000004,
        "id": "src\\transformers\\models\\lighton_ocr"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 49.403000000000006,
        "id": "src\\transformers\\models\\lilt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 62.254000000000005,
        "id": "src\\transformers\\models\\llama"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 150.872,
        "id": "src\\transformers\\models\\llama4"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 68.35000000000001,
        "id": "src\\transformers\\models\\llava"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 120.875,
        "id": "src\\transformers\\models\\llava_next"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 115.509,
        "id": "src\\transformers\\models\\llava_next_video"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 177.058,
        "id": "src\\transformers\\models\\llava_onevision"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 63.121,
        "id": "src\\transformers\\models\\longcat_flash"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 113.607,
        "id": "src\\transformers\\models\\longformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 100.78099999999999,
        "id": "src\\transformers\\models\\longt5"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 193.748,
        "id": "src\\transformers\\models\\luke"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 192.558,
        "id": "src\\transformers\\models\\lw_detr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 74.20100000000001,
        "id": "src\\transformers\\models\\lxmert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 79.285,
        "id": "src\\transformers\\models\\m2m_100"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 54.118,
        "id": "src\\transformers\\models\\mamba"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 64.259,
        "id": "src\\transformers\\models\\mamba2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 151.679,
        "id": "src\\transformers\\models\\marian"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 108.521,
        "id": "src\\transformers\\models\\markuplm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 282.008,
        "id": "src\\transformers\\models\\mask2former"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 303.375,
        "id": "src\\transformers\\models\\maskformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 91.02499999999999,
        "id": "src\\transformers\\models\\mbart"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 15.811,
        "id": "src\\transformers\\models\\mbart50"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 82.251,
        "id": "src\\transformers\\models\\megatron_bert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 55.306,
        "id": "src\\transformers\\models\\megatron_gpt2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 121.649,
        "id": "src\\transformers\\models\\metaclip_2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 36.897999999999996,
        "id": "src\\transformers\\models\\mgp_str"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 103.286,
        "id": "src\\transformers\\models\\mimi"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 84.494,
        "id": "src\\transformers\\models\\minimax"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 58.592,
        "id": "src\\transformers\\models\\minimax_m2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 44.728,
        "id": "src\\transformers\\models\\ministral"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 52.658,
        "id": "src\\transformers\\models\\ministral3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 50.226,
        "id": "src\\transformers\\models\\mistral"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 55.434,
        "id": "src\\transformers\\models\\mistral3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 69.991,
        "id": "src\\transformers\\models\\mixtral"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 65.11,
        "id": "src\\transformers\\models\\mlcd"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 193.03,
        "id": "src\\transformers\\models\\mllama"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 99.243,
        "id": "src\\transformers\\models\\mluke"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 195.397,
        "id": "src\\transformers\\models\\mm_grounding_dino"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 65.912,
        "id": "src\\transformers\\models\\mobilebert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 41.186,
        "id": "src\\transformers\\models\\mobilenet_v1"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 77.595,
        "id": "src\\transformers\\models\\mobilenet_v2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 88.247,
        "id": "src\\transformers\\models\\mobilevit"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 54.059,
        "id": "src\\transformers\\models\\mobilevitv2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 99.134,
        "id": "src\\transformers\\models\\modernbert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 85.407,
        "id": "src\\transformers\\models\\modernbert_decoder"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 109.87700000000001,
        "id": "src\\transformers\\models\\moonshine"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 151.036,
        "id": "src\\transformers\\models\\moshi"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 50.229,
        "id": "src\\transformers\\models\\mpnet"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 44.586999999999996,
        "id": "src\\transformers\\models\\mpt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 65.386,
        "id": "src\\transformers\\models\\mra"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 85.171,
        "id": "src\\transformers\\models\\mt5"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 136.503,
        "id": "src\\transformers\\models\\musicgen"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 152.394,
        "id": "src\\transformers\\models\\musicgen_melody"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 82.584,
        "id": "src\\transformers\\models\\mvp"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 23.206999999999997,
        "id": "src\\transformers\\models\\myt5"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 51.116,
        "id": "src\\transformers\\models\\nanochat"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 68.142,
        "id": "src\\transformers\\models\\nemotron"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 15.134,
        "id": "src\\transformers\\models\\nllb"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 69.007,
        "id": "src\\transformers\\models\\nllb_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 79.287,
        "id": "src\\transformers\\models\\nougat"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 51.713,
        "id": "src\\transformers\\models\\nystromformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 47.961999999999996,
        "id": "src\\transformers\\models\\olmo"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 58.038,
        "id": "src\\transformers\\models\\olmo2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 66.938,
        "id": "src\\transformers\\models\\olmo3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 65.744,
        "id": "src\\transformers\\models\\olmoe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 121.81,
        "id": "src\\transformers\\models\\omdet_turbo"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 312.79,
        "id": "src\\transformers\\models\\oneformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 50.559,
        "id": "src\\transformers\\models\\openai"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 57.321999999999996,
        "id": "src\\transformers\\models\\opt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 118.565,
        "id": "src\\transformers\\models\\ovis2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 175.64700000000002,
        "id": "src\\transformers\\models\\owlv2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 146.731,
        "id": "src\\transformers\\models\\owlvit"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 195.327,
        "id": "src\\transformers\\models\\paddleocr_vl"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 83.27,
        "id": "src\\transformers\\models\\paligemma"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 106.859,
        "id": "src\\transformers\\models\\parakeet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 98.742,
        "id": "src\\transformers\\models\\patchtsmixer"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 98.29599999999999,
        "id": "src\\transformers\\models\\patchtst"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 81.67,
        "id": "src\\transformers\\models\\pegasus"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 76.284,
        "id": "src\\transformers\\models\\pegasus_x"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 197.01899999999998,
        "id": "src\\transformers\\models\\perceiver"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 92.031,
        "id": "src\\transformers\\models\\perception_lm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.284,
        "id": "src\\transformers\\models\\persimmon"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 61.181,
        "id": "src\\transformers\\models\\pe_audio"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 92.608,
        "id": "src\\transformers\\models\\pe_audio_video"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 48.233,
        "id": "src\\transformers\\models\\pe_video"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 52.974,
        "id": "src\\transformers\\models\\phi"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 48.339999999999996,
        "id": "src\\transformers\\models\\phi3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 224.542,
        "id": "src\\transformers\\models\\phi4_multimodal"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 66.491,
        "id": "src\\transformers\\models\\phimoe"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 14.015,
        "id": "src\\transformers\\models\\phobert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 127.434,
        "id": "src\\transformers\\models\\pix2struct"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 55.403,
        "id": "src\\transformers\\models\\pixio"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 76.436,
        "id": "src\\transformers\\models\\pixtral"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 111.269,
        "id": "src\\transformers\\models\\plbart"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 56.446,
        "id": "src\\transformers\\models\\poolformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 131.6,
        "id": "src\\transformers\\models\\pop2piano"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 219.302,
        "id": "src\\transformers\\models\\pp_doclayout_v3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 94.847,
        "id": "src\\transformers\\models\\prompt_depth_anything"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 124.299,
        "id": "src\\transformers\\models\\prophetnet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 54.474,
        "id": "src\\transformers\\models\\pvt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 44.661,
        "id": "src\\transformers\\models\\pvt_v2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.459,
        "id": "src\\transformers\\models\\qwen2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 439.932,
        "id": "src\\transformers\\models\\qwen2_5_omni"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 159.658,
        "id": "src\\transformers\\models\\qwen2_5_vl"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 58.611999999999995,
        "id": "src\\transformers\\models\\qwen2_audio"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 56.830999999999996,
        "id": "src\\transformers\\models\\qwen2_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 151.504,
        "id": "src\\transformers\\models\\qwen2_vl"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 40.481,
        "id": "src\\transformers\\models\\qwen3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 51.661,
        "id": "src\\transformers\\models\\qwen3_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 108.498,
        "id": "src\\transformers\\models\\qwen3_next"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 395.358,
        "id": "src\\transformers\\models\\qwen3_omni_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 186.238,
        "id": "src\\transformers\\models\\qwen3_vl"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 122.369,
        "id": "src\\transformers\\models\\qwen3_vl_moe"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 130.491,
        "id": "src\\transformers\\models\\rag"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 55.649,
        "id": "src\\transformers\\models\\recurrent_gemma"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 140.27100000000002,
        "id": "src\\transformers\\models\\reformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 49.373,
        "id": "src\\transformers\\models\\regnet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 70.402,
        "id": "src\\transformers\\models\\rembert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 30.047,
        "id": "src\\transformers\\models\\resnet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 119.86699999999999,
        "id": "src\\transformers\\models\\roberta"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 68.974,
        "id": "src\\transformers\\models\\roberta_prelayernorm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 142.353,
        "id": "src\\transformers\\models\\roc_bert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 78.367,
        "id": "src\\transformers\\models\\roformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 252.874,
        "id": "src\\transformers\\models\\rt_detr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 167.998,
        "id": "src\\transformers\\models\\rt_detr_v2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.641,
        "id": "src\\transformers\\models\\rwkv"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 185.864,
        "id": "src\\transformers\\models\\sam"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 224.464,
        "id": "src\\transformers\\models\\sam2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 336.00600000000003,
        "id": "src\\transformers\\models\\sam2_video"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 228.198,
        "id": "src\\transformers\\models\\sam3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 96.969,
        "id": "src\\transformers\\models\\sam3_tracker"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 222.272,
        "id": "src\\transformers\\models\\sam3_tracker_video"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 163.113,
        "id": "src\\transformers\\models\\sam3_video"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 139.964,
        "id": "src\\transformers\\models\\sam_hq"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 260.732,
        "id": "src\\transformers\\models\\seamless_m4t"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 243.449,
        "id": "src\\transformers\\models\\seamless_m4t_v2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 40.798,
        "id": "src\\transformers\\models\\seed_oss"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 90.923,
        "id": "src\\transformers\\models\\segformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 91.667,
        "id": "src\\transformers\\models\\seggpt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 92.637,
        "id": "src\\transformers\\models\\sew"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 98.581,
        "id": "src\\transformers\\models\\sew_d"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 39.41,
        "id": "src\\transformers\\models\\shieldgemma2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 106.81899999999999,
        "id": "src\\transformers\\models\\siglip"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 134.174,
        "id": "src\\transformers\\models\\siglip2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 48.712,
        "id": "src\\transformers\\models\\smollm3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 168.365,
        "id": "src\\transformers\\models\\smolvlm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 47.14,
        "id": "src\\transformers\\models\\solar_open"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 222.06199999999998,
        "id": "src\\transformers\\models\\speecht5"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 56.995000000000005,
        "id": "src\\transformers\\models\\speech_encoder_decoder"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 94.113,
        "id": "src\\transformers\\models\\speech_to_text"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 47.814,
        "id": "src\\transformers\\models\\splinter"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.483,
        "id": "src\\transformers\\models\\squeezebert"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 44.431,
        "id": "src\\transformers\\models\\stablelm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 41.334,
        "id": "src\\transformers\\models\\starcoder2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 86.789,
        "id": "src\\transformers\\models\\superglue"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 54.836999999999996,
        "id": "src\\transformers\\models\\superpoint"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 31.951999999999998,
        "id": "src\\transformers\\models\\swiftformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 72.006,
        "id": "src\\transformers\\models\\swin"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 76.785,
        "id": "src\\transformers\\models\\swin2sr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 72.073,
        "id": "src\\transformers\\models\\swinv2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 131.774,
        "id": "src\\transformers\\models\\switch_transformers"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 106.331,
        "id": "src\\transformers\\models\\t5"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 136.104,
        "id": "src\\transformers\\models\\t5gemma"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 173.915,
        "id": "src\\transformers\\models\\t5gemma2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 111.057,
        "id": "src\\transformers\\models\\table_transformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 242.083,
        "id": "src\\transformers\\models\\tapas"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 53.472,
        "id": "src\\transformers\\models\\textnet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 85.454,
        "id": "src\\transformers\\models\\timesfm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 48.665,
        "id": "src\\transformers\\models\\timesformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 96.988,
        "id": "src\\transformers\\models\\time_series_transformer"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 11.577,
        "id": "src\\transformers\\models\\timm_backbone"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 28.927999999999997,
        "id": "src\\transformers\\models\\timm_wrapper"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 55.185,
        "id": "src\\transformers\\models\\trocr"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 81.118,
        "id": "src\\transformers\\models\\tvp"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 186.107,
        "id": "src\\transformers\\models\\udop"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 101.197,
        "id": "src\\transformers\\models\\umt5"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 107.765,
        "id": "src\\transformers\\models\\unispeech"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 125.4,
        "id": "src\\transformers\\models\\unispeech_sat"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 62.122,
        "id": "src\\transformers\\models\\univnet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 46.273,
        "id": "src\\transformers\\models\\upernet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.798,
        "id": "src\\transformers\\models\\vaultgemma"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 70.589,
        "id": "src\\transformers\\models\\videomae"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 201.651,
        "id": "src\\transformers\\models\\video_llama_3"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 72.045,
        "id": "src\\transformers\\models\\video_llava"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 107.69200000000001,
        "id": "src\\transformers\\models\\vilt"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 45.419000000000004,
        "id": "src\\transformers\\models\\vipllava"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 27.720000000000002,
        "id": "src\\transformers\\models\\vision_encoder_decoder"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 24.727,
        "id": "src\\transformers\\models\\vision_text_dual_encoder"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 80.37899999999999,
        "id": "src\\transformers\\models\\visual_bert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 67.772,
        "id": "src\\transformers\\models\\vit"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 38.36,
        "id": "src\\transformers\\models\\vitdet"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 43.361,
        "id": "src\\transformers\\models\\vitmatte"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 75.251,
        "id": "src\\transformers\\models\\vitpose"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 24.562,
        "id": "src\\transformers\\models\\vitpose_backbone"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 104.554,
        "id": "src\\transformers\\models\\vits"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 53.187,
        "id": "src\\transformers\\models\\vit_mae"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 37.561,
        "id": "src\\transformers\\models\\vit_msn"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 59.888,
        "id": "src\\transformers\\models\\vivit"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 76.987,
        "id": "src\\transformers\\models\\vjepa2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 73.342,
        "id": "src\\transformers\\models\\voxtral"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 177.77,
        "id": "src\\transformers\\models\\wav2vec2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 142.547,
        "id": "src\\transformers\\models\\wav2vec2_bert"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 152.01,
        "id": "src\\transformers\\models\\wav2vec2_conformer"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 23.814999999999998,
        "id": "src\\transformers\\models\\wav2vec2_phoneme"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 28.435,
        "id": "src\\transformers\\models\\wav2vec2_with_lm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 126.30099999999999,
        "id": "src\\transformers\\models\\wavlm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 307.064,
        "id": "src\\transformers\\models\\whisper"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 48.814,
        "id": "src\\transformers\\models\\xcodec"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 43.672,
        "id": "src\\transformers\\models\\xglm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 111.263,
        "id": "src\\transformers\\models\\xlm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 89.42699999999999,
        "id": "src\\transformers\\models\\xlm_roberta"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 96.964,
        "id": "src\\transformers\\models\\xlm_roberta_xl"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 126.685,
        "id": "src\\transformers\\models\\xlnet"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 78.43799999999999,
        "id": "src\\transformers\\models\\xlstm"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 78.813,
        "id": "src\\transformers\\models\\xmod"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 101.708,
        "id": "src\\transformers\\models\\x_clip"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 144.98999999999998,
        "id": "src\\transformers\\models\\yolos"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 57.779,
        "id": "src\\transformers\\models\\yoso"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 48.975,
        "id": "src\\transformers\\models\\youtu"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 70.919,
        "id": "src\\transformers\\models\\zamba"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 148.145,
        "id": "src\\transformers\\models\\zamba2"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 127.241,
        "id": "src\\transformers\\models\\zoedepth"
      },
      {
        "type": "file",
        "size": 12.625,
        "file_type": "py",
        "inclusive_size": 12.625,
        "id": "src\\transformers\\models\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.796,
        "file_type": "py",
        "inclusive_size": 12.796,
        "id": "src\\transformers\\models\\zoedepth\\configuration_zoedepth.py"
      },
      {
        "type": "file",
        "size": 18.08,
        "file_type": "py",
        "inclusive_size": 18.08,
        "id": "src\\transformers\\models\\zoedepth\\convert_zoedepth_to_hf.py"
      },
      {
        "type": "file",
        "size": 28.963,
        "file_type": "py",
        "inclusive_size": 28.963,
        "id": "src\\transformers\\models\\zoedepth\\image_processing_zoedepth.py"
      },
      {
        "type": "file",
        "size": 11.983,
        "file_type": "py",
        "inclusive_size": 11.983,
        "id": "src\\transformers\\models\\zoedepth\\image_processing_zoedepth_fast.py"
      },
      {
        "type": "file",
        "size": 54.327,
        "file_type": "py",
        "inclusive_size": 54.327,
        "id": "src\\transformers\\models\\zoedepth\\modeling_zoedepth.py"
      },
      {
        "type": "file",
        "size": 1.092,
        "file_type": "py",
        "inclusive_size": 1.092,
        "id": "src\\transformers\\models\\zoedepth\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.786,
        "file_type": "py",
        "inclusive_size": 12.786,
        "id": "src\\transformers\\models\\zamba2\\configuration_zamba2.py"
      },
      {
        "type": "file",
        "size": 81.492,
        "file_type": "py",
        "inclusive_size": 81.492,
        "id": "src\\transformers\\models\\zamba2\\modeling_zamba2.py"
      },
      {
        "type": "file",
        "size": 52.874,
        "file_type": "py",
        "inclusive_size": 52.874,
        "id": "src\\transformers\\models\\zamba2\\modular_zamba2.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\zamba2\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.262,
        "file_type": "py",
        "inclusive_size": 11.262,
        "id": "src\\transformers\\models\\zamba\\configuration_zamba.py"
      },
      {
        "type": "file",
        "size": 58.666,
        "file_type": "py",
        "inclusive_size": 58.666,
        "id": "src\\transformers\\models\\zamba\\modeling_zamba.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\zamba\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.169,
        "file_type": "py",
        "inclusive_size": 10.169,
        "id": "src\\transformers\\models\\youtu\\configuration_youtu.py"
      },
      {
        "type": "file",
        "size": 27.041,
        "file_type": "py",
        "inclusive_size": 27.041,
        "id": "src\\transformers\\models\\youtu\\modeling_youtu.py"
      },
      {
        "type": "file",
        "size": 10.769,
        "file_type": "py",
        "inclusive_size": 10.769,
        "id": "src\\transformers\\models\\youtu\\modular_youtu.py"
      },
      {
        "type": "file",
        "size": 0.996,
        "file_type": "py",
        "inclusive_size": 0.996,
        "id": "src\\transformers\\models\\youtu\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.625,
        "file_type": "py",
        "inclusive_size": 6.625,
        "id": "src\\transformers\\models\\yoso\\configuration_yoso.py"
      },
      {
        "type": "file",
        "size": 4.113,
        "file_type": "py",
        "inclusive_size": 4.113,
        "id": "src\\transformers\\models\\yoso\\convert_yoso_pytorch_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 46.052,
        "file_type": "py",
        "inclusive_size": 46.052,
        "id": "src\\transformers\\models\\yoso\\modeling_yoso.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\yoso\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.983,
        "file_type": "py",
        "inclusive_size": 6.983,
        "id": "src\\transformers\\models\\yolos\\configuration_yolos.py"
      },
      {
        "type": "file",
        "size": 11.341,
        "file_type": "py",
        "inclusive_size": 11.341,
        "id": "src\\transformers\\models\\yolos\\convert_yolos_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 63.407,
        "file_type": "py",
        "inclusive_size": 63.407,
        "id": "src\\transformers\\models\\yolos\\image_processing_yolos.py"
      },
      {
        "type": "file",
        "size": 29.683,
        "file_type": "py",
        "inclusive_size": 29.683,
        "id": "src\\transformers\\models\\yolos\\image_processing_yolos_fast.py"
      },
      {
        "type": "file",
        "size": 27.409,
        "file_type": "py",
        "inclusive_size": 27.409,
        "id": "src\\transformers\\models\\yolos\\modeling_yolos.py"
      },
      {
        "type": "file",
        "size": 5.043,
        "file_type": "py",
        "inclusive_size": 5.043,
        "id": "src\\transformers\\models\\yolos\\modular_yolos.py"
      },
      {
        "type": "file",
        "size": 1.124,
        "file_type": "py",
        "inclusive_size": 1.124,
        "id": "src\\transformers\\models\\yolos\\__init__.py"
      },
      {
        "type": "file",
        "size": 18.461,
        "file_type": "py",
        "inclusive_size": 18.461,
        "id": "src\\transformers\\models\\x_clip\\configuration_x_clip.py"
      },
      {
        "type": "file",
        "size": 18.08,
        "file_type": "py",
        "inclusive_size": 18.08,
        "id": "src\\transformers\\models\\x_clip\\convert_x_clip_original_pytorch_to_hf.py"
      },
      {
        "type": "file",
        "size": 63.154,
        "file_type": "py",
        "inclusive_size": 63.154,
        "id": "src\\transformers\\models\\x_clip\\modeling_x_clip.py"
      },
      {
        "type": "file",
        "size": 0.983,
        "file_type": "py",
        "inclusive_size": 0.983,
        "id": "src\\transformers\\models\\x_clip\\processing_x_clip.py"
      },
      {
        "type": "file",
        "size": 1.03,
        "file_type": "py",
        "inclusive_size": 1.03,
        "id": "src\\transformers\\models\\x_clip\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.013,
        "file_type": "py",
        "inclusive_size": 8.013,
        "id": "src\\transformers\\models\\xmod\\configuration_xmod.py"
      },
      {
        "type": "file",
        "size": 9.831,
        "file_type": "py",
        "inclusive_size": 9.831,
        "id": "src\\transformers\\models\\xmod\\convert_xmod_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 59.98,
        "file_type": "py",
        "inclusive_size": 59.98,
        "id": "src\\transformers\\models\\xmod\\modeling_xmod.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\xmod\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.794,
        "file_type": "py",
        "inclusive_size": 12.794,
        "id": "src\\transformers\\models\\xlstm\\configuration_xlstm.py"
      },
      {
        "type": "file",
        "size": 64.597,
        "file_type": "py",
        "inclusive_size": 64.597,
        "id": "src\\transformers\\models\\xlstm\\modeling_xlstm.py"
      },
      {
        "type": "file",
        "size": 1.047,
        "file_type": "py",
        "inclusive_size": 1.047,
        "id": "src\\transformers\\models\\xlstm\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.413,
        "file_type": "py",
        "inclusive_size": 10.413,
        "id": "src\\transformers\\models\\xlnet\\configuration_xlnet.py"
      },
      {
        "type": "file",
        "size": 10.006,
        "file_type": "py",
        "inclusive_size": 10.006,
        "id": "src\\transformers\\models\\xlnet\\convert_xlnet_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 97.569,
        "file_type": "py",
        "inclusive_size": 97.569,
        "id": "src\\transformers\\models\\xlnet\\modeling_xlnet.py"
      },
      {
        "type": "file",
        "size": 7.668,
        "file_type": "py",
        "inclusive_size": 7.668,
        "id": "src\\transformers\\models\\xlnet\\tokenization_xlnet.py"
      },
      {
        "type": "file",
        "size": 1.029,
        "file_type": "py",
        "inclusive_size": 1.029,
        "id": "src\\transformers\\models\\xlnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.089,
        "file_type": "py",
        "inclusive_size": 6.089,
        "id": "src\\transformers\\models\\xlm_roberta_xl\\configuration_xlm_roberta_xl.py"
      },
      {
        "type": "file",
        "size": 8.217,
        "file_type": "py",
        "inclusive_size": 8.217,
        "id": "src\\transformers\\models\\xlm_roberta_xl\\convert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 51.831,
        "file_type": "py",
        "inclusive_size": 51.831,
        "id": "src\\transformers\\models\\xlm_roberta_xl\\modeling_xlm_roberta_xl.py"
      },
      {
        "type": "file",
        "size": 29.818,
        "file_type": "py",
        "inclusive_size": 29.818,
        "id": "src\\transformers\\models\\xlm_roberta_xl\\modular_xlm_roberta_xl.py"
      },
      {
        "type": "file",
        "size": 1.009,
        "file_type": "py",
        "inclusive_size": 1.009,
        "id": "src\\transformers\\models\\xlm_roberta_xl\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.322,
        "file_type": "py",
        "inclusive_size": 6.322,
        "id": "src\\transformers\\models\\xlm_roberta\\configuration_xlm_roberta.py"
      },
      {
        "type": "file",
        "size": 54.001,
        "file_type": "py",
        "inclusive_size": 54.001,
        "id": "src\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py"
      },
      {
        "type": "file",
        "size": 23.214,
        "file_type": "py",
        "inclusive_size": 23.214,
        "id": "src\\transformers\\models\\xlm_roberta\\modular_xlm_roberta.py"
      },
      {
        "type": "file",
        "size": 4.794,
        "file_type": "py",
        "inclusive_size": 4.794,
        "id": "src\\transformers\\models\\xlm_roberta\\tokenization_xlm_roberta.py"
      },
      {
        "type": "file",
        "size": 1.096,
        "file_type": "py",
        "inclusive_size": 1.096,
        "id": "src\\transformers\\models\\xlm_roberta\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.443,
        "file_type": "py",
        "inclusive_size": 10.443,
        "id": "src\\transformers\\models\\xlm\\configuration_xlm.py"
      },
      {
        "type": "file",
        "size": 2.937,
        "file_type": "py",
        "inclusive_size": 2.937,
        "id": "src\\transformers\\models\\xlm\\convert_xlm_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 73.503,
        "file_type": "py",
        "inclusive_size": 73.503,
        "id": "src\\transformers\\models\\xlm\\modeling_xlm.py"
      },
      {
        "type": "file",
        "size": 23.357,
        "file_type": "py",
        "inclusive_size": 23.357,
        "id": "src\\transformers\\models\\xlm\\tokenization_xlm.py"
      },
      {
        "type": "file",
        "size": 1.023,
        "file_type": "py",
        "inclusive_size": 1.023,
        "id": "src\\transformers\\models\\xlm\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.009,
        "file_type": "py",
        "inclusive_size": 6.009,
        "id": "src\\transformers\\models\\xglm\\configuration_xglm.py"
      },
      {
        "type": "file",
        "size": 2.344,
        "file_type": "py",
        "inclusive_size": 2.344,
        "id": "src\\transformers\\models\\xglm\\convert_xglm_original_ckpt_to_trfms.py"
      },
      {
        "type": "file",
        "size": 29.108,
        "file_type": "py",
        "inclusive_size": 29.108,
        "id": "src\\transformers\\models\\xglm\\modeling_xglm.py"
      },
      {
        "type": "file",
        "size": 5.185,
        "file_type": "py",
        "inclusive_size": 5.185,
        "id": "src\\transformers\\models\\xglm\\tokenization_xglm.py"
      },
      {
        "type": "file",
        "size": 1.026,
        "file_type": "py",
        "inclusive_size": 1.026,
        "id": "src\\transformers\\models\\xglm\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.878,
        "file_type": "py",
        "inclusive_size": 7.878,
        "id": "src\\transformers\\models\\xcodec\\configuration_xcodec.py"
      },
      {
        "type": "file",
        "size": 14.286,
        "file_type": "py",
        "inclusive_size": 14.286,
        "id": "src\\transformers\\models\\xcodec\\convert_xcodec_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 25.657,
        "file_type": "py",
        "inclusive_size": 25.657,
        "id": "src\\transformers\\models\\xcodec\\modeling_xcodec.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\xcodec\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.701,
        "file_type": "py",
        "inclusive_size": 14.701,
        "id": "src\\transformers\\models\\whisper\\configuration_whisper.py"
      },
      {
        "type": "file",
        "size": 14.98,
        "file_type": "py",
        "inclusive_size": 14.98,
        "id": "src\\transformers\\models\\whisper\\convert_openai_to_hf.py"
      },
      {
        "type": "file",
        "size": 22.804,
        "file_type": "py",
        "inclusive_size": 22.804,
        "id": "src\\transformers\\models\\whisper\\english_normalizer.py"
      },
      {
        "type": "file",
        "size": 16.784,
        "file_type": "py",
        "inclusive_size": 16.784,
        "id": "src\\transformers\\models\\whisper\\feature_extraction_whisper.py"
      },
      {
        "type": "file",
        "size": 109.268,
        "file_type": "py",
        "inclusive_size": 109.268,
        "id": "src\\transformers\\models\\whisper\\generation_whisper.py"
      },
      {
        "type": "file",
        "size": 67.446,
        "file_type": "py",
        "inclusive_size": 67.446,
        "id": "src\\transformers\\models\\whisper\\modeling_whisper.py"
      },
      {
        "type": "file",
        "size": 2.101,
        "file_type": "py",
        "inclusive_size": 2.101,
        "id": "src\\transformers\\models\\whisper\\processing_whisper.py"
      },
      {
        "type": "file",
        "size": 57.861,
        "file_type": "py",
        "inclusive_size": 57.861,
        "id": "src\\transformers\\models\\whisper\\tokenization_whisper.py"
      },
      {
        "type": "file",
        "size": 1.119,
        "file_type": "py",
        "inclusive_size": 1.119,
        "id": "src\\transformers\\models\\whisper\\__init__.py"
      },
      {
        "type": "file",
        "size": 18.615,
        "file_type": "py",
        "inclusive_size": 18.615,
        "id": "src\\transformers\\models\\wavlm\\configuration_wavlm.py"
      },
      {
        "type": "file",
        "size": 8.583,
        "file_type": "py",
        "inclusive_size": 8.583,
        "id": "src\\transformers\\models\\wavlm\\convert_wavlm_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 4.817,
        "file_type": "py",
        "inclusive_size": 4.817,
        "id": "src\\transformers\\models\\wavlm\\convert_wavlm_original_s3prl_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 70.097,
        "file_type": "py",
        "inclusive_size": 70.097,
        "id": "src\\transformers\\models\\wavlm\\modeling_wavlm.py"
      },
      {
        "type": "file",
        "size": 23.198,
        "file_type": "py",
        "inclusive_size": 23.198,
        "id": "src\\transformers\\models\\wavlm\\modular_wavlm.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\wavlm\\__init__.py"
      },
      {
        "type": "file",
        "size": 27.47,
        "file_type": "py",
        "inclusive_size": 27.47,
        "id": "src\\transformers\\models\\wav2vec2_with_lm\\processing_wav2vec2_with_lm.py"
      },
      {
        "type": "file",
        "size": 0.965,
        "file_type": "py",
        "inclusive_size": 0.965,
        "id": "src\\transformers\\models\\wav2vec2_with_lm\\__init__.py"
      },
      {
        "type": "file",
        "size": 22.848,
        "file_type": "py",
        "inclusive_size": 22.848,
        "id": "src\\transformers\\models\\wav2vec2_phoneme\\tokenization_wav2vec2_phoneme.py"
      },
      {
        "type": "file",
        "size": 0.967,
        "file_type": "py",
        "inclusive_size": 0.967,
        "id": "src\\transformers\\models\\wav2vec2_phoneme\\__init__.py"
      },
      {
        "type": "file",
        "size": 20.965,
        "file_type": "py",
        "inclusive_size": 20.965,
        "id": "src\\transformers\\models\\wav2vec2_conformer\\configuration_wav2vec2_conformer.py"
      },
      {
        "type": "file",
        "size": 13.339,
        "file_type": "py",
        "inclusive_size": 13.339,
        "id": "src\\transformers\\models\\wav2vec2_conformer\\convert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 85.967,
        "file_type": "py",
        "inclusive_size": 85.967,
        "id": "src\\transformers\\models\\wav2vec2_conformer\\modeling_wav2vec2_conformer.py"
      },
      {
        "type": "file",
        "size": 30.722,
        "file_type": "py",
        "inclusive_size": 30.722,
        "id": "src\\transformers\\models\\wav2vec2_conformer\\modular_wav2vec2_conformer.py"
      },
      {
        "type": "file",
        "size": 1.017,
        "file_type": "py",
        "inclusive_size": 1.017,
        "id": "src\\transformers\\models\\wav2vec2_conformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 18.169,
        "file_type": "py",
        "inclusive_size": 18.169,
        "id": "src\\transformers\\models\\wav2vec2_bert\\configuration_wav2vec2_bert.py"
      },
      {
        "type": "file",
        "size": 7.355,
        "file_type": "py",
        "inclusive_size": 7.355,
        "id": "src\\transformers\\models\\wav2vec2_bert\\convert_wav2vec2_seamless_checkpoint.py"
      },
      {
        "type": "file",
        "size": 66.544,
        "file_type": "py",
        "inclusive_size": 66.544,
        "id": "src\\transformers\\models\\wav2vec2_bert\\modeling_wav2vec2_bert.py"
      },
      {
        "type": "file",
        "size": 45.198,
        "file_type": "py",
        "inclusive_size": 45.198,
        "id": "src\\transformers\\models\\wav2vec2_bert\\modular_wav2vec2_bert.py"
      },
      {
        "type": "file",
        "size": 4.23,
        "file_type": "py",
        "inclusive_size": 4.23,
        "id": "src\\transformers\\models\\wav2vec2_bert\\processing_wav2vec2_bert.py"
      },
      {
        "type": "file",
        "size": 1.051,
        "file_type": "py",
        "inclusive_size": 1.051,
        "id": "src\\transformers\\models\\wav2vec2_bert\\__init__.py"
      },
      {
        "type": "file",
        "size": 20.104,
        "file_type": "py",
        "inclusive_size": 20.104,
        "id": "src\\transformers\\models\\wav2vec2\\configuration_wav2vec2.py"
      },
      {
        "type": "file",
        "size": 15.11,
        "file_type": "py",
        "inclusive_size": 15.11,
        "id": "src\\transformers\\models\\wav2vec2\\convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 4.841,
        "file_type": "py",
        "inclusive_size": 4.841,
        "id": "src\\transformers\\models\\wav2vec2\\convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 11.461,
        "file_type": "py",
        "inclusive_size": 11.461,
        "id": "src\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py"
      },
      {
        "type": "file",
        "size": 92.182,
        "file_type": "py",
        "inclusive_size": 92.182,
        "id": "src\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py"
      },
      {
        "type": "file",
        "size": 4.105,
        "file_type": "py",
        "inclusive_size": 4.105,
        "id": "src\\transformers\\models\\wav2vec2\\processing_wav2vec2.py"
      },
      {
        "type": "file",
        "size": 28.843,
        "file_type": "py",
        "inclusive_size": 28.843,
        "id": "src\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py"
      },
      {
        "type": "file",
        "size": 1.124,
        "file_type": "py",
        "inclusive_size": 1.124,
        "id": "src\\transformers\\models\\wav2vec2\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.41,
        "file_type": "py",
        "inclusive_size": 8.41,
        "id": "src\\transformers\\models\\voxtral\\configuration_voxtral.py"
      },
      {
        "type": "file",
        "size": 11.988,
        "file_type": "py",
        "inclusive_size": 11.988,
        "id": "src\\transformers\\models\\voxtral\\convert_voxtral_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 21.764,
        "file_type": "py",
        "inclusive_size": 21.764,
        "id": "src\\transformers\\models\\voxtral\\modeling_voxtral.py"
      },
      {
        "type": "file",
        "size": 11.689,
        "file_type": "py",
        "inclusive_size": 11.689,
        "id": "src\\transformers\\models\\voxtral\\modular_voxtral.py"
      },
      {
        "type": "file",
        "size": 18.453,
        "file_type": "py",
        "inclusive_size": 18.453,
        "id": "src\\transformers\\models\\voxtral\\processing_voxtral.py"
      },
      {
        "type": "file",
        "size": 1.038,
        "file_type": "py",
        "inclusive_size": 1.038,
        "id": "src\\transformers\\models\\voxtral\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.04,
        "file_type": "py",
        "inclusive_size": 7.04,
        "id": "src\\transformers\\models\\vjepa2\\configuration_vjepa2.py"
      },
      {
        "type": "file",
        "size": 8.636,
        "file_type": "py",
        "inclusive_size": 8.636,
        "id": "src\\transformers\\models\\vjepa2\\convert_vjepa2_classifier_to_hf.py"
      },
      {
        "type": "file",
        "size": 13.719,
        "file_type": "py",
        "inclusive_size": 13.719,
        "id": "src\\transformers\\models\\vjepa2\\convert_vjepa2_to_hf.py"
      },
      {
        "type": "file",
        "size": 44.812,
        "file_type": "py",
        "inclusive_size": 44.812,
        "id": "src\\transformers\\models\\vjepa2\\modeling_vjepa2.py"
      },
      {
        "type": "file",
        "size": 1.739,
        "file_type": "py",
        "inclusive_size": 1.739,
        "id": "src\\transformers\\models\\vjepa2\\video_processing_vjepa2.py"
      },
      {
        "type": "file",
        "size": 1.041,
        "file_type": "py",
        "inclusive_size": 1.041,
        "id": "src\\transformers\\models\\vjepa2\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.127,
        "file_type": "py",
        "inclusive_size": 5.127,
        "id": "src\\transformers\\models\\vivit\\configuration_vivit.py"
      },
      {
        "type": "file",
        "size": 9.085,
        "file_type": "py",
        "inclusive_size": 9.085,
        "id": "src\\transformers\\models\\vivit\\convert_vivit_flax_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 18.761,
        "file_type": "py",
        "inclusive_size": 18.761,
        "id": "src\\transformers\\models\\vivit\\image_processing_vivit.py"
      },
      {
        "type": "file",
        "size": 25.882,
        "file_type": "py",
        "inclusive_size": 25.882,
        "id": "src\\transformers\\models\\vivit\\modeling_vivit.py"
      },
      {
        "type": "file",
        "size": 1.033,
        "file_type": "py",
        "inclusive_size": 1.033,
        "id": "src\\transformers\\models\\vivit\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.849,
        "file_type": "py",
        "inclusive_size": 4.849,
        "id": "src\\transformers\\models\\vit_msn\\configuration_vit_msn.py"
      },
      {
        "type": "file",
        "size": 9.887,
        "file_type": "py",
        "inclusive_size": 9.887,
        "id": "src\\transformers\\models\\vit_msn\\convert_msn_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 21.83,
        "file_type": "py",
        "inclusive_size": 21.83,
        "id": "src\\transformers\\models\\vit_msn\\modeling_vit_msn.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\vit_msn\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.357,
        "file_type": "py",
        "inclusive_size": 6.357,
        "id": "src\\transformers\\models\\vit_mae\\configuration_vit_mae.py"
      },
      {
        "type": "file",
        "size": 7.555,
        "file_type": "py",
        "inclusive_size": 7.555,
        "id": "src\\transformers\\models\\vit_mae\\convert_vit_mae_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 38.28,
        "file_type": "py",
        "inclusive_size": 38.28,
        "id": "src\\transformers\\models\\vit_mae\\modeling_vit_mae.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\vit_mae\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.017,
        "file_type": "py",
        "inclusive_size": 14.017,
        "id": "src\\transformers\\models\\vits\\configuration_vits.py"
      },
      {
        "type": "file",
        "size": 18.61,
        "file_type": "py",
        "inclusive_size": 18.61,
        "id": "src\\transformers\\models\\vits\\convert_original_checkpoint.py"
      },
      {
        "type": "file",
        "size": 61.499,
        "file_type": "py",
        "inclusive_size": 61.499,
        "id": "src\\transformers\\models\\vits\\modeling_vits.py"
      },
      {
        "type": "file",
        "size": 9.402,
        "file_type": "py",
        "inclusive_size": 9.402,
        "id": "src\\transformers\\models\\vits\\tokenization_vits.py"
      },
      {
        "type": "file",
        "size": 1.026,
        "file_type": "py",
        "inclusive_size": 1.026,
        "id": "src\\transformers\\models\\vits\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.636,
        "file_type": "py",
        "inclusive_size": 6.636,
        "id": "src\\transformers\\models\\vitpose_backbone\\configuration_vitpose_backbone.py"
      },
      {
        "type": "file",
        "size": 17.349,
        "file_type": "py",
        "inclusive_size": 17.349,
        "id": "src\\transformers\\models\\vitpose_backbone\\modeling_vitpose_backbone.py"
      },
      {
        "type": "file",
        "size": 0.577,
        "file_type": "py",
        "inclusive_size": 0.577,
        "id": "src\\transformers\\models\\vitpose_backbone\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.814,
        "file_type": "py",
        "inclusive_size": 5.814,
        "id": "src\\transformers\\models\\vitpose\\configuration_vitpose.py"
      },
      {
        "type": "file",
        "size": 15.191,
        "file_type": "py",
        "inclusive_size": 15.191,
        "id": "src\\transformers\\models\\vitpose\\convert_vitpose_to_hf.py"
      },
      {
        "type": "file",
        "size": 29.828,
        "file_type": "py",
        "inclusive_size": 29.828,
        "id": "src\\transformers\\models\\vitpose\\image_processing_vitpose.py"
      },
      {
        "type": "file",
        "size": 11.811,
        "file_type": "py",
        "inclusive_size": 11.811,
        "id": "src\\transformers\\models\\vitpose\\image_processing_vitpose_fast.py"
      },
      {
        "type": "file",
        "size": 11.519,
        "file_type": "py",
        "inclusive_size": 11.519,
        "id": "src\\transformers\\models\\vitpose\\modeling_vitpose.py"
      },
      {
        "type": "file",
        "size": 1.088,
        "file_type": "py",
        "inclusive_size": 1.088,
        "id": "src\\transformers\\models\\vitpose\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.828,
        "file_type": "py",
        "inclusive_size": 5.828,
        "id": "src\\transformers\\models\\vitmatte\\configuration_vitmatte.py"
      },
      {
        "type": "file",
        "size": 6.528,
        "file_type": "py",
        "inclusive_size": 6.528,
        "id": "src\\transformers\\models\\vitmatte\\convert_vitmatte_to_hf.py"
      },
      {
        "type": "file",
        "size": 13.203,
        "file_type": "py",
        "inclusive_size": 13.203,
        "id": "src\\transformers\\models\\vitmatte\\image_processing_vitmatte.py"
      },
      {
        "type": "file",
        "size": 5.831,
        "file_type": "py",
        "inclusive_size": 5.831,
        "id": "src\\transformers\\models\\vitmatte\\image_processing_vitmatte_fast.py"
      },
      {
        "type": "file",
        "size": 10.879,
        "file_type": "py",
        "inclusive_size": 10.879,
        "id": "src\\transformers\\models\\vitmatte\\modeling_vitmatte.py"
      },
      {
        "type": "file",
        "size": 1.092,
        "file_type": "py",
        "inclusive_size": 1.092,
        "id": "src\\transformers\\models\\vitmatte\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.526,
        "file_type": "py",
        "inclusive_size": 7.526,
        "id": "src\\transformers\\models\\vitdet\\configuration_vitdet.py"
      },
      {
        "type": "file",
        "size": 29.841,
        "file_type": "py",
        "inclusive_size": 29.841,
        "id": "src\\transformers\\models\\vitdet\\modeling_vitdet.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\vitdet\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.541,
        "file_type": "py",
        "inclusive_size": 5.541,
        "id": "src\\transformers\\models\\vit\\configuration_vit.py"
      },
      {
        "type": "file",
        "size": 8.905,
        "file_type": "py",
        "inclusive_size": 8.905,
        "id": "src\\transformers\\models\\vit\\convert_dino_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 10.941,
        "file_type": "py",
        "inclusive_size": 10.941,
        "id": "src\\transformers\\models\\vit\\convert_vit_timm_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 14.034,
        "file_type": "py",
        "inclusive_size": 14.034,
        "id": "src\\transformers\\models\\vit\\image_processing_vit.py"
      },
      {
        "type": "file",
        "size": 1.222,
        "file_type": "py",
        "inclusive_size": 1.222,
        "id": "src\\transformers\\models\\vit\\image_processing_vit_fast.py"
      },
      {
        "type": "file",
        "size": 26.015,
        "file_type": "py",
        "inclusive_size": 26.015,
        "id": "src\\transformers\\models\\vit\\modeling_vit.py"
      },
      {
        "type": "file",
        "size": 1.114,
        "file_type": "py",
        "inclusive_size": 1.114,
        "id": "src\\transformers\\models\\vit\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.883,
        "file_type": "py",
        "inclusive_size": 6.883,
        "id": "src\\transformers\\models\\visual_bert\\configuration_visual_bert.py"
      },
      {
        "type": "file",
        "size": 5.161,
        "file_type": "py",
        "inclusive_size": 5.161,
        "id": "src\\transformers\\models\\visual_bert\\convert_visual_bert_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 67.332,
        "file_type": "py",
        "inclusive_size": 67.332,
        "id": "src\\transformers\\models\\visual_bert\\modeling_visual_bert.py"
      },
      {
        "type": "file",
        "size": 1.003,
        "file_type": "py",
        "inclusive_size": 1.003,
        "id": "src\\transformers\\models\\visual_bert\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.008,
        "file_type": "py",
        "inclusive_size": 5.008,
        "id": "src\\transformers\\models\\vision_text_dual_encoder\\configuration_vision_text_dual_encoder.py"
      },
      {
        "type": "file",
        "size": 17.553,
        "file_type": "py",
        "inclusive_size": 17.553,
        "id": "src\\transformers\\models\\vision_text_dual_encoder\\modeling_vision_text_dual_encoder.py"
      },
      {
        "type": "file",
        "size": 1.082,
        "file_type": "py",
        "inclusive_size": 1.082,
        "id": "src\\transformers\\models\\vision_text_dual_encoder\\processing_vision_text_dual_encoder.py"
      },
      {
        "type": "file",
        "size": 1.084,
        "file_type": "py",
        "inclusive_size": 1.084,
        "id": "src\\transformers\\models\\vision_text_dual_encoder\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.685,
        "file_type": "py",
        "inclusive_size": 4.685,
        "id": "src\\transformers\\models\\vision_encoder_decoder\\configuration_vision_encoder_decoder.py"
      },
      {
        "type": "file",
        "size": 22.01,
        "file_type": "py",
        "inclusive_size": 22.01,
        "id": "src\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py"
      },
      {
        "type": "file",
        "size": 1.025,
        "file_type": "py",
        "inclusive_size": 1.025,
        "id": "src\\transformers\\models\\vision_encoder_decoder\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.24,
        "file_type": "py",
        "inclusive_size": 5.24,
        "id": "src\\transformers\\models\\vipllava\\configuration_vipllava.py"
      },
      {
        "type": "file",
        "size": 4.809,
        "file_type": "py",
        "inclusive_size": 4.809,
        "id": "src\\transformers\\models\\vipllava\\convert_vipllava_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 21.216,
        "file_type": "py",
        "inclusive_size": 21.216,
        "id": "src\\transformers\\models\\vipllava\\modeling_vipllava.py"
      },
      {
        "type": "file",
        "size": 13.157,
        "file_type": "py",
        "inclusive_size": 13.157,
        "id": "src\\transformers\\models\\vipllava\\modular_vipllava.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\vipllava\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.935,
        "file_type": "py",
        "inclusive_size": 6.935,
        "id": "src\\transformers\\models\\vilt\\configuration_vilt.py"
      },
      {
        "type": "file",
        "size": 12.961,
        "file_type": "py",
        "inclusive_size": 12.961,
        "id": "src\\transformers\\models\\vilt\\convert_vilt_original_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 22.082,
        "file_type": "py",
        "inclusive_size": 22.082,
        "id": "src\\transformers\\models\\vilt\\image_processing_vilt.py"
      },
      {
        "type": "file",
        "size": 9.012,
        "file_type": "py",
        "inclusive_size": 9.012,
        "id": "src\\transformers\\models\\vilt\\image_processing_vilt_fast.py"
      },
      {
        "type": "file",
        "size": 54.14,
        "file_type": "py",
        "inclusive_size": 54.14,
        "id": "src\\transformers\\models\\vilt\\modeling_vilt.py"
      },
      {
        "type": "file",
        "size": 1.408,
        "file_type": "py",
        "inclusive_size": 1.408,
        "id": "src\\transformers\\models\\vilt\\processing_vilt.py"
      },
      {
        "type": "file",
        "size": 1.154,
        "file_type": "py",
        "inclusive_size": 1.154,
        "id": "src\\transformers\\models\\vilt\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.639,
        "file_type": "py",
        "inclusive_size": 6.639,
        "id": "src\\transformers\\models\\video_llava\\configuration_video_llava.py"
      },
      {
        "type": "file",
        "size": 6.063,
        "file_type": "py",
        "inclusive_size": 6.063,
        "id": "src\\transformers\\models\\video_llava\\convert_video_llava_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 16.825,
        "file_type": "py",
        "inclusive_size": 16.825,
        "id": "src\\transformers\\models\\video_llava\\image_processing_video_llava.py"
      },
      {
        "type": "file",
        "size": 32.655,
        "file_type": "py",
        "inclusive_size": 32.655,
        "id": "src\\transformers\\models\\video_llava\\modeling_video_llava.py"
      },
      {
        "type": "file",
        "size": 7.45,
        "file_type": "py",
        "inclusive_size": 7.45,
        "id": "src\\transformers\\models\\video_llava\\processing_video_llava.py"
      },
      {
        "type": "file",
        "size": 1.32,
        "file_type": "py",
        "inclusive_size": 1.32,
        "id": "src\\transformers\\models\\video_llava\\video_processing_video_llava.py"
      },
      {
        "type": "file",
        "size": 1.093,
        "file_type": "py",
        "inclusive_size": 1.093,
        "id": "src\\transformers\\models\\video_llava\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.275,
        "file_type": "py",
        "inclusive_size": 7.275,
        "id": "src\\transformers\\models\\video_llama_3\\configuration_video_llama_3.py"
      },
      {
        "type": "file",
        "size": 25.849,
        "file_type": "py",
        "inclusive_size": 25.849,
        "id": "src\\transformers\\models\\video_llama_3\\image_processing_video_llama_3.py"
      },
      {
        "type": "file",
        "size": 12.604,
        "file_type": "py",
        "inclusive_size": 12.604,
        "id": "src\\transformers\\models\\video_llama_3\\image_processing_video_llama_3_fast.py"
      },
      {
        "type": "file",
        "size": 50.3,
        "file_type": "py",
        "inclusive_size": 50.3,
        "id": "src\\transformers\\models\\video_llama_3\\modeling_video_llama_3.py"
      },
      {
        "type": "file",
        "size": 74.346,
        "file_type": "py",
        "inclusive_size": 74.346,
        "id": "src\\transformers\\models\\video_llama_3\\modular_video_llama_3.py"
      },
      {
        "type": "file",
        "size": 11.716,
        "file_type": "py",
        "inclusive_size": 11.716,
        "id": "src\\transformers\\models\\video_llama_3\\processing_video_llama_3.py"
      },
      {
        "type": "file",
        "size": 18.355,
        "file_type": "py",
        "inclusive_size": 18.355,
        "id": "src\\transformers\\models\\video_llama_3\\video_processing_video_llama_3.py"
      },
      {
        "type": "file",
        "size": 1.206,
        "file_type": "py",
        "inclusive_size": 1.206,
        "id": "src\\transformers\\models\\video_llama_3\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.585,
        "file_type": "py",
        "inclusive_size": 6.585,
        "id": "src\\transformers\\models\\videomae\\configuration_videomae.py"
      },
      {
        "type": "file",
        "size": 14.027,
        "file_type": "py",
        "inclusive_size": 14.027,
        "id": "src\\transformers\\models\\videomae\\convert_videomae_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 16.317,
        "file_type": "py",
        "inclusive_size": 16.317,
        "id": "src\\transformers\\models\\videomae\\image_processing_videomae.py"
      },
      {
        "type": "file",
        "size": 30.944,
        "file_type": "py",
        "inclusive_size": 30.944,
        "id": "src\\transformers\\models\\videomae\\modeling_videomae.py"
      },
      {
        "type": "file",
        "size": 1.582,
        "file_type": "py",
        "inclusive_size": 1.582,
        "id": "src\\transformers\\models\\videomae\\video_processing_videomae.py"
      },
      {
        "type": "file",
        "size": 1.134,
        "file_type": "py",
        "inclusive_size": 1.134,
        "id": "src\\transformers\\models\\videomae\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.319,
        "file_type": "py",
        "inclusive_size": 10.319,
        "id": "src\\transformers\\models\\vaultgemma\\configuration_vaultgemma.py"
      },
      {
        "type": "file",
        "size": 23.853,
        "file_type": "py",
        "inclusive_size": 23.853,
        "id": "src\\transformers\\models\\vaultgemma\\modeling_vaultgemma.py"
      },
      {
        "type": "file",
        "size": 10.624,
        "file_type": "py",
        "inclusive_size": 10.624,
        "id": "src\\transformers\\models\\vaultgemma\\modular_vaultgemma.py"
      },
      {
        "type": "file",
        "size": 1.002,
        "file_type": "py",
        "inclusive_size": 1.002,
        "id": "src\\transformers\\models\\vaultgemma\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.692,
        "file_type": "py",
        "inclusive_size": 6.692,
        "id": "src\\transformers\\models\\upernet\\configuration_upernet.py"
      },
      {
        "type": "file",
        "size": 10.335,
        "file_type": "py",
        "inclusive_size": 10.335,
        "id": "src\\transformers\\models\\upernet\\convert_convnext_upernet_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 14.09,
        "file_type": "py",
        "inclusive_size": 14.09,
        "id": "src\\transformers\\models\\upernet\\convert_swin_upernet_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 14.161,
        "file_type": "py",
        "inclusive_size": 14.161,
        "id": "src\\transformers\\models\\upernet\\modeling_upernet.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\upernet\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.758,
        "file_type": "py",
        "inclusive_size": 6.758,
        "id": "src\\transformers\\models\\univnet\\configuration_univnet.py"
      },
      {
        "type": "file",
        "size": 6.152,
        "file_type": "py",
        "inclusive_size": 6.152,
        "id": "src\\transformers\\models\\univnet\\convert_univnet.py"
      },
      {
        "type": "file",
        "size": 22.748,
        "file_type": "py",
        "inclusive_size": 22.748,
        "id": "src\\transformers\\models\\univnet\\feature_extraction_univnet.py"
      },
      {
        "type": "file",
        "size": 25.423,
        "file_type": "py",
        "inclusive_size": 25.423,
        "id": "src\\transformers\\models\\univnet\\modeling_univnet.py"
      },
      {
        "type": "file",
        "size": 1.041,
        "file_type": "py",
        "inclusive_size": 1.041,
        "id": "src\\transformers\\models\\univnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 18.882,
        "file_type": "py",
        "inclusive_size": 18.882,
        "id": "src\\transformers\\models\\unispeech_sat\\configuration_unispeech_sat.py"
      },
      {
        "type": "file",
        "size": 4.873,
        "file_type": "py",
        "inclusive_size": 4.873,
        "id": "src\\transformers\\models\\unispeech_sat\\convert_unispeech_original_s3prl_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 9.273,
        "file_type": "py",
        "inclusive_size": 9.273,
        "id": "src\\transformers\\models\\unispeech_sat\\convert_unispeech_sat_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 73.406,
        "file_type": "py",
        "inclusive_size": 73.406,
        "id": "src\\transformers\\models\\unispeech_sat\\modeling_unispeech_sat.py"
      },
      {
        "type": "file",
        "size": 17.959,
        "file_type": "py",
        "inclusive_size": 17.959,
        "id": "src\\transformers\\models\\unispeech_sat\\modular_unispeech_sat.py"
      },
      {
        "type": "file",
        "size": 1.007,
        "file_type": "py",
        "inclusive_size": 1.007,
        "id": "src\\transformers\\models\\unispeech_sat\\__init__.py"
      },
      {
        "type": "file",
        "size": 17.537,
        "file_type": "py",
        "inclusive_size": 17.537,
        "id": "src\\transformers\\models\\unispeech\\configuration_unispeech.py"
      },
      {
        "type": "file",
        "size": 11.297,
        "file_type": "py",
        "inclusive_size": 11.297,
        "id": "src\\transformers\\models\\unispeech\\convert_unispeech_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 60.366,
        "file_type": "py",
        "inclusive_size": 60.366,
        "id": "src\\transformers\\models\\unispeech\\modeling_unispeech.py"
      },
      {
        "type": "file",
        "size": 17.566,
        "file_type": "py",
        "inclusive_size": 17.566,
        "id": "src\\transformers\\models\\unispeech\\modular_unispeech.py"
      },
      {
        "type": "file",
        "size": 0.999,
        "file_type": "py",
        "inclusive_size": 0.999,
        "id": "src\\transformers\\models\\unispeech\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.454,
        "file_type": "py",
        "inclusive_size": 6.454,
        "id": "src\\transformers\\models\\umt5\\configuration_umt5.py"
      },
      {
        "type": "file",
        "size": 12.048,
        "file_type": "py",
        "inclusive_size": 12.048,
        "id": "src\\transformers\\models\\umt5\\convert_umt5_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 81.706,
        "file_type": "py",
        "inclusive_size": 81.706,
        "id": "src\\transformers\\models\\umt5\\modeling_umt5.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\umt5\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.343,
        "file_type": "py",
        "inclusive_size": 8.343,
        "id": "src\\transformers\\models\\udop\\configuration_udop.py"
      },
      {
        "type": "file",
        "size": 33.61,
        "file_type": "py",
        "inclusive_size": 33.61,
        "id": "src\\transformers\\models\\udop\\convert_udop_to_hf.py"
      },
      {
        "type": "file",
        "size": 86.277,
        "file_type": "py",
        "inclusive_size": 86.277,
        "id": "src\\transformers\\models\\udop\\modeling_udop.py"
      },
      {
        "type": "file",
        "size": 7.148,
        "file_type": "py",
        "inclusive_size": 7.148,
        "id": "src\\transformers\\models\\udop\\processing_udop.py"
      },
      {
        "type": "file",
        "size": 49.668,
        "file_type": "py",
        "inclusive_size": 49.668,
        "id": "src\\transformers\\models\\udop\\tokenization_udop.py"
      },
      {
        "type": "file",
        "size": 1.061,
        "file_type": "py",
        "inclusive_size": 1.061,
        "id": "src\\transformers\\models\\udop\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.389,
        "file_type": "py",
        "inclusive_size": 9.389,
        "id": "src\\transformers\\models\\tvp\\configuration_tvp.py"
      },
      {
        "type": "file",
        "size": 22.837,
        "file_type": "py",
        "inclusive_size": 22.837,
        "id": "src\\transformers\\models\\tvp\\image_processing_tvp.py"
      },
      {
        "type": "file",
        "size": 7.866,
        "file_type": "py",
        "inclusive_size": 7.866,
        "id": "src\\transformers\\models\\tvp\\image_processing_tvp_fast.py"
      },
      {
        "type": "file",
        "size": 37.947,
        "file_type": "py",
        "inclusive_size": 37.947,
        "id": "src\\transformers\\models\\tvp\\modeling_tvp.py"
      },
      {
        "type": "file",
        "size": 1.973,
        "file_type": "py",
        "inclusive_size": 1.973,
        "id": "src\\transformers\\models\\tvp\\processing_tvp.py"
      },
      {
        "type": "file",
        "size": 1.106,
        "file_type": "py",
        "inclusive_size": 1.106,
        "id": "src\\transformers\\models\\tvp\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.793,
        "file_type": "py",
        "inclusive_size": 6.793,
        "id": "src\\transformers\\models\\trocr\\configuration_trocr.py"
      },
      {
        "type": "file",
        "size": 10.217,
        "file_type": "py",
        "inclusive_size": 10.217,
        "id": "src\\transformers\\models\\trocr\\convert_trocr_unilm_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 34.772,
        "file_type": "py",
        "inclusive_size": 34.772,
        "id": "src\\transformers\\models\\trocr\\modeling_trocr.py"
      },
      {
        "type": "file",
        "size": 2.376,
        "file_type": "py",
        "inclusive_size": 2.376,
        "id": "src\\transformers\\models\\trocr\\processing_trocr.py"
      },
      {
        "type": "file",
        "size": 1.027,
        "file_type": "py",
        "inclusive_size": 1.027,
        "id": "src\\transformers\\models\\trocr\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.583,
        "file_type": "py",
        "inclusive_size": 5.583,
        "id": "src\\transformers\\models\\timm_wrapper\\configuration_timm_wrapper.py"
      },
      {
        "type": "file",
        "size": 5.298,
        "file_type": "py",
        "inclusive_size": 5.298,
        "id": "src\\transformers\\models\\timm_wrapper\\image_processing_timm_wrapper.py"
      },
      {
        "type": "file",
        "size": 16.993,
        "file_type": "py",
        "inclusive_size": 16.993,
        "id": "src\\transformers\\models\\timm_wrapper\\modeling_timm_wrapper.py"
      },
      {
        "type": "file",
        "size": 1.054,
        "file_type": "py",
        "inclusive_size": 1.054,
        "id": "src\\transformers\\models\\timm_wrapper\\__init__.py"
      },
      {
        "type": "file",
        "size": 3.171,
        "file_type": "py",
        "inclusive_size": 3.171,
        "id": "src\\transformers\\models\\timm_backbone\\configuration_timm_backbone.py"
      },
      {
        "type": "file",
        "size": 7.399,
        "file_type": "py",
        "inclusive_size": 7.399,
        "id": "src\\transformers\\models\\timm_backbone\\modeling_timm_backbone.py"
      },
      {
        "type": "file",
        "size": 1.007,
        "file_type": "py",
        "inclusive_size": 1.007,
        "id": "src\\transformers\\models\\timm_backbone\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.623,
        "file_type": "py",
        "inclusive_size": 11.623,
        "id": "src\\transformers\\models\\time_series_transformer\\configuration_time_series_transformer.py"
      },
      {
        "type": "file",
        "size": 84.338,
        "file_type": "py",
        "inclusive_size": 84.338,
        "id": "src\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py"
      },
      {
        "type": "file",
        "size": 1.027,
        "file_type": "py",
        "inclusive_size": 1.027,
        "id": "src\\transformers\\models\\time_series_transformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.553,
        "file_type": "py",
        "inclusive_size": 5.553,
        "id": "src\\transformers\\models\\timesformer\\configuration_timesformer.py"
      },
      {
        "type": "file",
        "size": 10.198,
        "file_type": "py",
        "inclusive_size": 10.198,
        "id": "src\\transformers\\models\\timesformer\\convert_timesformer_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 31.911,
        "file_type": "py",
        "inclusive_size": 31.911,
        "id": "src\\transformers\\models\\timesformer\\modeling_timesformer.py"
      },
      {
        "type": "file",
        "size": 1.003,
        "file_type": "py",
        "inclusive_size": 1.003,
        "id": "src\\transformers\\models\\timesformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.66,
        "file_type": "py",
        "inclusive_size": 5.66,
        "id": "src\\transformers\\models\\timesfm\\configuration_timesfm.py"
      },
      {
        "type": "file",
        "size": 11.732,
        "file_type": "py",
        "inclusive_size": 11.732,
        "id": "src\\transformers\\models\\timesfm\\convert_timesfm_orignal_to_hf.py"
      },
      {
        "type": "file",
        "size": 34.701,
        "file_type": "py",
        "inclusive_size": 34.701,
        "id": "src\\transformers\\models\\timesfm\\modeling_timesfm.py"
      },
      {
        "type": "file",
        "size": 32.366,
        "file_type": "py",
        "inclusive_size": 32.366,
        "id": "src\\transformers\\models\\timesfm\\modular_timesfm.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\timesfm\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.197,
        "file_type": "py",
        "inclusive_size": 6.197,
        "id": "src\\transformers\\models\\textnet\\configuration_textnet.py"
      },
      {
        "type": "file",
        "size": 8.083,
        "file_type": "py",
        "inclusive_size": 8.083,
        "id": "src\\transformers\\models\\textnet\\convert_textnet_to_hf.py"
      },
      {
        "type": "file",
        "size": 17.488,
        "file_type": "py",
        "inclusive_size": 17.488,
        "id": "src\\transformers\\models\\textnet\\image_processing_textnet.py"
      },
      {
        "type": "file",
        "size": 5.336,
        "file_type": "py",
        "inclusive_size": 5.336,
        "id": "src\\transformers\\models\\textnet\\image_processing_textnet_fast.py"
      },
      {
        "type": "file",
        "size": 15.28,
        "file_type": "py",
        "inclusive_size": 15.28,
        "id": "src\\transformers\\models\\textnet\\modeling_textnet.py"
      },
      {
        "type": "file",
        "size": 1.088,
        "file_type": "py",
        "inclusive_size": 1.088,
        "id": "src\\transformers\\models\\textnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.67,
        "file_type": "py",
        "inclusive_size": 12.67,
        "id": "src\\transformers\\models\\tapas\\configuration_tapas.py"
      },
      {
        "type": "file",
        "size": 11.022,
        "file_type": "py",
        "inclusive_size": 11.022,
        "id": "src\\transformers\\models\\tapas\\convert_tapas_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 98.183,
        "file_type": "py",
        "inclusive_size": 98.183,
        "id": "src\\transformers\\models\\tapas\\modeling_tapas.py"
      },
      {
        "type": "file",
        "size": 119.179,
        "file_type": "py",
        "inclusive_size": 119.179,
        "id": "src\\transformers\\models\\tapas\\tokenization_tapas.py"
      },
      {
        "type": "file",
        "size": 1.029,
        "file_type": "py",
        "inclusive_size": 1.029,
        "id": "src\\transformers\\models\\tapas\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.532,
        "file_type": "py",
        "inclusive_size": 12.532,
        "id": "src\\transformers\\models\\table_transformer\\configuration_table_transformer.py"
      },
      {
        "type": "file",
        "size": 15.097,
        "file_type": "py",
        "inclusive_size": 15.097,
        "id": "src\\transformers\\models\\table_transformer\\convert_table_transformer_to_hf.py"
      },
      {
        "type": "file",
        "size": 21.188,
        "file_type": "py",
        "inclusive_size": 21.188,
        "id": "src\\transformers\\models\\table_transformer\\convert_table_transformer_to_hf_no_timm.py"
      },
      {
        "type": "file",
        "size": 61.225,
        "file_type": "py",
        "inclusive_size": 61.225,
        "id": "src\\transformers\\models\\table_transformer\\modeling_table_transformer.py"
      },
      {
        "type": "file",
        "size": 1.015,
        "file_type": "py",
        "inclusive_size": 1.015,
        "id": "src\\transformers\\models\\table_transformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 32.472,
        "file_type": "py",
        "inclusive_size": 32.472,
        "id": "src\\transformers\\models\\t5gemma2\\configuration_t5gemma2.py"
      },
      {
        "type": "file",
        "size": 68.875,
        "file_type": "py",
        "inclusive_size": 68.875,
        "id": "src\\transformers\\models\\t5gemma2\\modeling_t5gemma2.py"
      },
      {
        "type": "file",
        "size": 71.571,
        "file_type": "py",
        "inclusive_size": 71.571,
        "id": "src\\transformers\\models\\t5gemma2\\modular_t5gemma2.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\t5gemma2\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.723,
        "file_type": "py",
        "inclusive_size": 15.723,
        "id": "src\\transformers\\models\\t5gemma\\configuration_t5gemma.py"
      },
      {
        "type": "file",
        "size": 59.427,
        "file_type": "py",
        "inclusive_size": 59.427,
        "id": "src\\transformers\\models\\t5gemma\\modeling_t5gemma.py"
      },
      {
        "type": "file",
        "size": 59.959,
        "file_type": "py",
        "inclusive_size": 59.959,
        "id": "src\\transformers\\models\\t5gemma\\modular_t5gemma.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\t5gemma\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.862,
        "file_type": "py",
        "inclusive_size": 6.862,
        "id": "src\\transformers\\models\\t5\\configuration_t5.py"
      },
      {
        "type": "file",
        "size": 10.467,
        "file_type": "py",
        "inclusive_size": 10.467,
        "id": "src\\transformers\\models\\t5\\convert_t5x_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 6.681,
        "file_type": "py",
        "inclusive_size": 6.681,
        "id": "src\\transformers\\models\\t5\\convert_t5_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 1.626,
        "file_type": "sh",
        "inclusive_size": 1.626,
        "id": "src\\transformers\\models\\t5\\download_from_gcp.sh"
      },
      {
        "type": "file",
        "size": 73.287,
        "file_type": "py",
        "inclusive_size": 73.287,
        "id": "src\\transformers\\models\\t5\\modeling_t5.py"
      },
      {
        "type": "file",
        "size": 6.388,
        "file_type": "py",
        "inclusive_size": 6.388,
        "id": "src\\transformers\\models\\t5\\tokenization_t5.py"
      },
      {
        "type": "file",
        "size": 1.02,
        "file_type": "py",
        "inclusive_size": 1.02,
        "id": "src\\transformers\\models\\t5\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.318,
        "file_type": "py",
        "inclusive_size": 9.318,
        "id": "src\\transformers\\models\\switch_transformers\\configuration_switch_transformers.py"
      },
      {
        "type": "file",
        "size": 7.608,
        "file_type": "py",
        "inclusive_size": 7.608,
        "id": "src\\transformers\\models\\switch_transformers\\convert_big_switch.py"
      },
      {
        "type": "file",
        "size": 14.519,
        "file_type": "py",
        "inclusive_size": 14.519,
        "id": "src\\transformers\\models\\switch_transformers\\convert_switch_transformers_original_flax_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 57.435,
        "file_type": "py",
        "inclusive_size": 57.435,
        "id": "src\\transformers\\models\\switch_transformers\\modeling_switch_transformers.py"
      },
      {
        "type": "file",
        "size": 41.875,
        "file_type": "py",
        "inclusive_size": 41.875,
        "id": "src\\transformers\\models\\switch_transformers\\modular_switch_transformers.py"
      },
      {
        "type": "file",
        "size": 1.019,
        "file_type": "py",
        "inclusive_size": 1.019,
        "id": "src\\transformers\\models\\switch_transformers\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.532,
        "file_type": "py",
        "inclusive_size": 7.532,
        "id": "src\\transformers\\models\\swinv2\\configuration_swinv2.py"
      },
      {
        "type": "file",
        "size": 7.623,
        "file_type": "py",
        "inclusive_size": 7.623,
        "id": "src\\transformers\\models\\swinv2\\convert_swinv2_timm_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 55.925,
        "file_type": "py",
        "inclusive_size": 55.925,
        "id": "src\\transformers\\models\\swinv2\\modeling_swinv2.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\swinv2\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.826,
        "file_type": "py",
        "inclusive_size": 6.826,
        "id": "src\\transformers\\models\\swin2sr\\configuration_swin2sr.py"
      },
      {
        "type": "file",
        "size": 11.377,
        "file_type": "py",
        "inclusive_size": 11.377,
        "id": "src\\transformers\\models\\swin2sr\\convert_swin2sr_original_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 9.022,
        "file_type": "py",
        "inclusive_size": 9.022,
        "id": "src\\transformers\\models\\swin2sr\\image_processing_swin2sr.py"
      },
      {
        "type": "file",
        "size": 3.581,
        "file_type": "py",
        "inclusive_size": 3.581,
        "id": "src\\transformers\\models\\swin2sr\\image_processing_swin2sr_fast.py"
      },
      {
        "type": "file",
        "size": 44.891,
        "file_type": "py",
        "inclusive_size": 44.891,
        "id": "src\\transformers\\models\\swin2sr\\modeling_swin2sr.py"
      },
      {
        "type": "file",
        "size": 1.088,
        "file_type": "py",
        "inclusive_size": 1.088,
        "id": "src\\transformers\\models\\swin2sr\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.39,
        "file_type": "py",
        "inclusive_size": 7.39,
        "id": "src\\transformers\\models\\swin\\configuration_swin.py"
      },
      {
        "type": "file",
        "size": 6.715,
        "file_type": "py",
        "inclusive_size": 6.715,
        "id": "src\\transformers\\models\\swin\\convert_swin_simmim_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 5.863,
        "file_type": "py",
        "inclusive_size": 5.863,
        "id": "src\\transformers\\models\\swin\\convert_swin_timm_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 51.049,
        "file_type": "py",
        "inclusive_size": 51.049,
        "id": "src\\transformers\\models\\swin\\modeling_swin.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\swin\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.285,
        "file_type": "py",
        "inclusive_size": 5.285,
        "id": "src\\transformers\\models\\swiftformer\\configuration_swiftformer.py"
      },
      {
        "type": "file",
        "size": 6.302,
        "file_type": "py",
        "inclusive_size": 6.302,
        "id": "src\\transformers\\models\\swiftformer\\convert_swiftformer_original_to_hf.py"
      },
      {
        "type": "file",
        "size": 19.362,
        "file_type": "py",
        "inclusive_size": 19.362,
        "id": "src\\transformers\\models\\swiftformer\\modeling_swiftformer.py"
      },
      {
        "type": "file",
        "size": 1.003,
        "file_type": "py",
        "inclusive_size": 1.003,
        "id": "src\\transformers\\models\\swiftformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.048,
        "file_type": "py",
        "inclusive_size": 4.048,
        "id": "src\\transformers\\models\\superpoint\\configuration_superpoint.py"
      },
      {
        "type": "file",
        "size": 7.382,
        "file_type": "py",
        "inclusive_size": 7.382,
        "id": "src\\transformers\\models\\superpoint\\convert_superpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 16.373,
        "file_type": "py",
        "inclusive_size": 16.373,
        "id": "src\\transformers\\models\\superpoint\\image_processing_superpoint.py"
      },
      {
        "type": "file",
        "size": 6.444,
        "file_type": "py",
        "inclusive_size": 6.444,
        "id": "src\\transformers\\models\\superpoint\\image_processing_superpoint_fast.py"
      },
      {
        "type": "file",
        "size": 19.49,
        "file_type": "py",
        "inclusive_size": 19.49,
        "id": "src\\transformers\\models\\superpoint\\modeling_superpoint.py"
      },
      {
        "type": "file",
        "size": 1.1,
        "file_type": "py",
        "inclusive_size": 1.1,
        "id": "src\\transformers\\models\\superpoint\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.659,
        "file_type": "py",
        "inclusive_size": 5.659,
        "id": "src\\transformers\\models\\superglue\\configuration_superglue.py"
      },
      {
        "type": "file",
        "size": 12.987,
        "file_type": "py",
        "inclusive_size": 12.987,
        "id": "src\\transformers\\models\\superglue\\convert_superglue_to_hf.py"
      },
      {
        "type": "file",
        "size": 21.963,
        "file_type": "py",
        "inclusive_size": 21.963,
        "id": "src\\transformers\\models\\superglue\\image_processing_superglue.py"
      },
      {
        "type": "file",
        "size": 12.487,
        "file_type": "py",
        "inclusive_size": 12.487,
        "id": "src\\transformers\\models\\superglue\\image_processing_superglue_fast.py"
      },
      {
        "type": "file",
        "size": 32.597,
        "file_type": "py",
        "inclusive_size": 32.597,
        "id": "src\\transformers\\models\\superglue\\modeling_superglue.py"
      },
      {
        "type": "file",
        "size": 1.096,
        "file_type": "py",
        "inclusive_size": 1.096,
        "id": "src\\transformers\\models\\superglue\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.397,
        "file_type": "py",
        "inclusive_size": 8.397,
        "id": "src\\transformers\\models\\starcoder2\\configuration_starcoder2.py"
      },
      {
        "type": "file",
        "size": 22.494,
        "file_type": "py",
        "inclusive_size": 22.494,
        "id": "src\\transformers\\models\\starcoder2\\modeling_starcoder2.py"
      },
      {
        "type": "file",
        "size": 9.442,
        "file_type": "py",
        "inclusive_size": 9.442,
        "id": "src\\transformers\\models\\starcoder2\\modular_starcoder2.py"
      },
      {
        "type": "file",
        "size": 1.001,
        "file_type": "py",
        "inclusive_size": 1.001,
        "id": "src\\transformers\\models\\starcoder2\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.003,
        "file_type": "py",
        "inclusive_size": 8.003,
        "id": "src\\transformers\\models\\stablelm\\configuration_stablelm.py"
      },
      {
        "type": "file",
        "size": 35.431,
        "file_type": "py",
        "inclusive_size": 35.431,
        "id": "src\\transformers\\models\\stablelm\\modeling_stablelm.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\stablelm\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.062,
        "file_type": "py",
        "inclusive_size": 7.062,
        "id": "src\\transformers\\models\\squeezebert\\configuration_squeezebert.py"
      },
      {
        "type": "file",
        "size": 36.318,
        "file_type": "py",
        "inclusive_size": 36.318,
        "id": "src\\transformers\\models\\squeezebert\\modeling_squeezebert.py"
      },
      {
        "type": "file",
        "size": 1.011,
        "file_type": "py",
        "inclusive_size": 1.011,
        "id": "src\\transformers\\models\\squeezebert\\tokenization_squeezebert.py"
      },
      {
        "type": "file",
        "size": 1.092,
        "file_type": "py",
        "inclusive_size": 1.092,
        "id": "src\\transformers\\models\\squeezebert\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.478,
        "file_type": "py",
        "inclusive_size": 5.478,
        "id": "src\\transformers\\models\\splinter\\configuration_splinter.py"
      },
      {
        "type": "file",
        "size": 34.56,
        "file_type": "py",
        "inclusive_size": 34.56,
        "id": "src\\transformers\\models\\splinter\\modeling_splinter.py"
      },
      {
        "type": "file",
        "size": 6.692,
        "file_type": "py",
        "inclusive_size": 6.692,
        "id": "src\\transformers\\models\\splinter\\tokenization_splinter.py"
      },
      {
        "type": "file",
        "size": 1.084,
        "file_type": "py",
        "inclusive_size": 1.084,
        "id": "src\\transformers\\models\\splinter\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.987,
        "file_type": "py",
        "inclusive_size": 9.987,
        "id": "src\\transformers\\models\\speech_to_text\\configuration_speech_to_text.py"
      },
      {
        "type": "file",
        "size": 4.497,
        "file_type": "py",
        "inclusive_size": 4.497,
        "id": "src\\transformers\\models\\speech_to_text\\convert_s2t_fairseq_to_tfms.py"
      },
      {
        "type": "file",
        "size": 13.798,
        "file_type": "py",
        "inclusive_size": 13.798,
        "id": "src\\transformers\\models\\speech_to_text\\feature_extraction_speech_to_text.py"
      },
      {
        "type": "file",
        "size": 51.452,
        "file_type": "py",
        "inclusive_size": 51.452,
        "id": "src\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py"
      },
      {
        "type": "file",
        "size": 1.77,
        "file_type": "py",
        "inclusive_size": 1.77,
        "id": "src\\transformers\\models\\speech_to_text\\processing_speech_to_text.py"
      },
      {
        "type": "file",
        "size": 11.455,
        "file_type": "py",
        "inclusive_size": 11.455,
        "id": "src\\transformers\\models\\speech_to_text\\tokenization_speech_to_text.py"
      },
      {
        "type": "file",
        "size": 1.154,
        "file_type": "py",
        "inclusive_size": 1.154,
        "id": "src\\transformers\\models\\speech_to_text\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.678,
        "file_type": "py",
        "inclusive_size": 4.678,
        "id": "src\\transformers\\models\\speech_encoder_decoder\\configuration_speech_encoder_decoder.py"
      },
      {
        "type": "file",
        "size": 14.737,
        "file_type": "py",
        "inclusive_size": 14.737,
        "id": "src\\transformers\\models\\speech_encoder_decoder\\convert_mbart_wav2vec2_seq2seq_original_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 11.955,
        "file_type": "py",
        "inclusive_size": 11.955,
        "id": "src\\transformers\\models\\speech_encoder_decoder\\convert_speech_to_text_wav2vec2_seq2seq_original_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 24.6,
        "file_type": "py",
        "inclusive_size": 24.6,
        "id": "src\\transformers\\models\\speech_encoder_decoder\\modeling_speech_encoder_decoder.py"
      },
      {
        "type": "file",
        "size": 1.025,
        "file_type": "py",
        "inclusive_size": 1.025,
        "id": "src\\transformers\\models\\speech_encoder_decoder\\__init__.py"
      },
      {
        "type": "file",
        "size": 23.513,
        "file_type": "py",
        "inclusive_size": 23.513,
        "id": "src\\transformers\\models\\speecht5\\configuration_speecht5.py"
      },
      {
        "type": "file",
        "size": 4.253,
        "file_type": "py",
        "inclusive_size": 4.253,
        "id": "src\\transformers\\models\\speecht5\\convert_hifigan.py"
      },
      {
        "type": "file",
        "size": 17.207,
        "file_type": "py",
        "inclusive_size": 17.207,
        "id": "src\\transformers\\models\\speecht5\\convert_speecht5_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 16.724,
        "file_type": "py",
        "inclusive_size": 16.724,
        "id": "src\\transformers\\models\\speecht5\\feature_extraction_speecht5.py"
      },
      {
        "type": "file",
        "size": 140.176,
        "file_type": "py",
        "inclusive_size": 140.176,
        "id": "src\\transformers\\models\\speecht5\\modeling_speecht5.py"
      },
      {
        "type": "file",
        "size": 7.004,
        "file_type": "py",
        "inclusive_size": 7.004,
        "id": "src\\transformers\\models\\speecht5\\number_normalizer.py"
      },
      {
        "type": "file",
        "size": 5.373,
        "file_type": "py",
        "inclusive_size": 5.373,
        "id": "src\\transformers\\models\\speecht5\\processing_speecht5.py"
      },
      {
        "type": "file",
        "size": 6.688,
        "file_type": "py",
        "inclusive_size": 6.688,
        "id": "src\\transformers\\models\\speecht5\\tokenization_speecht5.py"
      },
      {
        "type": "file",
        "size": 1.124,
        "file_type": "py",
        "inclusive_size": 1.124,
        "id": "src\\transformers\\models\\speecht5\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.006,
        "file_type": "py",
        "inclusive_size": 9.006,
        "id": "src\\transformers\\models\\solar_open\\configuration_solar_open.py"
      },
      {
        "type": "file",
        "size": 28.119,
        "file_type": "py",
        "inclusive_size": 28.119,
        "id": "src\\transformers\\models\\solar_open\\modeling_solar_open.py"
      },
      {
        "type": "file",
        "size": 9.014,
        "file_type": "py",
        "inclusive_size": 9.014,
        "id": "src\\transformers\\models\\solar_open\\modular_solar_open.py"
      },
      {
        "type": "file",
        "size": 1.001,
        "file_type": "py",
        "inclusive_size": 1.001,
        "id": "src\\transformers\\models\\solar_open\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.233,
        "file_type": "py",
        "inclusive_size": 9.233,
        "id": "src\\transformers\\models\\smolvlm\\configuration_smolvlm.py"
      },
      {
        "type": "file",
        "size": 44.704,
        "file_type": "py",
        "inclusive_size": 44.704,
        "id": "src\\transformers\\models\\smolvlm\\image_processing_smolvlm.py"
      },
      {
        "type": "file",
        "size": 23.768,
        "file_type": "py",
        "inclusive_size": 23.768,
        "id": "src\\transformers\\models\\smolvlm\\image_processing_smolvlm_fast.py"
      },
      {
        "type": "file",
        "size": 39.668,
        "file_type": "py",
        "inclusive_size": 39.668,
        "id": "src\\transformers\\models\\smolvlm\\modeling_smolvlm.py"
      },
      {
        "type": "file",
        "size": 19.479,
        "file_type": "py",
        "inclusive_size": 19.479,
        "id": "src\\transformers\\models\\smolvlm\\modular_smolvlm.py"
      },
      {
        "type": "file",
        "size": 15.692,
        "file_type": "py",
        "inclusive_size": 15.692,
        "id": "src\\transformers\\models\\smolvlm\\processing_smolvlm.py"
      },
      {
        "type": "file",
        "size": 14.695,
        "file_type": "py",
        "inclusive_size": 14.695,
        "id": "src\\transformers\\models\\smolvlm\\video_processing_smolvlm.py"
      },
      {
        "type": "file",
        "size": 1.126,
        "file_type": "py",
        "inclusive_size": 1.126,
        "id": "src\\transformers\\models\\smolvlm\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.686,
        "file_type": "py",
        "inclusive_size": 10.686,
        "id": "src\\transformers\\models\\smollm3\\configuration_smollm3.py"
      },
      {
        "type": "file",
        "size": 23.409,
        "file_type": "py",
        "inclusive_size": 23.409,
        "id": "src\\transformers\\models\\smollm3\\modeling_smollm3.py"
      },
      {
        "type": "file",
        "size": 13.617,
        "file_type": "py",
        "inclusive_size": 13.617,
        "id": "src\\transformers\\models\\smollm3\\modular_smollm3.py"
      },
      {
        "type": "file",
        "size": 1.0,
        "file_type": "py",
        "inclusive_size": 1.0,
        "id": "src\\transformers\\models\\smollm3\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.052,
        "file_type": "py",
        "inclusive_size": 13.052,
        "id": "src\\transformers\\models\\siglip2\\configuration_siglip2.py"
      },
      {
        "type": "file",
        "size": 20.305,
        "file_type": "py",
        "inclusive_size": 20.305,
        "id": "src\\transformers\\models\\siglip2\\convert_siglip2_to_hf.py"
      },
      {
        "type": "file",
        "size": 16.314,
        "file_type": "py",
        "inclusive_size": 16.314,
        "id": "src\\transformers\\models\\siglip2\\image_processing_siglip2.py"
      },
      {
        "type": "file",
        "size": 5.65,
        "file_type": "py",
        "inclusive_size": 5.65,
        "id": "src\\transformers\\models\\siglip2\\image_processing_siglip2_fast.py"
      },
      {
        "type": "file",
        "size": 46.139,
        "file_type": "py",
        "inclusive_size": 46.139,
        "id": "src\\transformers\\models\\siglip2\\modeling_siglip2.py"
      },
      {
        "type": "file",
        "size": 26.483,
        "file_type": "py",
        "inclusive_size": 26.483,
        "id": "src\\transformers\\models\\siglip2\\modular_siglip2.py"
      },
      {
        "type": "file",
        "size": 1.315,
        "file_type": "py",
        "inclusive_size": 1.315,
        "id": "src\\transformers\\models\\siglip2\\processing_siglip2.py"
      },
      {
        "type": "file",
        "size": 3.75,
        "file_type": "py",
        "inclusive_size": 3.75,
        "id": "src\\transformers\\models\\siglip2\\tokenization_siglip2.py"
      },
      {
        "type": "file",
        "size": 1.166,
        "file_type": "py",
        "inclusive_size": 1.166,
        "id": "src\\transformers\\models\\siglip2\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.928,
        "file_type": "py",
        "inclusive_size": 11.928,
        "id": "src\\transformers\\models\\siglip\\configuration_siglip.py"
      },
      {
        "type": "file",
        "size": 25.673,
        "file_type": "py",
        "inclusive_size": 25.673,
        "id": "src\\transformers\\models\\siglip\\convert_siglip_to_hf.py"
      },
      {
        "type": "file",
        "size": 11.69,
        "file_type": "py",
        "inclusive_size": 11.69,
        "id": "src\\transformers\\models\\siglip\\image_processing_siglip.py"
      },
      {
        "type": "file",
        "size": 1.242,
        "file_type": "py",
        "inclusive_size": 1.242,
        "id": "src\\transformers\\models\\siglip\\image_processing_siglip_fast.py"
      },
      {
        "type": "file",
        "size": 39.925,
        "file_type": "py",
        "inclusive_size": 39.925,
        "id": "src\\transformers\\models\\siglip\\modeling_siglip.py"
      },
      {
        "type": "file",
        "size": 0.915,
        "file_type": "py",
        "inclusive_size": 0.915,
        "id": "src\\transformers\\models\\siglip\\processing_siglip.py"
      },
      {
        "type": "file",
        "size": 14.286,
        "file_type": "py",
        "inclusive_size": 14.286,
        "id": "src\\transformers\\models\\siglip\\tokenization_siglip.py"
      },
      {
        "type": "file",
        "size": 1.16,
        "file_type": "py",
        "inclusive_size": 1.16,
        "id": "src\\transformers\\models\\siglip\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.79,
        "file_type": "py",
        "inclusive_size": 4.79,
        "id": "src\\transformers\\models\\shieldgemma2\\configuration_shieldgemma2.py"
      },
      {
        "type": "file",
        "size": 19.395,
        "file_type": "py",
        "inclusive_size": 19.395,
        "id": "src\\transformers\\models\\shieldgemma2\\convert_shieldgemma2_weights_orbax_to_hf.py"
      },
      {
        "type": "file",
        "size": 5.745,
        "file_type": "py",
        "inclusive_size": 5.745,
        "id": "src\\transformers\\models\\shieldgemma2\\modeling_shieldgemma2.py"
      },
      {
        "type": "file",
        "size": 8.432,
        "file_type": "py",
        "inclusive_size": 8.432,
        "id": "src\\transformers\\models\\shieldgemma2\\processing_shieldgemma2.py"
      },
      {
        "type": "file",
        "size": 1.048,
        "file_type": "py",
        "inclusive_size": 1.048,
        "id": "src\\transformers\\models\\shieldgemma2\\__init__.py"
      },
      {
        "type": "file",
        "size": 16.218,
        "file_type": "py",
        "inclusive_size": 16.218,
        "id": "src\\transformers\\models\\sew_d\\configuration_sew_d.py"
      },
      {
        "type": "file",
        "size": 13.532,
        "file_type": "py",
        "inclusive_size": 13.532,
        "id": "src\\transformers\\models\\sew_d\\convert_sew_d_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 67.84,
        "file_type": "py",
        "inclusive_size": 67.84,
        "id": "src\\transformers\\models\\sew_d\\modeling_sew_d.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\sew_d\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.258,
        "file_type": "py",
        "inclusive_size": 14.258,
        "id": "src\\transformers\\models\\sew\\configuration_sew.py"
      },
      {
        "type": "file",
        "size": 12.702,
        "file_type": "py",
        "inclusive_size": 12.702,
        "id": "src\\transformers\\models\\sew\\convert_sew_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 46.324,
        "file_type": "py",
        "inclusive_size": 46.324,
        "id": "src\\transformers\\models\\sew\\modeling_sew.py"
      },
      {
        "type": "file",
        "size": 18.366,
        "file_type": "py",
        "inclusive_size": 18.366,
        "id": "src\\transformers\\models\\sew\\modular_sew.py"
      },
      {
        "type": "file",
        "size": 0.987,
        "file_type": "py",
        "inclusive_size": 0.987,
        "id": "src\\transformers\\models\\sew\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.477,
        "file_type": "py",
        "inclusive_size": 6.477,
        "id": "src\\transformers\\models\\seggpt\\configuration_seggpt.py"
      },
      {
        "type": "file",
        "size": 9.932,
        "file_type": "py",
        "inclusive_size": 9.932,
        "id": "src\\transformers\\models\\seggpt\\convert_seggpt_to_hf.py"
      },
      {
        "type": "file",
        "size": 30.82,
        "file_type": "py",
        "inclusive_size": 30.82,
        "id": "src\\transformers\\models\\seggpt\\image_processing_seggpt.py"
      },
      {
        "type": "file",
        "size": 43.402,
        "file_type": "py",
        "inclusive_size": 43.402,
        "id": "src\\transformers\\models\\seggpt\\modeling_seggpt.py"
      },
      {
        "type": "file",
        "size": 1.036,
        "file_type": "py",
        "inclusive_size": 1.036,
        "id": "src\\transformers\\models\\seggpt\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.392,
        "file_type": "py",
        "inclusive_size": 6.392,
        "id": "src\\transformers\\models\\segformer\\configuration_segformer.py"
      },
      {
        "type": "file",
        "size": 17.182,
        "file_type": "py",
        "inclusive_size": 17.182,
        "id": "src\\transformers\\models\\segformer\\convert_segformer_original_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 22.215,
        "file_type": "py",
        "inclusive_size": 22.215,
        "id": "src\\transformers\\models\\segformer\\image_processing_segformer.py"
      },
      {
        "type": "file",
        "size": 9.363,
        "file_type": "py",
        "inclusive_size": 9.363,
        "id": "src\\transformers\\models\\segformer\\image_processing_segformer_fast.py"
      },
      {
        "type": "file",
        "size": 28.991,
        "file_type": "py",
        "inclusive_size": 28.991,
        "id": "src\\transformers\\models\\segformer\\modeling_segformer.py"
      },
      {
        "type": "file",
        "size": 5.636,
        "file_type": "py",
        "inclusive_size": 5.636,
        "id": "src\\transformers\\models\\segformer\\modular_segformer.py"
      },
      {
        "type": "file",
        "size": 1.144,
        "file_type": "py",
        "inclusive_size": 1.144,
        "id": "src\\transformers\\models\\segformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.288,
        "file_type": "py",
        "inclusive_size": 9.288,
        "id": "src\\transformers\\models\\seed_oss\\configuration_seed_oss.py"
      },
      {
        "type": "file",
        "size": 22.992,
        "file_type": "py",
        "inclusive_size": 22.992,
        "id": "src\\transformers\\models\\seed_oss\\modeling_seed_oss.py"
      },
      {
        "type": "file",
        "size": 7.493,
        "file_type": "py",
        "inclusive_size": 7.493,
        "id": "src\\transformers\\models\\seed_oss\\modular_seed_oss.py"
      },
      {
        "type": "file",
        "size": 1.025,
        "file_type": "py",
        "inclusive_size": 1.025,
        "id": "src\\transformers\\models\\seed_oss\\__init__.py"
      },
      {
        "type": "file",
        "size": 24.437,
        "file_type": "py",
        "inclusive_size": 24.437,
        "id": "src\\transformers\\models\\seamless_m4t_v2\\configuration_seamless_m4t_v2.py"
      },
      {
        "type": "file",
        "size": 15.07,
        "file_type": "py",
        "inclusive_size": 15.07,
        "id": "src\\transformers\\models\\seamless_m4t_v2\\convert_fairseq2_to_hf.py"
      },
      {
        "type": "file",
        "size": 202.931,
        "file_type": "py",
        "inclusive_size": 202.931,
        "id": "src\\transformers\\models\\seamless_m4t_v2\\modeling_seamless_m4t_v2.py"
      },
      {
        "type": "file",
        "size": 1.011,
        "file_type": "py",
        "inclusive_size": 1.011,
        "id": "src\\transformers\\models\\seamless_m4t_v2\\__init__.py"
      },
      {
        "type": "file",
        "size": 23.57,
        "file_type": "py",
        "inclusive_size": 23.57,
        "id": "src\\transformers\\models\\seamless_m4t\\configuration_seamless_m4t.py"
      },
      {
        "type": "file",
        "size": 15.946,
        "file_type": "py",
        "inclusive_size": 15.946,
        "id": "src\\transformers\\models\\seamless_m4t\\convert_fairseq2_to_hf.py"
      },
      {
        "type": "file",
        "size": 13.477,
        "file_type": "py",
        "inclusive_size": 13.477,
        "id": "src\\transformers\\models\\seamless_m4t\\feature_extraction_seamless_m4t.py"
      },
      {
        "type": "file",
        "size": 185.275,
        "file_type": "py",
        "inclusive_size": 185.275,
        "id": "src\\transformers\\models\\seamless_m4t\\modeling_seamless_m4t.py"
      },
      {
        "type": "file",
        "size": 3.119,
        "file_type": "py",
        "inclusive_size": 3.119,
        "id": "src\\transformers\\models\\seamless_m4t\\processing_seamless_m4t.py"
      },
      {
        "type": "file",
        "size": 18.201,
        "file_type": "py",
        "inclusive_size": 18.201,
        "id": "src\\transformers\\models\\seamless_m4t\\tokenization_seamless_m4t.py"
      },
      {
        "type": "file",
        "size": 1.144,
        "file_type": "py",
        "inclusive_size": 1.144,
        "id": "src\\transformers\\models\\seamless_m4t\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.957,
        "file_type": "py",
        "inclusive_size": 14.957,
        "id": "src\\transformers\\models\\sam_hq\\configuration_sam_hq.py"
      },
      {
        "type": "file",
        "size": 10.352,
        "file_type": "py",
        "inclusive_size": 10.352,
        "id": "src\\transformers\\models\\sam_hq\\convert_samhq_to_hf.py"
      },
      {
        "type": "file",
        "size": 68.868,
        "file_type": "py",
        "inclusive_size": 68.868,
        "id": "src\\transformers\\models\\sam_hq\\modeling_sam_hq.py"
      },
      {
        "type": "file",
        "size": 31.372,
        "file_type": "py",
        "inclusive_size": 31.372,
        "id": "src\\transformers\\models\\sam_hq\\modular_sam_hq.py"
      },
      {
        "type": "file",
        "size": 13.385,
        "file_type": "py",
        "inclusive_size": 13.385,
        "id": "src\\transformers\\models\\sam_hq\\processing_sam_hq.py"
      },
      {
        "type": "file",
        "size": 1.03,
        "file_type": "py",
        "inclusive_size": 1.03,
        "id": "src\\transformers\\models\\sam_hq\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.013,
        "file_type": "py",
        "inclusive_size": 12.013,
        "id": "src\\transformers\\models\\sam3_video\\configuration_sam3_video.py"
      },
      {
        "type": "file",
        "size": 39.425,
        "file_type": "py",
        "inclusive_size": 39.425,
        "id": "src\\transformers\\models\\sam3_video\\convert_sam3_video_to_hf.py"
      },
      {
        "type": "file",
        "size": 92.98,
        "file_type": "py",
        "inclusive_size": 92.98,
        "id": "src\\transformers\\models\\sam3_video\\modeling_sam3_video.py"
      },
      {
        "type": "file",
        "size": 17.653,
        "file_type": "py",
        "inclusive_size": 17.653,
        "id": "src\\transformers\\models\\sam3_video\\processing_sam3_video.py"
      },
      {
        "type": "file",
        "size": 1.042,
        "file_type": "py",
        "inclusive_size": 1.042,
        "id": "src\\transformers\\models\\sam3_video\\__init__.py"
      },
      {
        "type": "file",
        "size": 22.17,
        "file_type": "py",
        "inclusive_size": 22.17,
        "id": "src\\transformers\\models\\sam3_tracker_video\\configuration_sam3_tracker_video.py"
      },
      {
        "type": "file",
        "size": 134.951,
        "file_type": "py",
        "inclusive_size": 134.951,
        "id": "src\\transformers\\models\\sam3_tracker_video\\modeling_sam3_tracker_video.py"
      },
      {
        "type": "file",
        "size": 26.543,
        "file_type": "py",
        "inclusive_size": 26.543,
        "id": "src\\transformers\\models\\sam3_tracker_video\\modular_sam3_tracker_video.py"
      },
      {
        "type": "file",
        "size": 37.541,
        "file_type": "py",
        "inclusive_size": 37.541,
        "id": "src\\transformers\\models\\sam3_tracker_video\\processing_sam3_tracker_video.py"
      },
      {
        "type": "file",
        "size": 1.067,
        "file_type": "py",
        "inclusive_size": 1.067,
        "id": "src\\transformers\\models\\sam3_tracker_video\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.293,
        "file_type": "py",
        "inclusive_size": 11.293,
        "id": "src\\transformers\\models\\sam3_tracker\\configuration_sam3_tracker.py"
      },
      {
        "type": "file",
        "size": 53.397,
        "file_type": "py",
        "inclusive_size": 53.397,
        "id": "src\\transformers\\models\\sam3_tracker\\modeling_sam3_tracker.py"
      },
      {
        "type": "file",
        "size": 8.087,
        "file_type": "py",
        "inclusive_size": 8.087,
        "id": "src\\transformers\\models\\sam3_tracker\\modular_sam3_tracker.py"
      },
      {
        "type": "file",
        "size": 23.143,
        "file_type": "py",
        "inclusive_size": 23.143,
        "id": "src\\transformers\\models\\sam3_tracker\\processing_sam3_tracker.py"
      },
      {
        "type": "file",
        "size": 1.049,
        "file_type": "py",
        "inclusive_size": 1.049,
        "id": "src\\transformers\\models\\sam3_tracker\\__init__.py"
      },
      {
        "type": "file",
        "size": 20.945,
        "file_type": "py",
        "inclusive_size": 20.945,
        "id": "src\\transformers\\models\\sam3\\configuration_sam3.py"
      },
      {
        "type": "file",
        "size": 24.967,
        "file_type": "py",
        "inclusive_size": 24.967,
        "id": "src\\transformers\\models\\sam3\\convert_sam3_to_hf.py"
      },
      {
        "type": "file",
        "size": 40.707,
        "file_type": "py",
        "inclusive_size": 40.707,
        "id": "src\\transformers\\models\\sam3\\image_processing_sam3_fast.py"
      },
      {
        "type": "file",
        "size": 101.358,
        "file_type": "py",
        "inclusive_size": 101.358,
        "id": "src\\transformers\\models\\sam3\\modeling_sam3.py"
      },
      {
        "type": "file",
        "size": 11.324,
        "file_type": "py",
        "inclusive_size": 11.324,
        "id": "src\\transformers\\models\\sam3\\modular_sam3.py"
      },
      {
        "type": "file",
        "size": 27.873,
        "file_type": "py",
        "inclusive_size": 27.873,
        "id": "src\\transformers\\models\\sam3\\processing_sam3.py"
      },
      {
        "type": "file",
        "size": 1.024,
        "file_type": "py",
        "inclusive_size": 1.024,
        "id": "src\\transformers\\models\\sam3\\__init__.py"
      },
      {
        "type": "file",
        "size": 20.735,
        "file_type": "py",
        "inclusive_size": 20.735,
        "id": "src\\transformers\\models\\sam2_video\\configuration_sam2_video.py"
      },
      {
        "type": "file",
        "size": 14.017,
        "file_type": "py",
        "inclusive_size": 14.017,
        "id": "src\\transformers\\models\\sam2_video\\convert_sam2_video_to_hf.py"
      },
      {
        "type": "file",
        "size": 133.18,
        "file_type": "py",
        "inclusive_size": 133.18,
        "id": "src\\transformers\\models\\sam2_video\\modeling_sam2_video.py"
      },
      {
        "type": "file",
        "size": 124.67,
        "file_type": "py",
        "inclusive_size": 124.67,
        "id": "src\\transformers\\models\\sam2_video\\modular_sam2_video.py"
      },
      {
        "type": "file",
        "size": 37.447,
        "file_type": "py",
        "inclusive_size": 37.447,
        "id": "src\\transformers\\models\\sam2_video\\processing_sam2_video.py"
      },
      {
        "type": "file",
        "size": 4.868,
        "file_type": "py",
        "inclusive_size": 4.868,
        "id": "src\\transformers\\models\\sam2_video\\video_processing_sam2_video.py"
      },
      {
        "type": "file",
        "size": 1.089,
        "file_type": "py",
        "inclusive_size": 1.089,
        "id": "src\\transformers\\models\\sam2_video\\__init__.py"
      },
      {
        "type": "file",
        "size": 20.62,
        "file_type": "py",
        "inclusive_size": 20.62,
        "id": "src\\transformers\\models\\sam2\\configuration_sam2.py"
      },
      {
        "type": "file",
        "size": 13.5,
        "file_type": "py",
        "inclusive_size": 13.5,
        "id": "src\\transformers\\models\\sam2\\convert_sam2_to_hf.py"
      },
      {
        "type": "file",
        "size": 30.44,
        "file_type": "py",
        "inclusive_size": 30.44,
        "id": "src\\transformers\\models\\sam2\\image_processing_sam2_fast.py"
      },
      {
        "type": "file",
        "size": 72.081,
        "file_type": "py",
        "inclusive_size": 72.081,
        "id": "src\\transformers\\models\\sam2\\modeling_sam2.py"
      },
      {
        "type": "file",
        "size": 64.342,
        "file_type": "py",
        "inclusive_size": 64.342,
        "id": "src\\transformers\\models\\sam2\\modular_sam2.py"
      },
      {
        "type": "file",
        "size": 22.411,
        "file_type": "py",
        "inclusive_size": 22.411,
        "id": "src\\transformers\\models\\sam2\\processing_sam2.py"
      },
      {
        "type": "file",
        "size": 1.07,
        "file_type": "py",
        "inclusive_size": 1.07,
        "id": "src\\transformers\\models\\sam2\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.833,
        "file_type": "py",
        "inclusive_size": 14.833,
        "id": "src\\transformers\\models\\sam\\configuration_sam.py"
      },
      {
        "type": "file",
        "size": 8.6,
        "file_type": "py",
        "inclusive_size": 8.6,
        "id": "src\\transformers\\models\\sam\\convert_sam_to_hf.py"
      },
      {
        "type": "file",
        "size": 54.438,
        "file_type": "py",
        "inclusive_size": 54.438,
        "id": "src\\transformers\\models\\sam\\image_processing_sam.py"
      },
      {
        "type": "file",
        "size": 33.122,
        "file_type": "py",
        "inclusive_size": 33.122,
        "id": "src\\transformers\\models\\sam\\image_processing_sam_fast.py"
      },
      {
        "type": "file",
        "size": 61.212,
        "file_type": "py",
        "inclusive_size": 61.212,
        "id": "src\\transformers\\models\\sam\\modeling_sam.py"
      },
      {
        "type": "file",
        "size": 12.553,
        "file_type": "py",
        "inclusive_size": 12.553,
        "id": "src\\transformers\\models\\sam\\processing_sam.py"
      },
      {
        "type": "file",
        "size": 1.106,
        "file_type": "py",
        "inclusive_size": 1.106,
        "id": "src\\transformers\\models\\sam\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.127,
        "file_type": "py",
        "inclusive_size": 5.127,
        "id": "src\\transformers\\models\\rwkv\\configuration_rwkv.py"
      },
      {
        "type": "file",
        "size": 7.396,
        "file_type": "py",
        "inclusive_size": 7.396,
        "id": "src\\transformers\\models\\rwkv\\convert_rwkv_checkpoint_to_hf.py"
      },
      {
        "type": "file",
        "size": 32.129,
        "file_type": "py",
        "inclusive_size": 32.129,
        "id": "src\\transformers\\models\\rwkv\\modeling_rwkv.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\rwkv\\__init__.py"
      },
      {
        "type": "file",
        "size": 19.322,
        "file_type": "py",
        "inclusive_size": 19.322,
        "id": "src\\transformers\\models\\rt_detr_v2\\configuration_rt_detr_v2.py"
      },
      {
        "type": "file",
        "size": 21.497,
        "file_type": "py",
        "inclusive_size": 21.497,
        "id": "src\\transformers\\models\\rt_detr_v2\\convert_rt_detr_v2_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 96.48,
        "file_type": "py",
        "inclusive_size": 96.48,
        "id": "src\\transformers\\models\\rt_detr_v2\\modeling_rt_detr_v2.py"
      },
      {
        "type": "file",
        "size": 29.696,
        "file_type": "py",
        "inclusive_size": 29.696,
        "id": "src\\transformers\\models\\rt_detr_v2\\modular_rt_detr_v2.py"
      },
      {
        "type": "file",
        "size": 1.003,
        "file_type": "py",
        "inclusive_size": 1.003,
        "id": "src\\transformers\\models\\rt_detr_v2\\__init__.py"
      },
      {
        "type": "file",
        "size": 17.402,
        "file_type": "py",
        "inclusive_size": 17.402,
        "id": "src\\transformers\\models\\rt_detr\\configuration_rt_detr.py"
      },
      {
        "type": "file",
        "size": 5.542,
        "file_type": "py",
        "inclusive_size": 5.542,
        "id": "src\\transformers\\models\\rt_detr\\configuration_rt_detr_resnet.py"
      },
      {
        "type": "file",
        "size": 32.823,
        "file_type": "py",
        "inclusive_size": 32.823,
        "id": "src\\transformers\\models\\rt_detr\\convert_rt_detr_original_pytorch_checkpoint_to_hf.py"
      },
      {
        "type": "file",
        "size": 49.676,
        "file_type": "py",
        "inclusive_size": 49.676,
        "id": "src\\transformers\\models\\rt_detr\\image_processing_rt_detr.py"
      },
      {
        "type": "file",
        "size": 22.443,
        "file_type": "py",
        "inclusive_size": 22.443,
        "id": "src\\transformers\\models\\rt_detr\\image_processing_rt_detr_fast.py"
      },
      {
        "type": "file",
        "size": 94.596,
        "file_type": "py",
        "inclusive_size": 94.596,
        "id": "src\\transformers\\models\\rt_detr\\modeling_rt_detr.py"
      },
      {
        "type": "file",
        "size": 15.532,
        "file_type": "py",
        "inclusive_size": 15.532,
        "id": "src\\transformers\\models\\rt_detr\\modeling_rt_detr_resnet.py"
      },
      {
        "type": "file",
        "size": 13.679,
        "file_type": "py",
        "inclusive_size": 13.679,
        "id": "src\\transformers\\models\\rt_detr\\modular_rt_detr.py"
      },
      {
        "type": "file",
        "size": 1.181,
        "file_type": "py",
        "inclusive_size": 1.181,
        "id": "src\\transformers\\models\\rt_detr\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.517,
        "file_type": "py",
        "inclusive_size": 6.517,
        "id": "src\\transformers\\models\\roformer\\configuration_roformer.py"
      },
      {
        "type": "file",
        "size": 5.239,
        "file_type": "py",
        "inclusive_size": 5.239,
        "id": "src\\transformers\\models\\roformer\\convert_roformer_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 56.671,
        "file_type": "py",
        "inclusive_size": 56.671,
        "id": "src\\transformers\\models\\roformer\\modeling_roformer.py"
      },
      {
        "type": "file",
        "size": 6.246,
        "file_type": "py",
        "inclusive_size": 6.246,
        "id": "src\\transformers\\models\\roformer\\tokenization_roformer.py"
      },
      {
        "type": "file",
        "size": 2.61,
        "file_type": "py",
        "inclusive_size": 2.61,
        "id": "src\\transformers\\models\\roformer\\tokenization_utils.py"
      },
      {
        "type": "file",
        "size": 1.084,
        "file_type": "py",
        "inclusive_size": 1.084,
        "id": "src\\transformers\\models\\roformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.02,
        "file_type": "py",
        "inclusive_size": 8.02,
        "id": "src\\transformers\\models\\roc_bert\\configuration_roc_bert.py"
      },
      {
        "type": "file",
        "size": 74.625,
        "file_type": "py",
        "inclusive_size": 74.625,
        "id": "src\\transformers\\models\\roc_bert\\modeling_roc_bert.py"
      },
      {
        "type": "file",
        "size": 58.67,
        "file_type": "py",
        "inclusive_size": 58.67,
        "id": "src\\transformers\\models\\roc_bert\\tokenization_roc_bert.py"
      },
      {
        "type": "file",
        "size": 1.038,
        "file_type": "py",
        "inclusive_size": 1.038,
        "id": "src\\transformers\\models\\roc_bert\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.592,
        "file_type": "py",
        "inclusive_size": 6.592,
        "id": "src\\transformers\\models\\roberta_prelayernorm\\configuration_roberta_prelayernorm.py"
      },
      {
        "type": "file",
        "size": 2.992,
        "file_type": "py",
        "inclusive_size": 2.992,
        "id": "src\\transformers\\models\\roberta_prelayernorm\\convert_roberta_prelayernorm_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 58.369,
        "file_type": "py",
        "inclusive_size": 58.369,
        "id": "src\\transformers\\models\\roberta_prelayernorm\\modeling_roberta_prelayernorm.py"
      },
      {
        "type": "file",
        "size": 1.021,
        "file_type": "py",
        "inclusive_size": 1.021,
        "id": "src\\transformers\\models\\roberta_prelayernorm\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.196,
        "file_type": "py",
        "inclusive_size": 6.196,
        "id": "src\\transformers\\models\\roberta\\configuration_roberta.py"
      },
      {
        "type": "file",
        "size": 7.99,
        "file_type": "py",
        "inclusive_size": 7.99,
        "id": "src\\transformers\\models\\roberta\\convert_roberta_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 54.657,
        "file_type": "py",
        "inclusive_size": 54.657,
        "id": "src\\transformers\\models\\roberta\\modeling_roberta.py"
      },
      {
        "type": "file",
        "size": 31.676,
        "file_type": "py",
        "inclusive_size": 31.676,
        "id": "src\\transformers\\models\\roberta\\modular_roberta.py"
      },
      {
        "type": "file",
        "size": 7.378,
        "file_type": "py",
        "inclusive_size": 7.378,
        "id": "src\\transformers\\models\\roberta\\tokenization_roberta.py"
      },
      {
        "type": "file",
        "size": 10.935,
        "file_type": "py",
        "inclusive_size": 10.935,
        "id": "src\\transformers\\models\\roberta\\tokenization_roberta_old.py"
      },
      {
        "type": "file",
        "size": 1.035,
        "file_type": "py",
        "inclusive_size": 1.035,
        "id": "src\\transformers\\models\\roberta\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.504,
        "file_type": "py",
        "inclusive_size": 5.504,
        "id": "src\\transformers\\models\\resnet\\configuration_resnet.py"
      },
      {
        "type": "file",
        "size": 6.936,
        "file_type": "py",
        "inclusive_size": 6.936,
        "id": "src\\transformers\\models\\resnet\\convert_resnet_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 16.614,
        "file_type": "py",
        "inclusive_size": 16.614,
        "id": "src\\transformers\\models\\resnet\\modeling_resnet.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\resnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.731,
        "file_type": "py",
        "inclusive_size": 6.731,
        "id": "src\\transformers\\models\\rembert\\configuration_rembert.py"
      },
      {
        "type": "file",
        "size": 5.664,
        "file_type": "py",
        "inclusive_size": 5.664,
        "id": "src\\transformers\\models\\rembert\\convert_rembert_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 49.164,
        "file_type": "py",
        "inclusive_size": 49.164,
        "id": "src\\transformers\\models\\rembert\\modeling_rembert.py"
      },
      {
        "type": "file",
        "size": 7.808,
        "file_type": "py",
        "inclusive_size": 7.808,
        "id": "src\\transformers\\models\\rembert\\tokenization_rembert.py"
      },
      {
        "type": "file",
        "size": 1.035,
        "file_type": "py",
        "inclusive_size": 1.035,
        "id": "src\\transformers\\models\\rembert\\__init__.py"
      },
      {
        "type": "file",
        "size": 3.959,
        "file_type": "py",
        "inclusive_size": 3.959,
        "id": "src\\transformers\\models\\regnet\\configuration_regnet.py"
      },
      {
        "type": "file",
        "size": 11.712,
        "file_type": "py",
        "inclusive_size": 11.712,
        "id": "src\\transformers\\models\\regnet\\convert_regnet_seer_10b_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 18.374,
        "file_type": "py",
        "inclusive_size": 18.374,
        "id": "src\\transformers\\models\\regnet\\convert_regnet_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 14.335,
        "file_type": "py",
        "inclusive_size": 14.335,
        "id": "src\\transformers\\models\\regnet\\modeling_regnet.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\regnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.234,
        "file_type": "py",
        "inclusive_size": 13.234,
        "id": "src\\transformers\\models\\reformer\\configuration_reformer.py"
      },
      {
        "type": "file",
        "size": 8.521,
        "file_type": "py",
        "inclusive_size": 8.521,
        "id": "src\\transformers\\models\\reformer\\convert_reformer_trax_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 113.259,
        "file_type": "py",
        "inclusive_size": 113.259,
        "id": "src\\transformers\\models\\reformer\\modeling_reformer.py"
      },
      {
        "type": "file",
        "size": 4.219,
        "file_type": "py",
        "inclusive_size": 4.219,
        "id": "src\\transformers\\models\\reformer\\tokenization_reformer.py"
      },
      {
        "type": "file",
        "size": 1.038,
        "file_type": "py",
        "inclusive_size": 1.038,
        "id": "src\\transformers\\models\\reformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.391,
        "file_type": "py",
        "inclusive_size": 8.391,
        "id": "src\\transformers\\models\\recurrent_gemma\\configuration_recurrent_gemma.py"
      },
      {
        "type": "file",
        "size": 7.638,
        "file_type": "py",
        "inclusive_size": 7.638,
        "id": "src\\transformers\\models\\recurrent_gemma\\convert_recurrent_gemma_to_hf.py"
      },
      {
        "type": "file",
        "size": 38.609,
        "file_type": "py",
        "inclusive_size": 38.609,
        "id": "src\\transformers\\models\\recurrent_gemma\\modeling_recurrent_gemma.py"
      },
      {
        "type": "file",
        "size": 1.011,
        "file_type": "py",
        "inclusive_size": 1.011,
        "id": "src\\transformers\\models\\recurrent_gemma\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.104,
        "file_type": "py",
        "inclusive_size": 8.104,
        "id": "src\\transformers\\models\\rag\\configuration_rag.py"
      },
      {
        "type": "file",
        "size": 88.599,
        "file_type": "py",
        "inclusive_size": 88.599,
        "id": "src\\transformers\\models\\rag\\modeling_rag.py"
      },
      {
        "type": "file",
        "size": 29.948,
        "file_type": "py",
        "inclusive_size": 29.948,
        "id": "src\\transformers\\models\\rag\\retrieval_rag.py"
      },
      {
        "type": "file",
        "size": 2.784,
        "file_type": "py",
        "inclusive_size": 2.784,
        "id": "src\\transformers\\models\\rag\\tokenization_rag.py"
      },
      {
        "type": "file",
        "size": 1.056,
        "file_type": "py",
        "inclusive_size": 1.056,
        "id": "src\\transformers\\models\\rag\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.433,
        "file_type": "py",
        "inclusive_size": 14.433,
        "id": "src\\transformers\\models\\qwen3_vl_moe\\configuration_qwen3_vl_moe.py"
      },
      {
        "type": "file",
        "size": 86.209,
        "file_type": "py",
        "inclusive_size": 86.209,
        "id": "src\\transformers\\models\\qwen3_vl_moe\\modeling_qwen3_vl_moe.py"
      },
      {
        "type": "file",
        "size": 20.699,
        "file_type": "py",
        "inclusive_size": 20.699,
        "id": "src\\transformers\\models\\qwen3_vl_moe\\modular_qwen3_vl_moe.py"
      },
      {
        "type": "file",
        "size": 1.028,
        "file_type": "py",
        "inclusive_size": 1.028,
        "id": "src\\transformers\\models\\qwen3_vl_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.369,
        "file_type": "py",
        "inclusive_size": 12.369,
        "id": "src\\transformers\\models\\qwen3_vl\\configuration_qwen3_vl.py"
      },
      {
        "type": "file",
        "size": 76.581,
        "file_type": "py",
        "inclusive_size": 76.581,
        "id": "src\\transformers\\models\\qwen3_vl\\modeling_qwen3_vl.py"
      },
      {
        "type": "file",
        "size": 71.222,
        "file_type": "py",
        "inclusive_size": 71.222,
        "id": "src\\transformers\\models\\qwen3_vl\\modular_qwen3_vl.py"
      },
      {
        "type": "file",
        "size": 13.673,
        "file_type": "py",
        "inclusive_size": 13.673,
        "id": "src\\transformers\\models\\qwen3_vl\\processing_qwen3_vl.py"
      },
      {
        "type": "file",
        "size": 11.289,
        "file_type": "py",
        "inclusive_size": 11.289,
        "id": "src\\transformers\\models\\qwen3_vl\\video_processing_qwen3_vl.py"
      },
      {
        "type": "file",
        "size": 1.104,
        "file_type": "py",
        "inclusive_size": 1.104,
        "id": "src\\transformers\\models\\qwen3_vl\\__init__.py"
      },
      {
        "type": "file",
        "size": 58.14,
        "file_type": "py",
        "inclusive_size": 58.14,
        "id": "src\\transformers\\models\\qwen3_omni_moe\\configuration_qwen3_omni_moe.py"
      },
      {
        "type": "file",
        "size": 185.799,
        "file_type": "py",
        "inclusive_size": 185.799,
        "id": "src\\transformers\\models\\qwen3_omni_moe\\modeling_qwen3_omni_moe.py"
      },
      {
        "type": "file",
        "size": 131.279,
        "file_type": "py",
        "inclusive_size": 131.279,
        "id": "src\\transformers\\models\\qwen3_omni_moe\\modular_qwen3_omni_moe.py"
      },
      {
        "type": "file",
        "size": 19.063,
        "file_type": "py",
        "inclusive_size": 19.063,
        "id": "src\\transformers\\models\\qwen3_omni_moe\\processing_qwen3_omni_moe.py"
      },
      {
        "type": "file",
        "size": 1.077,
        "file_type": "py",
        "inclusive_size": 1.077,
        "id": "src\\transformers\\models\\qwen3_omni_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.417,
        "file_type": "py",
        "inclusive_size": 12.417,
        "id": "src\\transformers\\models\\qwen3_next\\configuration_qwen3_next.py"
      },
      {
        "type": "file",
        "size": 56.834,
        "file_type": "py",
        "inclusive_size": 56.834,
        "id": "src\\transformers\\models\\qwen3_next\\modeling_qwen3_next.py"
      },
      {
        "type": "file",
        "size": 38.246,
        "file_type": "py",
        "inclusive_size": 38.246,
        "id": "src\\transformers\\models\\qwen3_next\\modular_qwen3_next.py"
      },
      {
        "type": "file",
        "size": 1.001,
        "file_type": "py",
        "inclusive_size": 1.001,
        "id": "src\\transformers\\models\\qwen3_next\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.579,
        "file_type": "py",
        "inclusive_size": 10.579,
        "id": "src\\transformers\\models\\qwen3_moe\\configuration_qwen3_moe.py"
      },
      {
        "type": "file",
        "size": 32.692,
        "file_type": "py",
        "inclusive_size": 32.692,
        "id": "src\\transformers\\models\\qwen3_moe\\modeling_qwen3_moe.py"
      },
      {
        "type": "file",
        "size": 7.391,
        "file_type": "py",
        "inclusive_size": 7.391,
        "id": "src\\transformers\\models\\qwen3_moe\\modular_qwen3_moe.py"
      },
      {
        "type": "file",
        "size": 0.999,
        "file_type": "py",
        "inclusive_size": 0.999,
        "id": "src\\transformers\\models\\qwen3_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.433,
        "file_type": "py",
        "inclusive_size": 9.433,
        "id": "src\\transformers\\models\\qwen3\\configuration_qwen3.py"
      },
      {
        "type": "file",
        "size": 24.069,
        "file_type": "py",
        "inclusive_size": 24.069,
        "id": "src\\transformers\\models\\qwen3\\modeling_qwen3.py"
      },
      {
        "type": "file",
        "size": 5.965,
        "file_type": "py",
        "inclusive_size": 5.965,
        "id": "src\\transformers\\models\\qwen3\\modular_qwen3.py"
      },
      {
        "type": "file",
        "size": 1.014,
        "file_type": "py",
        "inclusive_size": 1.014,
        "id": "src\\transformers\\models\\qwen3\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.575,
        "file_type": "py",
        "inclusive_size": 14.575,
        "id": "src\\transformers\\models\\qwen2_vl\\configuration_qwen2_vl.py"
      },
      {
        "type": "file",
        "size": 24.667,
        "file_type": "py",
        "inclusive_size": 24.667,
        "id": "src\\transformers\\models\\qwen2_vl\\image_processing_qwen2_vl.py"
      },
      {
        "type": "file",
        "size": 10.537,
        "file_type": "py",
        "inclusive_size": 10.537,
        "id": "src\\transformers\\models\\qwen2_vl\\image_processing_qwen2_vl_fast.py"
      },
      {
        "type": "file",
        "size": 76.578,
        "file_type": "py",
        "inclusive_size": 76.578,
        "id": "src\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py"
      },
      {
        "type": "file",
        "size": 9.518,
        "file_type": "py",
        "inclusive_size": 9.518,
        "id": "src\\transformers\\models\\qwen2_vl\\processing_qwen2_vl.py"
      },
      {
        "type": "file",
        "size": 14.498,
        "file_type": "py",
        "inclusive_size": 14.498,
        "id": "src\\transformers\\models\\qwen2_vl\\video_processing_qwen2_vl.py"
      },
      {
        "type": "file",
        "size": 1.131,
        "file_type": "py",
        "inclusive_size": 1.131,
        "id": "src\\transformers\\models\\qwen2_vl\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.609,
        "file_type": "py",
        "inclusive_size": 11.609,
        "id": "src\\transformers\\models\\qwen2_moe\\configuration_qwen2_moe.py"
      },
      {
        "type": "file",
        "size": 33.183,
        "file_type": "py",
        "inclusive_size": 33.183,
        "id": "src\\transformers\\models\\qwen2_moe\\modeling_qwen2_moe.py"
      },
      {
        "type": "file",
        "size": 11.04,
        "file_type": "py",
        "inclusive_size": 11.04,
        "id": "src\\transformers\\models\\qwen2_moe\\modular_qwen2_moe.py"
      },
      {
        "type": "file",
        "size": 0.999,
        "file_type": "py",
        "inclusive_size": 0.999,
        "id": "src\\transformers\\models\\qwen2_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.658,
        "file_type": "py",
        "inclusive_size": 8.658,
        "id": "src\\transformers\\models\\qwen2_audio\\configuration_qwen2_audio.py"
      },
      {
        "type": "file",
        "size": 39.607,
        "file_type": "py",
        "inclusive_size": 39.607,
        "id": "src\\transformers\\models\\qwen2_audio\\modeling_qwen2_audio.py"
      },
      {
        "type": "file",
        "size": 9.302,
        "file_type": "py",
        "inclusive_size": 9.302,
        "id": "src\\transformers\\models\\qwen2_audio\\processing_qwen2_audio.py"
      },
      {
        "type": "file",
        "size": 1.045,
        "file_type": "py",
        "inclusive_size": 1.045,
        "id": "src\\transformers\\models\\qwen2_audio\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.942,
        "file_type": "py",
        "inclusive_size": 15.942,
        "id": "src\\transformers\\models\\qwen2_5_vl\\configuration_qwen2_5_vl.py"
      },
      {
        "type": "file",
        "size": 84.044,
        "file_type": "py",
        "inclusive_size": 84.044,
        "id": "src\\transformers\\models\\qwen2_5_vl\\modeling_qwen2_5_vl.py"
      },
      {
        "type": "file",
        "size": 46.729,
        "file_type": "py",
        "inclusive_size": 46.729,
        "id": "src\\transformers\\models\\qwen2_5_vl\\modular_qwen2_5_vl.py"
      },
      {
        "type": "file",
        "size": 11.878,
        "file_type": "py",
        "inclusive_size": 11.878,
        "id": "src\\transformers\\models\\qwen2_5_vl\\processing_qwen2_5_vl.py"
      },
      {
        "type": "file",
        "size": 1.065,
        "file_type": "py",
        "inclusive_size": 1.065,
        "id": "src\\transformers\\models\\qwen2_5_vl\\__init__.py"
      },
      {
        "type": "file",
        "size": 47.829,
        "file_type": "py",
        "inclusive_size": 47.829,
        "id": "src\\transformers\\models\\qwen2_5_omni\\configuration_qwen2_5_omni.py"
      },
      {
        "type": "file",
        "size": 183.176,
        "file_type": "py",
        "inclusive_size": 183.176,
        "id": "src\\transformers\\models\\qwen2_5_omni\\modeling_qwen2_5_omni.py"
      },
      {
        "type": "file",
        "size": 188.96,
        "file_type": "py",
        "inclusive_size": 188.96,
        "id": "src\\transformers\\models\\qwen2_5_omni\\modular_qwen2_5_omni.py"
      },
      {
        "type": "file",
        "size": 18.896,
        "file_type": "py",
        "inclusive_size": 18.896,
        "id": "src\\transformers\\models\\qwen2_5_omni\\processing_qwen2_5_omni.py"
      },
      {
        "type": "file",
        "size": 1.071,
        "file_type": "py",
        "inclusive_size": 1.071,
        "id": "src\\transformers\\models\\qwen2_5_omni\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.994,
        "file_type": "py",
        "inclusive_size": 8.994,
        "id": "src\\transformers\\models\\qwen2\\configuration_qwen2.py"
      },
      {
        "type": "file",
        "size": 22.638,
        "file_type": "py",
        "inclusive_size": 22.638,
        "id": "src\\transformers\\models\\qwen2\\modeling_qwen2.py"
      },
      {
        "type": "file",
        "size": 9.409,
        "file_type": "py",
        "inclusive_size": 9.409,
        "id": "src\\transformers\\models\\qwen2\\modular_qwen2.py"
      },
      {
        "type": "file",
        "size": 3.323,
        "file_type": "py",
        "inclusive_size": 3.323,
        "id": "src\\transformers\\models\\qwen2\\tokenization_qwen2.py"
      },
      {
        "type": "file",
        "size": 1.095,
        "file_type": "py",
        "inclusive_size": 1.095,
        "id": "src\\transformers\\models\\qwen2\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.953,
        "file_type": "py",
        "inclusive_size": 7.953,
        "id": "src\\transformers\\models\\pvt_v2\\configuration_pvt_v2.py"
      },
      {
        "type": "file",
        "size": 12.136,
        "file_type": "py",
        "inclusive_size": 12.136,
        "id": "src\\transformers\\models\\pvt_v2\\convert_pvt_v2_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 23.579,
        "file_type": "py",
        "inclusive_size": 23.579,
        "id": "src\\transformers\\models\\pvt_v2\\modeling_pvt_v2.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\pvt_v2\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.362,
        "file_type": "py",
        "inclusive_size": 6.362,
        "id": "src\\transformers\\models\\pvt\\configuration_pvt.py"
      },
      {
        "type": "file",
        "size": 9.802,
        "file_type": "py",
        "inclusive_size": 9.802,
        "id": "src\\transformers\\models\\pvt\\convert_pvt_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 13.388,
        "file_type": "py",
        "inclusive_size": 13.388,
        "id": "src\\transformers\\models\\pvt\\image_processing_pvt.py"
      },
      {
        "type": "file",
        "size": 1.325,
        "file_type": "py",
        "inclusive_size": 1.325,
        "id": "src\\transformers\\models\\pvt\\image_processing_pvt_fast.py"
      },
      {
        "type": "file",
        "size": 22.525,
        "file_type": "py",
        "inclusive_size": 22.525,
        "id": "src\\transformers\\models\\pvt\\modeling_pvt.py"
      },
      {
        "type": "file",
        "size": 1.072,
        "file_type": "py",
        "inclusive_size": 1.072,
        "id": "src\\transformers\\models\\pvt\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.943,
        "file_type": "py",
        "inclusive_size": 8.943,
        "id": "src\\transformers\\models\\prophetnet\\configuration_prophetnet.py"
      },
      {
        "type": "file",
        "size": 6.969,
        "file_type": "py",
        "inclusive_size": 6.969,
        "id": "src\\transformers\\models\\prophetnet\\convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 87.498,
        "file_type": "py",
        "inclusive_size": 87.498,
        "id": "src\\transformers\\models\\prophetnet\\modeling_prophetnet.py"
      },
      {
        "type": "file",
        "size": 19.845,
        "file_type": "py",
        "inclusive_size": 19.845,
        "id": "src\\transformers\\models\\prophetnet\\tokenization_prophetnet.py"
      },
      {
        "type": "file",
        "size": 1.044,
        "file_type": "py",
        "inclusive_size": 1.044,
        "id": "src\\transformers\\models\\prophetnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.291,
        "file_type": "py",
        "inclusive_size": 8.291,
        "id": "src\\transformers\\models\\prompt_depth_anything\\configuration_prompt_depth_anything.py"
      },
      {
        "type": "file",
        "size": 12.588,
        "file_type": "py",
        "inclusive_size": 12.588,
        "id": "src\\transformers\\models\\prompt_depth_anything\\convert_prompt_depth_anything_to_hf.py"
      },
      {
        "type": "file",
        "size": 25.03,
        "file_type": "py",
        "inclusive_size": 25.03,
        "id": "src\\transformers\\models\\prompt_depth_anything\\image_processing_prompt_depth_anything.py"
      },
      {
        "type": "file",
        "size": 13.209,
        "file_type": "py",
        "inclusive_size": 13.209,
        "id": "src\\transformers\\models\\prompt_depth_anything\\image_processing_prompt_depth_anything_fast.py"
      },
      {
        "type": "file",
        "size": 20.524,
        "file_type": "py",
        "inclusive_size": 20.524,
        "id": "src\\transformers\\models\\prompt_depth_anything\\modeling_prompt_depth_anything.py"
      },
      {
        "type": "file",
        "size": 13.872,
        "file_type": "py",
        "inclusive_size": 13.872,
        "id": "src\\transformers\\models\\prompt_depth_anything\\modular_prompt_depth_anything.py"
      },
      {
        "type": "file",
        "size": 1.333,
        "file_type": "py",
        "inclusive_size": 1.333,
        "id": "src\\transformers\\models\\prompt_depth_anything\\__init__.py"
      },
      {
        "type": "file",
        "size": 16.933,
        "file_type": "py",
        "inclusive_size": 16.933,
        "id": "src\\transformers\\models\\pp_doclayout_v3\\configuration_pp_doclayout_v3.py"
      },
      {
        "type": "file",
        "size": 13.491,
        "file_type": "py",
        "inclusive_size": 13.491,
        "id": "src\\transformers\\models\\pp_doclayout_v3\\image_processing_pp_doclayout_v3_fast.py"
      },
      {
        "type": "file",
        "size": 103.732,
        "file_type": "py",
        "inclusive_size": 103.732,
        "id": "src\\transformers\\models\\pp_doclayout_v3\\modeling_pp_doclayout_v3.py"
      },
      {
        "type": "file",
        "size": 84.076,
        "file_type": "py",
        "inclusive_size": 84.076,
        "id": "src\\transformers\\models\\pp_doclayout_v3\\modular_pp_doclayout_v3.py"
      },
      {
        "type": "file",
        "size": 1.07,
        "file_type": "py",
        "inclusive_size": 1.07,
        "id": "src\\transformers\\models\\pp_doclayout_v3\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.057,
        "file_type": "py",
        "inclusive_size": 6.057,
        "id": "src\\transformers\\models\\pop2piano\\configuration_pop2piano.py"
      },
      {
        "type": "file",
        "size": 8.643,
        "file_type": "py",
        "inclusive_size": 8.643,
        "id": "src\\transformers\\models\\pop2piano\\convert_pop2piano_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 19.87,
        "file_type": "py",
        "inclusive_size": 19.87,
        "id": "src\\transformers\\models\\pop2piano\\feature_extraction_pop2piano.py"
      },
      {
        "type": "file",
        "size": 58.246,
        "file_type": "py",
        "inclusive_size": 58.246,
        "id": "src\\transformers\\models\\pop2piano\\modeling_pop2piano.py"
      },
      {
        "type": "file",
        "size": 5.235,
        "file_type": "py",
        "inclusive_size": 5.235,
        "id": "src\\transformers\\models\\pop2piano\\processing_pop2piano.py"
      },
      {
        "type": "file",
        "size": 32.55,
        "file_type": "py",
        "inclusive_size": 32.55,
        "id": "src\\transformers\\models\\pop2piano\\tokenization_pop2piano.py"
      },
      {
        "type": "file",
        "size": 0.999,
        "file_type": "py",
        "inclusive_size": 0.999,
        "id": "src\\transformers\\models\\pop2piano\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.061,
        "file_type": "py",
        "inclusive_size": 5.061,
        "id": "src\\transformers\\models\\poolformer\\configuration_poolformer.py"
      },
      {
        "type": "file",
        "size": 8.011,
        "file_type": "py",
        "inclusive_size": 8.011,
        "id": "src\\transformers\\models\\poolformer\\convert_poolformer_original_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 17.874,
        "file_type": "py",
        "inclusive_size": 17.874,
        "id": "src\\transformers\\models\\poolformer\\image_processing_poolformer.py"
      },
      {
        "type": "file",
        "size": 9.892,
        "file_type": "py",
        "inclusive_size": 9.892,
        "id": "src\\transformers\\models\\poolformer\\image_processing_poolformer_fast.py"
      },
      {
        "type": "file",
        "size": 14.459,
        "file_type": "py",
        "inclusive_size": 14.459,
        "id": "src\\transformers\\models\\poolformer\\modeling_poolformer.py"
      },
      {
        "type": "file",
        "size": 1.149,
        "file_type": "py",
        "inclusive_size": 1.149,
        "id": "src\\transformers\\models\\poolformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.701,
        "file_type": "py",
        "inclusive_size": 7.701,
        "id": "src\\transformers\\models\\plbart\\configuration_plbart.py"
      },
      {
        "type": "file",
        "size": 3.572,
        "file_type": "py",
        "inclusive_size": 3.572,
        "id": "src\\transformers\\models\\plbart\\convert_plbart_original_checkpoint_to_torch.py"
      },
      {
        "type": "file",
        "size": 63.756,
        "file_type": "py",
        "inclusive_size": 63.756,
        "id": "src\\transformers\\models\\plbart\\modeling_plbart.py"
      },
      {
        "type": "file",
        "size": 18.769,
        "file_type": "py",
        "inclusive_size": 18.769,
        "id": "src\\transformers\\models\\plbart\\modular_plbart.py"
      },
      {
        "type": "file",
        "size": 16.439,
        "file_type": "py",
        "inclusive_size": 16.439,
        "id": "src\\transformers\\models\\plbart\\tokenization_plbart.py"
      },
      {
        "type": "file",
        "size": 1.032,
        "file_type": "py",
        "inclusive_size": 1.032,
        "id": "src\\transformers\\models\\plbart\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.453,
        "file_type": "py",
        "inclusive_size": 4.453,
        "id": "src\\transformers\\models\\pixtral\\configuration_pixtral.py"
      },
      {
        "type": "file",
        "size": 10.058,
        "file_type": "py",
        "inclusive_size": 10.058,
        "id": "src\\transformers\\models\\pixtral\\convert_pixtral_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 22.088,
        "file_type": "py",
        "inclusive_size": 22.088,
        "id": "src\\transformers\\models\\pixtral\\image_processing_pixtral.py"
      },
      {
        "type": "file",
        "size": 7.418,
        "file_type": "py",
        "inclusive_size": 7.418,
        "id": "src\\transformers\\models\\pixtral\\image_processing_pixtral_fast.py"
      },
      {
        "type": "file",
        "size": 21.785,
        "file_type": "py",
        "inclusive_size": 21.785,
        "id": "src\\transformers\\models\\pixtral\\modeling_pixtral.py"
      },
      {
        "type": "file",
        "size": 9.508,
        "file_type": "py",
        "inclusive_size": 9.508,
        "id": "src\\transformers\\models\\pixtral\\processing_pixtral.py"
      },
      {
        "type": "file",
        "size": 1.126,
        "file_type": "py",
        "inclusive_size": 1.126,
        "id": "src\\transformers\\models\\pixtral\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.977,
        "file_type": "py",
        "inclusive_size": 7.977,
        "id": "src\\transformers\\models\\pixio\\configuration_pixio.py"
      },
      {
        "type": "file",
        "size": 8.623,
        "file_type": "py",
        "inclusive_size": 8.623,
        "id": "src\\transformers\\models\\pixio\\convert_pixio_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 21.142,
        "file_type": "py",
        "inclusive_size": 21.142,
        "id": "src\\transformers\\models\\pixio\\modeling_pixio.py"
      },
      {
        "type": "file",
        "size": 16.62,
        "file_type": "py",
        "inclusive_size": 16.62,
        "id": "src\\transformers\\models\\pixio\\modular_pixio.py"
      },
      {
        "type": "file",
        "size": 1.041,
        "file_type": "py",
        "inclusive_size": 1.041,
        "id": "src\\transformers\\models\\pixio\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.862,
        "file_type": "py",
        "inclusive_size": 15.862,
        "id": "src\\transformers\\models\\pix2struct\\configuration_pix2struct.py"
      },
      {
        "type": "file",
        "size": 5.836,
        "file_type": "py",
        "inclusive_size": 5.836,
        "id": "src\\transformers\\models\\pix2struct\\convert_pix2struct_original_pytorch_to_hf.py"
      },
      {
        "type": "file",
        "size": 19.918,
        "file_type": "py",
        "inclusive_size": 19.918,
        "id": "src\\transformers\\models\\pix2struct\\image_processing_pix2struct.py"
      },
      {
        "type": "file",
        "size": 13.189,
        "file_type": "py",
        "inclusive_size": 13.189,
        "id": "src\\transformers\\models\\pix2struct\\image_processing_pix2struct_fast.py"
      },
      {
        "type": "file",
        "size": 67.321,
        "file_type": "py",
        "inclusive_size": 67.321,
        "id": "src\\transformers\\models\\pix2struct\\modeling_pix2struct.py"
      },
      {
        "type": "file",
        "size": 4.167,
        "file_type": "py",
        "inclusive_size": 4.167,
        "id": "src\\transformers\\models\\pix2struct\\processing_pix2struct.py"
      },
      {
        "type": "file",
        "size": 1.141,
        "file_type": "py",
        "inclusive_size": 1.141,
        "id": "src\\transformers\\models\\pix2struct\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.057,
        "file_type": "py",
        "inclusive_size": 13.057,
        "id": "src\\transformers\\models\\phobert\\tokenization_phobert.py"
      },
      {
        "type": "file",
        "size": 0.958,
        "file_type": "py",
        "inclusive_size": 0.958,
        "id": "src\\transformers\\models\\phobert\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.337,
        "file_type": "py",
        "inclusive_size": 10.337,
        "id": "src\\transformers\\models\\phimoe\\configuration_phimoe.py"
      },
      {
        "type": "file",
        "size": 39.529,
        "file_type": "py",
        "inclusive_size": 39.529,
        "id": "src\\transformers\\models\\phimoe\\modeling_phimoe.py"
      },
      {
        "type": "file",
        "size": 15.612,
        "file_type": "py",
        "inclusive_size": 15.612,
        "id": "src\\transformers\\models\\phimoe\\modular_phimoe.py"
      },
      {
        "type": "file",
        "size": 1.013,
        "file_type": "py",
        "inclusive_size": 1.013,
        "id": "src\\transformers\\models\\phimoe\\__init__.py"
      },
      {
        "type": "file",
        "size": 25.062,
        "file_type": "py",
        "inclusive_size": 25.062,
        "id": "src\\transformers\\models\\phi4_multimodal\\configuration_phi4_multimodal.py"
      },
      {
        "type": "file",
        "size": 12.45,
        "file_type": "py",
        "inclusive_size": 12.45,
        "id": "src\\transformers\\models\\phi4_multimodal\\convert_phi4_multimodal_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 13.283,
        "file_type": "py",
        "inclusive_size": 13.283,
        "id": "src\\transformers\\models\\phi4_multimodal\\feature_extraction_phi4_multimodal.py"
      },
      {
        "type": "file",
        "size": 10.427,
        "file_type": "py",
        "inclusive_size": 10.427,
        "id": "src\\transformers\\models\\phi4_multimodal\\image_processing_phi4_multimodal_fast.py"
      },
      {
        "type": "file",
        "size": 78.804,
        "file_type": "py",
        "inclusive_size": 78.804,
        "id": "src\\transformers\\models\\phi4_multimodal\\modeling_phi4_multimodal.py"
      },
      {
        "type": "file",
        "size": 77.834,
        "file_type": "py",
        "inclusive_size": 77.834,
        "id": "src\\transformers\\models\\phi4_multimodal\\modular_phi4_multimodal.py"
      },
      {
        "type": "file",
        "size": 5.512,
        "file_type": "py",
        "inclusive_size": 5.512,
        "id": "src\\transformers\\models\\phi4_multimodal\\processing_phi4_multimodal.py"
      },
      {
        "type": "file",
        "size": 1.17,
        "file_type": "py",
        "inclusive_size": 1.17,
        "id": "src\\transformers\\models\\phi4_multimodal\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.196,
        "file_type": "py",
        "inclusive_size": 12.196,
        "id": "src\\transformers\\models\\phi3\\configuration_phi3.py"
      },
      {
        "type": "file",
        "size": 24.346,
        "file_type": "py",
        "inclusive_size": 24.346,
        "id": "src\\transformers\\models\\phi3\\modeling_phi3.py"
      },
      {
        "type": "file",
        "size": 10.809,
        "file_type": "py",
        "inclusive_size": 10.809,
        "id": "src\\transformers\\models\\phi3\\modular_phi3.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\phi3\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.304,
        "file_type": "py",
        "inclusive_size": 8.304,
        "id": "src\\transformers\\models\\phi\\configuration_phi.py"
      },
      {
        "type": "file",
        "size": 7.676,
        "file_type": "py",
        "inclusive_size": 7.676,
        "id": "src\\transformers\\models\\phi\\convert_phi_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 23.253,
        "file_type": "py",
        "inclusive_size": 23.253,
        "id": "src\\transformers\\models\\phi\\modeling_phi.py"
      },
      {
        "type": "file",
        "size": 12.735,
        "file_type": "py",
        "inclusive_size": 12.735,
        "id": "src\\transformers\\models\\phi\\modular_phi.py"
      },
      {
        "type": "file",
        "size": 1.006,
        "file_type": "py",
        "inclusive_size": 1.006,
        "id": "src\\transformers\\models\\phi\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.993,
        "file_type": "py",
        "inclusive_size": 8.993,
        "id": "src\\transformers\\models\\pe_video\\configuration_pe_video.py"
      },
      {
        "type": "file",
        "size": 26.54,
        "file_type": "py",
        "inclusive_size": 26.54,
        "id": "src\\transformers\\models\\pe_video\\modeling_pe_video.py"
      },
      {
        "type": "file",
        "size": 8.689,
        "file_type": "py",
        "inclusive_size": 8.689,
        "id": "src\\transformers\\models\\pe_video\\modular_pe_video.py"
      },
      {
        "type": "file",
        "size": 0.262,
        "file_type": "py",
        "inclusive_size": 0.262,
        "id": "src\\transformers\\models\\pe_video\\processing_pe_video.py"
      },
      {
        "type": "file",
        "size": 2.663,
        "file_type": "py",
        "inclusive_size": 2.663,
        "id": "src\\transformers\\models\\pe_video\\video_processing_pe_video.py"
      },
      {
        "type": "file",
        "size": 1.086,
        "file_type": "py",
        "inclusive_size": 1.086,
        "id": "src\\transformers\\models\\pe_video\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.742,
        "file_type": "py",
        "inclusive_size": 9.742,
        "id": "src\\transformers\\models\\pe_audio_video\\configuration_pe_audio_video.py"
      },
      {
        "type": "file",
        "size": 6.201,
        "file_type": "py",
        "inclusive_size": 6.201,
        "id": "src\\transformers\\models\\pe_audio_video\\convert_pe_audio_video_to_hf.py"
      },
      {
        "type": "file",
        "size": 42.366,
        "file_type": "py",
        "inclusive_size": 42.366,
        "id": "src\\transformers\\models\\pe_audio_video\\modeling_pe_audio_video.py"
      },
      {
        "type": "file",
        "size": 32.28,
        "file_type": "py",
        "inclusive_size": 32.28,
        "id": "src\\transformers\\models\\pe_audio_video\\modular_pe_audio_video.py"
      },
      {
        "type": "file",
        "size": 0.96,
        "file_type": "py",
        "inclusive_size": 0.96,
        "id": "src\\transformers\\models\\pe_audio_video\\processing_pe_audio_video.py"
      },
      {
        "type": "file",
        "size": 1.059,
        "file_type": "py",
        "inclusive_size": 1.059,
        "id": "src\\transformers\\models\\pe_audio_video\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.766,
        "file_type": "py",
        "inclusive_size": 8.766,
        "id": "src\\transformers\\models\\pe_audio\\configuration_pe_audio.py"
      },
      {
        "type": "file",
        "size": 6.484,
        "file_type": "py",
        "inclusive_size": 6.484,
        "id": "src\\transformers\\models\\pe_audio\\feature_extraction_pe_audio.py"
      },
      {
        "type": "file",
        "size": 32.848,
        "file_type": "py",
        "inclusive_size": 32.848,
        "id": "src\\transformers\\models\\pe_audio\\modeling_pe_audio.py"
      },
      {
        "type": "file",
        "size": 11.116,
        "file_type": "py",
        "inclusive_size": 11.116,
        "id": "src\\transformers\\models\\pe_audio\\modular_pe_audio.py"
      },
      {
        "type": "file",
        "size": 0.879,
        "file_type": "py",
        "inclusive_size": 0.879,
        "id": "src\\transformers\\models\\pe_audio\\processing_pe_audio.py"
      },
      {
        "type": "file",
        "size": 1.088,
        "file_type": "py",
        "inclusive_size": 1.088,
        "id": "src\\transformers\\models\\pe_audio\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.103,
        "file_type": "py",
        "inclusive_size": 6.103,
        "id": "src\\transformers\\models\\persimmon\\configuration_persimmon.py"
      },
      {
        "type": "file",
        "size": 4.435,
        "file_type": "py",
        "inclusive_size": 4.435,
        "id": "src\\transformers\\models\\persimmon\\convert_persimmon_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 33.747,
        "file_type": "py",
        "inclusive_size": 33.747,
        "id": "src\\transformers\\models\\persimmon\\modeling_persimmon.py"
      },
      {
        "type": "file",
        "size": 0.999,
        "file_type": "py",
        "inclusive_size": 0.999,
        "id": "src\\transformers\\models\\persimmon\\__init__.py"
      },
      {
        "type": "file",
        "size": 3.873,
        "file_type": "py",
        "inclusive_size": 3.873,
        "id": "src\\transformers\\models\\perception_lm\\configuration_perception_lm.py"
      },
      {
        "type": "file",
        "size": 23.094,
        "file_type": "py",
        "inclusive_size": 23.094,
        "id": "src\\transformers\\models\\perception_lm\\convert_perception_lm_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 13.806,
        "file_type": "py",
        "inclusive_size": 13.806,
        "id": "src\\transformers\\models\\perception_lm\\image_processing_perception_lm_fast.py"
      },
      {
        "type": "file",
        "size": 21.262,
        "file_type": "py",
        "inclusive_size": 21.262,
        "id": "src\\transformers\\models\\perception_lm\\modeling_perception_lm.py"
      },
      {
        "type": "file",
        "size": 18.851,
        "file_type": "py",
        "inclusive_size": 18.851,
        "id": "src\\transformers\\models\\perception_lm\\modular_perception_lm.py"
      },
      {
        "type": "file",
        "size": 8.828,
        "file_type": "py",
        "inclusive_size": 8.828,
        "id": "src\\transformers\\models\\perception_lm\\processing_perception_lm.py"
      },
      {
        "type": "file",
        "size": 1.211,
        "file_type": "py",
        "inclusive_size": 1.211,
        "id": "src\\transformers\\models\\perception_lm\\video_processing_perception_lm.py"
      },
      {
        "type": "file",
        "size": 1.106,
        "file_type": "py",
        "inclusive_size": 1.106,
        "id": "src\\transformers\\models\\perception_lm\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.986,
        "file_type": "py",
        "inclusive_size": 8.986,
        "id": "src\\transformers\\models\\perceiver\\configuration_perceiver.py"
      },
      {
        "type": "file",
        "size": 22.014,
        "file_type": "py",
        "inclusive_size": 22.014,
        "id": "src\\transformers\\models\\perceiver\\convert_perceiver_haiku_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 17.15,
        "file_type": "py",
        "inclusive_size": 17.15,
        "id": "src\\transformers\\models\\perceiver\\image_processing_perceiver.py"
      },
      {
        "type": "file",
        "size": 4.888,
        "file_type": "py",
        "inclusive_size": 4.888,
        "id": "src\\transformers\\models\\perceiver\\image_processing_perceiver_fast.py"
      },
      {
        "type": "file",
        "size": 134.813,
        "file_type": "py",
        "inclusive_size": 134.813,
        "id": "src\\transformers\\models\\perceiver\\modeling_perceiver.py"
      },
      {
        "type": "file",
        "size": 7.982,
        "file_type": "py",
        "inclusive_size": 7.982,
        "id": "src\\transformers\\models\\perceiver\\tokenization_perceiver.py"
      },
      {
        "type": "file",
        "size": 1.186,
        "file_type": "py",
        "inclusive_size": 1.186,
        "id": "src\\transformers\\models\\perceiver\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.034,
        "file_type": "py",
        "inclusive_size": 8.034,
        "id": "src\\transformers\\models\\pegasus_x\\configuration_pegasus_x.py"
      },
      {
        "type": "file",
        "size": 67.251,
        "file_type": "py",
        "inclusive_size": 67.251,
        "id": "src\\transformers\\models\\pegasus_x\\modeling_pegasus_x.py"
      },
      {
        "type": "file",
        "size": 0.999,
        "file_type": "py",
        "inclusive_size": 0.999,
        "id": "src\\transformers\\models\\pegasus_x\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.462,
        "file_type": "py",
        "inclusive_size": 7.462,
        "id": "src\\transformers\\models\\pegasus\\configuration_pegasus.py"
      },
      {
        "type": "file",
        "size": 5.32,
        "file_type": "py",
        "inclusive_size": 5.32,
        "id": "src\\transformers\\models\\pegasus\\convert_pegasus_tf_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 61.731,
        "file_type": "py",
        "inclusive_size": 61.731,
        "id": "src\\transformers\\models\\pegasus\\modeling_pegasus.py"
      },
      {
        "type": "file",
        "size": 6.122,
        "file_type": "py",
        "inclusive_size": 6.122,
        "id": "src\\transformers\\models\\pegasus\\tokenization_pegasus.py"
      },
      {
        "type": "file",
        "size": 1.035,
        "file_type": "py",
        "inclusive_size": 1.035,
        "id": "src\\transformers\\models\\pegasus\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.228,
        "file_type": "py",
        "inclusive_size": 12.228,
        "id": "src\\transformers\\models\\patchtst\\configuration_patchtst.py"
      },
      {
        "type": "file",
        "size": 85.071,
        "file_type": "py",
        "inclusive_size": 85.071,
        "id": "src\\transformers\\models\\patchtst\\modeling_patchtst.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\patchtst\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.502,
        "file_type": "py",
        "inclusive_size": 12.502,
        "id": "src\\transformers\\models\\patchtsmixer\\configuration_patchtsmixer.py"
      },
      {
        "type": "file",
        "size": 85.235,
        "file_type": "py",
        "inclusive_size": 85.235,
        "id": "src\\transformers\\models\\patchtsmixer\\modeling_patchtsmixer.py"
      },
      {
        "type": "file",
        "size": 1.005,
        "file_type": "py",
        "inclusive_size": 1.005,
        "id": "src\\transformers\\models\\patchtsmixer\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.231,
        "file_type": "py",
        "inclusive_size": 10.231,
        "id": "src\\transformers\\models\\parakeet\\configuration_parakeet.py"
      },
      {
        "type": "file",
        "size": 14.295,
        "file_type": "py",
        "inclusive_size": 14.295,
        "id": "src\\transformers\\models\\parakeet\\convert_nemo_to_hf.py"
      },
      {
        "type": "file",
        "size": 13.06,
        "file_type": "py",
        "inclusive_size": 13.06,
        "id": "src\\transformers\\models\\parakeet\\feature_extraction_parakeet.py"
      },
      {
        "type": "file",
        "size": 35.138,
        "file_type": "py",
        "inclusive_size": 35.138,
        "id": "src\\transformers\\models\\parakeet\\modeling_parakeet.py"
      },
      {
        "type": "file",
        "size": 27.484,
        "file_type": "py",
        "inclusive_size": 27.484,
        "id": "src\\transformers\\models\\parakeet\\modular_parakeet.py"
      },
      {
        "type": "file",
        "size": 3.688,
        "file_type": "py",
        "inclusive_size": 3.688,
        "id": "src\\transformers\\models\\parakeet\\processing_parakeet.py"
      },
      {
        "type": "file",
        "size": 1.873,
        "file_type": "py",
        "inclusive_size": 1.873,
        "id": "src\\transformers\\models\\parakeet\\tokenization_parakeet.py"
      },
      {
        "type": "file",
        "size": 1.09,
        "file_type": "py",
        "inclusive_size": 1.09,
        "id": "src\\transformers\\models\\parakeet\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.883,
        "file_type": "py",
        "inclusive_size": 5.883,
        "id": "src\\transformers\\models\\paligemma\\configuration_paligemma.py"
      },
      {
        "type": "file",
        "size": 20.745,
        "file_type": "py",
        "inclusive_size": 20.745,
        "id": "src\\transformers\\models\\paligemma\\convert_paligemma2_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 16.83,
        "file_type": "py",
        "inclusive_size": 16.83,
        "id": "src\\transformers\\models\\paligemma\\convert_paligemma_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 27.116,
        "file_type": "py",
        "inclusive_size": 27.116,
        "id": "src\\transformers\\models\\paligemma\\modeling_paligemma.py"
      },
      {
        "type": "file",
        "size": 11.657,
        "file_type": "py",
        "inclusive_size": 11.657,
        "id": "src\\transformers\\models\\paligemma\\processing_paligemma.py"
      },
      {
        "type": "file",
        "size": 1.039,
        "file_type": "py",
        "inclusive_size": 1.039,
        "id": "src\\transformers\\models\\paligemma\\__init__.py"
      },
      {
        "type": "file",
        "size": 16.64,
        "file_type": "py",
        "inclusive_size": 16.64,
        "id": "src\\transformers\\models\\paddleocr_vl\\configuration_paddleocr_vl.py"
      },
      {
        "type": "file",
        "size": 25.011,
        "file_type": "py",
        "inclusive_size": 25.011,
        "id": "src\\transformers\\models\\paddleocr_vl\\image_processing_paddleocr_vl.py"
      },
      {
        "type": "file",
        "size": 9.298,
        "file_type": "py",
        "inclusive_size": 9.298,
        "id": "src\\transformers\\models\\paddleocr_vl\\image_processing_paddleocr_vl_fast.py"
      },
      {
        "type": "file",
        "size": 76.965,
        "file_type": "py",
        "inclusive_size": 76.965,
        "id": "src\\transformers\\models\\paddleocr_vl\\modeling_paddleocr_vl.py"
      },
      {
        "type": "file",
        "size": 59.621,
        "file_type": "py",
        "inclusive_size": 59.621,
        "id": "src\\transformers\\models\\paddleocr_vl\\modular_paddleocr_vl.py"
      },
      {
        "type": "file",
        "size": 6.64,
        "file_type": "py",
        "inclusive_size": 6.64,
        "id": "src\\transformers\\models\\paddleocr_vl\\processing_paddleocr_vl.py"
      },
      {
        "type": "file",
        "size": 1.152,
        "file_type": "py",
        "inclusive_size": 1.152,
        "id": "src\\transformers\\models\\paddleocr_vl\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.382,
        "file_type": "py",
        "inclusive_size": 12.382,
        "id": "src\\transformers\\models\\owlvit\\configuration_owlvit.py"
      },
      {
        "type": "file",
        "size": 13.942,
        "file_type": "py",
        "inclusive_size": 13.942,
        "id": "src\\transformers\\models\\owlvit\\convert_owlvit_original_flax_to_hf.py"
      },
      {
        "type": "file",
        "size": 26.87,
        "file_type": "py",
        "inclusive_size": 26.87,
        "id": "src\\transformers\\models\\owlvit\\image_processing_owlvit.py"
      },
      {
        "type": "file",
        "size": 8.178,
        "file_type": "py",
        "inclusive_size": 8.178,
        "id": "src\\transformers\\models\\owlvit\\image_processing_owlvit_fast.py"
      },
      {
        "type": "file",
        "size": 73.804,
        "file_type": "py",
        "inclusive_size": 73.804,
        "id": "src\\transformers\\models\\owlvit\\modeling_owlvit.py"
      },
      {
        "type": "file",
        "size": 10.389,
        "file_type": "py",
        "inclusive_size": 10.389,
        "id": "src\\transformers\\models\\owlvit\\processing_owlvit.py"
      },
      {
        "type": "file",
        "size": 1.166,
        "file_type": "py",
        "inclusive_size": 1.166,
        "id": "src\\transformers\\models\\owlvit\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.836,
        "file_type": "py",
        "inclusive_size": 12.836,
        "id": "src\\transformers\\models\\owlv2\\configuration_owlv2.py"
      },
      {
        "type": "file",
        "size": 22.041,
        "file_type": "py",
        "inclusive_size": 22.041,
        "id": "src\\transformers\\models\\owlv2\\convert_owlv2_to_hf.py"
      },
      {
        "type": "file",
        "size": 27.632,
        "file_type": "py",
        "inclusive_size": 27.632,
        "id": "src\\transformers\\models\\owlv2\\image_processing_owlv2.py"
      },
      {
        "type": "file",
        "size": 15.231,
        "file_type": "py",
        "inclusive_size": 15.231,
        "id": "src\\transformers\\models\\owlv2\\image_processing_owlv2_fast.py"
      },
      {
        "type": "file",
        "size": 78.346,
        "file_type": "py",
        "inclusive_size": 78.346,
        "id": "src\\transformers\\models\\owlv2\\modeling_owlv2.py"
      },
      {
        "type": "file",
        "size": 8.006,
        "file_type": "py",
        "inclusive_size": 8.006,
        "id": "src\\transformers\\models\\owlv2\\modular_owlv2.py"
      },
      {
        "type": "file",
        "size": 10.439,
        "file_type": "py",
        "inclusive_size": 10.439,
        "id": "src\\transformers\\models\\owlv2\\processing_owlv2.py"
      },
      {
        "type": "file",
        "size": 1.116,
        "file_type": "py",
        "inclusive_size": 1.116,
        "id": "src\\transformers\\models\\owlv2\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.376,
        "file_type": "py",
        "inclusive_size": 8.376,
        "id": "src\\transformers\\models\\ovis2\\configuration_ovis2.py"
      },
      {
        "type": "file",
        "size": 13.783,
        "file_type": "py",
        "inclusive_size": 13.783,
        "id": "src\\transformers\\models\\ovis2\\convert_ovis2_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 28.736,
        "file_type": "py",
        "inclusive_size": 28.736,
        "id": "src\\transformers\\models\\ovis2\\image_processing_ovis2.py"
      },
      {
        "type": "file",
        "size": 9.334,
        "file_type": "py",
        "inclusive_size": 9.334,
        "id": "src\\transformers\\models\\ovis2\\image_processing_ovis2_fast.py"
      },
      {
        "type": "file",
        "size": 32.358,
        "file_type": "py",
        "inclusive_size": 32.358,
        "id": "src\\transformers\\models\\ovis2\\modeling_ovis2.py"
      },
      {
        "type": "file",
        "size": 19.009,
        "file_type": "py",
        "inclusive_size": 19.009,
        "id": "src\\transformers\\models\\ovis2\\modular_ovis2.py"
      },
      {
        "type": "file",
        "size": 5.847,
        "file_type": "py",
        "inclusive_size": 5.847,
        "id": "src\\transformers\\models\\ovis2\\processing_ovis2.py"
      },
      {
        "type": "file",
        "size": 1.122,
        "file_type": "py",
        "inclusive_size": 1.122,
        "id": "src\\transformers\\models\\ovis2\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.751,
        "file_type": "py",
        "inclusive_size": 6.751,
        "id": "src\\transformers\\models\\opt\\configuration_opt.py"
      },
      {
        "type": "file",
        "size": 3.873,
        "file_type": "py",
        "inclusive_size": 3.873,
        "id": "src\\transformers\\models\\opt\\convert_opt_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 45.711,
        "file_type": "py",
        "inclusive_size": 45.711,
        "id": "src\\transformers\\models\\opt\\modeling_opt.py"
      },
      {
        "type": "file",
        "size": 0.987,
        "file_type": "py",
        "inclusive_size": 0.987,
        "id": "src\\transformers\\models\\opt\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.686,
        "file_type": "py",
        "inclusive_size": 7.686,
        "id": "src\\transformers\\models\\openai\\configuration_openai.py"
      },
      {
        "type": "file",
        "size": 6.15,
        "file_type": "py",
        "inclusive_size": 6.15,
        "id": "src\\transformers\\models\\openai\\convert_openai_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 31.961,
        "file_type": "py",
        "inclusive_size": 31.961,
        "id": "src\\transformers\\models\\openai\\modeling_openai.py"
      },
      {
        "type": "file",
        "size": 3.73,
        "file_type": "py",
        "inclusive_size": 3.73,
        "id": "src\\transformers\\models\\openai\\tokenization_openai.py"
      },
      {
        "type": "file",
        "size": 1.032,
        "file_type": "py",
        "inclusive_size": 1.032,
        "id": "src\\transformers\\models\\openai\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.505,
        "file_type": "py",
        "inclusive_size": 13.505,
        "id": "src\\transformers\\models\\oneformer\\configuration_oneformer.py"
      },
      {
        "type": "file",
        "size": 50.508,
        "file_type": "py",
        "inclusive_size": 50.508,
        "id": "src\\transformers\\models\\oneformer\\convert_to_hf_oneformer.py"
      },
      {
        "type": "file",
        "size": 61.082,
        "file_type": "py",
        "inclusive_size": 61.082,
        "id": "src\\transformers\\models\\oneformer\\image_processing_oneformer.py"
      },
      {
        "type": "file",
        "size": 39.918,
        "file_type": "py",
        "inclusive_size": 39.918,
        "id": "src\\transformers\\models\\oneformer\\image_processing_oneformer_fast.py"
      },
      {
        "type": "file",
        "size": 138.41,
        "file_type": "py",
        "inclusive_size": 138.41,
        "id": "src\\transformers\\models\\oneformer\\modeling_oneformer.py"
      },
      {
        "type": "file",
        "size": 8.231,
        "file_type": "py",
        "inclusive_size": 8.231,
        "id": "src\\transformers\\models\\oneformer\\processing_oneformer.py"
      },
      {
        "type": "file",
        "size": 1.136,
        "file_type": "py",
        "inclusive_size": 1.136,
        "id": "src\\transformers\\models\\oneformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.54,
        "file_type": "py",
        "inclusive_size": 14.54,
        "id": "src\\transformers\\models\\omdet_turbo\\configuration_omdet_turbo.py"
      },
      {
        "type": "file",
        "size": 17.566,
        "file_type": "py",
        "inclusive_size": 17.566,
        "id": "src\\transformers\\models\\omdet_turbo\\convert_omdet_turbo_to_hf.py"
      },
      {
        "type": "file",
        "size": 74.309,
        "file_type": "py",
        "inclusive_size": 74.309,
        "id": "src\\transformers\\models\\omdet_turbo\\modeling_omdet_turbo.py"
      },
      {
        "type": "file",
        "size": 14.35,
        "file_type": "py",
        "inclusive_size": 14.35,
        "id": "src\\transformers\\models\\omdet_turbo\\processing_omdet_turbo.py"
      },
      {
        "type": "file",
        "size": 1.045,
        "file_type": "py",
        "inclusive_size": 1.045,
        "id": "src\\transformers\\models\\omdet_turbo\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.578,
        "file_type": "py",
        "inclusive_size": 8.578,
        "id": "src\\transformers\\models\\olmoe\\configuration_olmoe.py"
      },
      {
        "type": "file",
        "size": 12.781,
        "file_type": "py",
        "inclusive_size": 12.781,
        "id": "src\\transformers\\models\\olmoe\\convert_olmoe_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 31.768,
        "file_type": "py",
        "inclusive_size": 31.768,
        "id": "src\\transformers\\models\\olmoe\\modeling_olmoe.py"
      },
      {
        "type": "file",
        "size": 11.626,
        "file_type": "py",
        "inclusive_size": 11.626,
        "id": "src\\transformers\\models\\olmoe\\modular_olmoe.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\olmoe\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.566,
        "file_type": "py",
        "inclusive_size": 9.566,
        "id": "src\\transformers\\models\\olmo3\\configuration_olmo3.py"
      },
      {
        "type": "file",
        "size": 18.09,
        "file_type": "py",
        "inclusive_size": 18.09,
        "id": "src\\transformers\\models\\olmo3\\convert_olmo3_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 22.646,
        "file_type": "py",
        "inclusive_size": 22.646,
        "id": "src\\transformers\\models\\olmo3\\modeling_olmo3.py"
      },
      {
        "type": "file",
        "size": 15.644,
        "file_type": "py",
        "inclusive_size": 15.644,
        "id": "src\\transformers\\models\\olmo3\\modular_olmo3.py"
      },
      {
        "type": "file",
        "size": 0.992,
        "file_type": "py",
        "inclusive_size": 0.992,
        "id": "src\\transformers\\models\\olmo3\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.058,
        "file_type": "py",
        "inclusive_size": 9.058,
        "id": "src\\transformers\\models\\olmo2\\configuration_olmo2.py"
      },
      {
        "type": "file",
        "size": 11.235,
        "file_type": "py",
        "inclusive_size": 11.235,
        "id": "src\\transformers\\models\\olmo2\\convert_olmo2_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 22.178,
        "file_type": "py",
        "inclusive_size": 22.178,
        "id": "src\\transformers\\models\\olmo2\\modeling_olmo2.py"
      },
      {
        "type": "file",
        "size": 14.556,
        "file_type": "py",
        "inclusive_size": 14.556,
        "id": "src\\transformers\\models\\olmo2\\modular_olmo2.py"
      },
      {
        "type": "file",
        "size": 1.011,
        "file_type": "py",
        "inclusive_size": 1.011,
        "id": "src\\transformers\\models\\olmo2\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.097,
        "file_type": "py",
        "inclusive_size": 8.097,
        "id": "src\\transformers\\models\\olmo\\configuration_olmo.py"
      },
      {
        "type": "file",
        "size": 9.175,
        "file_type": "py",
        "inclusive_size": 9.175,
        "id": "src\\transformers\\models\\olmo\\convert_olmo_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 21.778,
        "file_type": "py",
        "inclusive_size": 21.778,
        "id": "src\\transformers\\models\\olmo\\modeling_olmo.py"
      },
      {
        "type": "file",
        "size": 7.903,
        "file_type": "py",
        "inclusive_size": 7.903,
        "id": "src\\transformers\\models\\olmo\\modular_olmo.py"
      },
      {
        "type": "file",
        "size": 1.009,
        "file_type": "py",
        "inclusive_size": 1.009,
        "id": "src\\transformers\\models\\olmo\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.608,
        "file_type": "py",
        "inclusive_size": 6.608,
        "id": "src\\transformers\\models\\nystromformer\\configuration_nystromformer.py"
      },
      {
        "type": "file",
        "size": 4.195,
        "file_type": "py",
        "inclusive_size": 4.195,
        "id": "src\\transformers\\models\\nystromformer\\convert_nystromformer_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 39.903,
        "file_type": "py",
        "inclusive_size": 39.903,
        "id": "src\\transformers\\models\\nystromformer\\modeling_nystromformer.py"
      },
      {
        "type": "file",
        "size": 1.007,
        "file_type": "py",
        "inclusive_size": 1.007,
        "id": "src\\transformers\\models\\nystromformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.935,
        "file_type": "py",
        "inclusive_size": 10.935,
        "id": "src\\transformers\\models\\nougat\\convert_nougat_to_hf.py"
      },
      {
        "type": "file",
        "size": 24.381,
        "file_type": "py",
        "inclusive_size": 24.381,
        "id": "src\\transformers\\models\\nougat\\image_processing_nougat.py"
      },
      {
        "type": "file",
        "size": 10.574,
        "file_type": "py",
        "inclusive_size": 10.574,
        "id": "src\\transformers\\models\\nougat\\image_processing_nougat_fast.py"
      },
      {
        "type": "file",
        "size": 6.221,
        "file_type": "py",
        "inclusive_size": 6.221,
        "id": "src\\transformers\\models\\nougat\\processing_nougat.py"
      },
      {
        "type": "file",
        "size": 26.091,
        "file_type": "py",
        "inclusive_size": 26.091,
        "id": "src\\transformers\\models\\nougat\\tokenization_nougat.py"
      },
      {
        "type": "file",
        "size": 1.085,
        "file_type": "py",
        "inclusive_size": 1.085,
        "id": "src\\transformers\\models\\nougat\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.269,
        "file_type": "py",
        "inclusive_size": 11.269,
        "id": "src\\transformers\\models\\nllb_moe\\configuration_nllb_moe.py"
      },
      {
        "type": "file",
        "size": 6.466,
        "file_type": "py",
        "inclusive_size": 6.466,
        "id": "src\\transformers\\models\\nllb_moe\\convert_nllb_moe_sharded_original_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 50.275,
        "file_type": "py",
        "inclusive_size": 50.275,
        "id": "src\\transformers\\models\\nllb_moe\\modeling_nllb_moe.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\nllb_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.179,
        "file_type": "py",
        "inclusive_size": 14.179,
        "id": "src\\transformers\\models\\nllb\\tokenization_nllb.py"
      },
      {
        "type": "file",
        "size": 0.955,
        "file_type": "py",
        "inclusive_size": 0.955,
        "id": "src\\transformers\\models\\nllb\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.732,
        "file_type": "py",
        "inclusive_size": 7.732,
        "id": "src\\transformers\\models\\nemotron\\configuration_nemotron.py"
      },
      {
        "type": "file",
        "size": 15.633,
        "file_type": "py",
        "inclusive_size": 15.633,
        "id": "src\\transformers\\models\\nemotron\\convert_nemotron_nemo_to_hf.py"
      },
      {
        "type": "file",
        "size": 43.78,
        "file_type": "py",
        "inclusive_size": 43.78,
        "id": "src\\transformers\\models\\nemotron\\modeling_nemotron.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\nemotron\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.579,
        "file_type": "py",
        "inclusive_size": 7.579,
        "id": "src\\transformers\\models\\nanochat\\configuration_nanochat.py"
      },
      {
        "type": "file",
        "size": 11.859,
        "file_type": "py",
        "inclusive_size": 11.859,
        "id": "src\\transformers\\models\\nanochat\\convert_nanochat_checkpoints.py"
      },
      {
        "type": "file",
        "size": 22.327,
        "file_type": "py",
        "inclusive_size": 22.327,
        "id": "src\\transformers\\models\\nanochat\\modeling_nanochat.py"
      },
      {
        "type": "file",
        "size": 8.96,
        "file_type": "py",
        "inclusive_size": 8.96,
        "id": "src\\transformers\\models\\nanochat\\modular_nanochat.py"
      },
      {
        "type": "file",
        "size": 0.391,
        "file_type": "py",
        "inclusive_size": 0.391,
        "id": "src\\transformers\\models\\nanochat\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.802,
        "file_type": "py",
        "inclusive_size": 6.802,
        "id": "src\\transformers\\models\\myt5\\convert_myt5_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 15.45,
        "file_type": "py",
        "inclusive_size": 15.45,
        "id": "src\\transformers\\models\\myt5\\tokenization_myt5.py"
      },
      {
        "type": "file",
        "size": 0.955,
        "file_type": "py",
        "inclusive_size": 0.955,
        "id": "src\\transformers\\models\\myt5\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.888,
        "file_type": "py",
        "inclusive_size": 7.888,
        "id": "src\\transformers\\models\\mvp\\configuration_mvp.py"
      },
      {
        "type": "file",
        "size": 73.629,
        "file_type": "py",
        "inclusive_size": 73.629,
        "id": "src\\transformers\\models\\mvp\\modeling_mvp.py"
      },
      {
        "type": "file",
        "size": 1.067,
        "file_type": "py",
        "inclusive_size": 1.067,
        "id": "src\\transformers\\models\\mvp\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.657,
        "file_type": "py",
        "inclusive_size": 11.657,
        "id": "src\\transformers\\models\\musicgen_melody\\configuration_musicgen_melody.py"
      },
      {
        "type": "file",
        "size": 11.341,
        "file_type": "py",
        "inclusive_size": 11.341,
        "id": "src\\transformers\\models\\musicgen_melody\\convert_musicgen_melody_transformers.py"
      },
      {
        "type": "file",
        "size": 15.232,
        "file_type": "py",
        "inclusive_size": 15.232,
        "id": "src\\transformers\\models\\musicgen_melody\\feature_extraction_musicgen_melody.py"
      },
      {
        "type": "file",
        "size": 108.129,
        "file_type": "py",
        "inclusive_size": 108.129,
        "id": "src\\transformers\\models\\musicgen_melody\\modeling_musicgen_melody.py"
      },
      {
        "type": "file",
        "size": 5.024,
        "file_type": "py",
        "inclusive_size": 5.024,
        "id": "src\\transformers\\models\\musicgen_melody\\processing_musicgen_melody.py"
      },
      {
        "type": "file",
        "size": 1.011,
        "file_type": "py",
        "inclusive_size": 1.011,
        "id": "src\\transformers\\models\\musicgen_melody\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.35,
        "file_type": "py",
        "inclusive_size": 10.35,
        "id": "src\\transformers\\models\\musicgen\\configuration_musicgen.py"
      },
      {
        "type": "file",
        "size": 9.052,
        "file_type": "py",
        "inclusive_size": 9.052,
        "id": "src\\transformers\\models\\musicgen\\convert_musicgen_transformers.py"
      },
      {
        "type": "file",
        "size": 112.705,
        "file_type": "py",
        "inclusive_size": 112.705,
        "id": "src\\transformers\\models\\musicgen\\modeling_musicgen.py"
      },
      {
        "type": "file",
        "size": 3.36,
        "file_type": "py",
        "inclusive_size": 3.36,
        "id": "src\\transformers\\models\\musicgen\\processing_musicgen.py"
      },
      {
        "type": "file",
        "size": 1.036,
        "file_type": "py",
        "inclusive_size": 1.036,
        "id": "src\\transformers\\models\\musicgen\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.864,
        "file_type": "py",
        "inclusive_size": 6.864,
        "id": "src\\transformers\\models\\mt5\\configuration_mt5.py"
      },
      {
        "type": "file",
        "size": 77.284,
        "file_type": "py",
        "inclusive_size": 77.284,
        "id": "src\\transformers\\models\\mt5\\modeling_mt5.py"
      },
      {
        "type": "file",
        "size": 1.023,
        "file_type": "py",
        "inclusive_size": 1.023,
        "id": "src\\transformers\\models\\mt5\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.446,
        "file_type": "py",
        "inclusive_size": 6.446,
        "id": "src\\transformers\\models\\mra\\configuration_mra.py"
      },
      {
        "type": "file",
        "size": 4.245,
        "file_type": "py",
        "inclusive_size": 4.245,
        "id": "src\\transformers\\models\\mra\\convert_mra_pytorch_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 53.708,
        "file_type": "py",
        "inclusive_size": 53.708,
        "id": "src\\transformers\\models\\mra\\modeling_mra.py"
      },
      {
        "type": "file",
        "size": 0.987,
        "file_type": "py",
        "inclusive_size": 0.987,
        "id": "src\\transformers\\models\\mra\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.859,
        "file_type": "py",
        "inclusive_size": 10.859,
        "id": "src\\transformers\\models\\mpt\\configuration_mpt.py"
      },
      {
        "type": "file",
        "size": 32.741,
        "file_type": "py",
        "inclusive_size": 32.741,
        "id": "src\\transformers\\models\\mpt\\modeling_mpt.py"
      },
      {
        "type": "file",
        "size": 0.987,
        "file_type": "py",
        "inclusive_size": 0.987,
        "id": "src\\transformers\\models\\mpt\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.401,
        "file_type": "py",
        "inclusive_size": 5.401,
        "id": "src\\transformers\\models\\mpnet\\configuration_mpnet.py"
      },
      {
        "type": "file",
        "size": 35.067,
        "file_type": "py",
        "inclusive_size": 35.067,
        "id": "src\\transformers\\models\\mpnet\\modeling_mpnet.py"
      },
      {
        "type": "file",
        "size": 8.732,
        "file_type": "py",
        "inclusive_size": 8.732,
        "id": "src\\transformers\\models\\mpnet\\tokenization_mpnet.py"
      },
      {
        "type": "file",
        "size": 1.029,
        "file_type": "py",
        "inclusive_size": 1.029,
        "id": "src\\transformers\\models\\mpnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 17.613,
        "file_type": "py",
        "inclusive_size": 17.613,
        "id": "src\\transformers\\models\\moshi\\configuration_moshi.py"
      },
      {
        "type": "file",
        "size": 11.544,
        "file_type": "py",
        "inclusive_size": 11.544,
        "id": "src\\transformers\\models\\moshi\\convert_moshi_transformers.py"
      },
      {
        "type": "file",
        "size": 120.888,
        "file_type": "py",
        "inclusive_size": 120.888,
        "id": "src\\transformers\\models\\moshi\\modeling_moshi.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\moshi\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.115,
        "file_type": "py",
        "inclusive_size": 11.115,
        "id": "src\\transformers\\models\\moonshine\\configuration_moonshine.py"
      },
      {
        "type": "file",
        "size": 7.168,
        "file_type": "py",
        "inclusive_size": 7.168,
        "id": "src\\transformers\\models\\moonshine\\convert_usefulsensors_to_hf.py"
      },
      {
        "type": "file",
        "size": 49.822,
        "file_type": "py",
        "inclusive_size": 49.822,
        "id": "src\\transformers\\models\\moonshine\\modeling_moonshine.py"
      },
      {
        "type": "file",
        "size": 40.773,
        "file_type": "py",
        "inclusive_size": 40.773,
        "id": "src\\transformers\\models\\moonshine\\modular_moonshine.py"
      },
      {
        "type": "file",
        "size": 0.999,
        "file_type": "py",
        "inclusive_size": 0.999,
        "id": "src\\transformers\\models\\moonshine\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.003,
        "file_type": "py",
        "inclusive_size": 13.003,
        "id": "src\\transformers\\models\\modernbert_decoder\\configuration_modernbert_decoder.py"
      },
      {
        "type": "file",
        "size": 34.376,
        "file_type": "py",
        "inclusive_size": 34.376,
        "id": "src\\transformers\\models\\modernbert_decoder\\modeling_modernbert_decoder.py"
      },
      {
        "type": "file",
        "size": 37.006,
        "file_type": "py",
        "inclusive_size": 37.006,
        "id": "src\\transformers\\models\\modernbert_decoder\\modular_modernbert_decoder.py"
      },
      {
        "type": "file",
        "size": 1.022,
        "file_type": "py",
        "inclusive_size": 1.022,
        "id": "src\\transformers\\models\\modernbert_decoder\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.838,
        "file_type": "py",
        "inclusive_size": 14.838,
        "id": "src\\transformers\\models\\modernbert\\configuration_modernbert.py"
      },
      {
        "type": "file",
        "size": 37.652,
        "file_type": "py",
        "inclusive_size": 37.652,
        "id": "src\\transformers\\models\\modernbert\\modeling_modernbert.py"
      },
      {
        "type": "file",
        "size": 45.638,
        "file_type": "py",
        "inclusive_size": 45.638,
        "id": "src\\transformers\\models\\modernbert\\modular_modernbert.py"
      },
      {
        "type": "file",
        "size": 1.006,
        "file_type": "py",
        "inclusive_size": 1.006,
        "id": "src\\transformers\\models\\modernbert\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.333,
        "file_type": "py",
        "inclusive_size": 6.333,
        "id": "src\\transformers\\models\\mobilevitv2\\configuration_mobilevitv2.py"
      },
      {
        "type": "file",
        "size": 12.784,
        "file_type": "py",
        "inclusive_size": 12.784,
        "id": "src\\transformers\\models\\mobilevitv2\\convert_mlcvnets_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 33.939,
        "file_type": "py",
        "inclusive_size": 33.939,
        "id": "src\\transformers\\models\\mobilevitv2\\modeling_mobilevitv2.py"
      },
      {
        "type": "file",
        "size": 1.003,
        "file_type": "py",
        "inclusive_size": 1.003,
        "id": "src\\transformers\\models\\mobilevitv2\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.774,
        "file_type": "py",
        "inclusive_size": 6.774,
        "id": "src\\transformers\\models\\mobilevit\\configuration_mobilevit.py"
      },
      {
        "type": "file",
        "size": 12.49,
        "file_type": "py",
        "inclusive_size": 12.49,
        "id": "src\\transformers\\models\\mobilevit\\convert_mlcvnets_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 23.639,
        "file_type": "py",
        "inclusive_size": 23.639,
        "id": "src\\transformers\\models\\mobilevit\\image_processing_mobilevit.py"
      },
      {
        "type": "file",
        "size": 9.054,
        "file_type": "py",
        "inclusive_size": 9.054,
        "id": "src\\transformers\\models\\mobilevit\\image_processing_mobilevit_fast.py"
      },
      {
        "type": "file",
        "size": 35.146,
        "file_type": "py",
        "inclusive_size": 35.146,
        "id": "src\\transformers\\models\\mobilevit\\modeling_mobilevit.py"
      },
      {
        "type": "file",
        "size": 1.144,
        "file_type": "py",
        "inclusive_size": 1.144,
        "id": "src\\transformers\\models\\mobilevit\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.053,
        "file_type": "py",
        "inclusive_size": 6.053,
        "id": "src\\transformers\\models\\mobilenet_v2\\configuration_mobilenet_v2.py"
      },
      {
        "type": "file",
        "size": 15.486,
        "file_type": "py",
        "inclusive_size": 15.486,
        "id": "src\\transformers\\models\\mobilenet_v2\\convert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 24.693,
        "file_type": "py",
        "inclusive_size": 24.693,
        "id": "src\\transformers\\models\\mobilenet_v2\\image_processing_mobilenet_v2.py"
      },
      {
        "type": "file",
        "size": 9.017,
        "file_type": "py",
        "inclusive_size": 9.017,
        "id": "src\\transformers\\models\\mobilenet_v2\\image_processing_mobilenet_v2_fast.py"
      },
      {
        "type": "file",
        "size": 21.187,
        "file_type": "py",
        "inclusive_size": 21.187,
        "id": "src\\transformers\\models\\mobilenet_v2\\modeling_mobilenet_v2.py"
      },
      {
        "type": "file",
        "size": 1.159,
        "file_type": "py",
        "inclusive_size": 1.159,
        "id": "src\\transformers\\models\\mobilenet_v2\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.157,
        "file_type": "py",
        "inclusive_size": 4.157,
        "id": "src\\transformers\\models\\mobilenet_v1\\configuration_mobilenet_v1.py"
      },
      {
        "type": "file",
        "size": 9.257,
        "file_type": "py",
        "inclusive_size": 9.257,
        "id": "src\\transformers\\models\\mobilenet_v1\\convert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 14.985,
        "file_type": "py",
        "inclusive_size": 14.985,
        "id": "src\\transformers\\models\\mobilenet_v1\\image_processing_mobilenet_v1.py"
      },
      {
        "type": "file",
        "size": 1.315,
        "file_type": "py",
        "inclusive_size": 1.315,
        "id": "src\\transformers\\models\\mobilenet_v1\\image_processing_mobilenet_v1_fast.py"
      },
      {
        "type": "file",
        "size": 10.313,
        "file_type": "py",
        "inclusive_size": 10.313,
        "id": "src\\transformers\\models\\mobilenet_v1\\modeling_mobilenet_v1.py"
      },
      {
        "type": "file",
        "size": 1.159,
        "file_type": "py",
        "inclusive_size": 1.159,
        "id": "src\\transformers\\models\\mobilenet_v1\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.555,
        "file_type": "py",
        "inclusive_size": 7.555,
        "id": "src\\transformers\\models\\mobilebert\\configuration_mobilebert.py"
      },
      {
        "type": "file",
        "size": 5.409,
        "file_type": "py",
        "inclusive_size": 5.409,
        "id": "src\\transformers\\models\\mobilebert\\convert_mobilebert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 50.859,
        "file_type": "py",
        "inclusive_size": 50.859,
        "id": "src\\transformers\\models\\mobilebert\\modeling_mobilebert.py"
      },
      {
        "type": "file",
        "size": 1.002,
        "file_type": "py",
        "inclusive_size": 1.002,
        "id": "src\\transformers\\models\\mobilebert\\tokenization_mobilebert.py"
      },
      {
        "type": "file",
        "size": 1.087,
        "file_type": "py",
        "inclusive_size": 1.087,
        "id": "src\\transformers\\models\\mobilebert\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.926,
        "file_type": "py",
        "inclusive_size": 14.926,
        "id": "src\\transformers\\models\\mm_grounding_dino\\configuration_mm_grounding_dino.py"
      },
      {
        "type": "file",
        "size": 29.983,
        "file_type": "py",
        "inclusive_size": 29.983,
        "id": "src\\transformers\\models\\mm_grounding_dino\\convert_mm_grounding_dino_to_hf.py"
      },
      {
        "type": "file",
        "size": 129.724,
        "file_type": "py",
        "inclusive_size": 129.724,
        "id": "src\\transformers\\models\\mm_grounding_dino\\modeling_mm_grounding_dino.py"
      },
      {
        "type": "file",
        "size": 19.749,
        "file_type": "py",
        "inclusive_size": 19.749,
        "id": "src\\transformers\\models\\mm_grounding_dino\\modular_mm_grounding_dino.py"
      },
      {
        "type": "file",
        "size": 1.015,
        "file_type": "py",
        "inclusive_size": 1.015,
        "id": "src\\transformers\\models\\mm_grounding_dino\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.166,
        "file_type": "py",
        "inclusive_size": 10.166,
        "id": "src\\transformers\\models\\mluke\\convert_mluke_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 88.121,
        "file_type": "py",
        "inclusive_size": 88.121,
        "id": "src\\transformers\\models\\mluke\\tokenization_mluke.py"
      },
      {
        "type": "file",
        "size": 0.956,
        "file_type": "py",
        "inclusive_size": 0.956,
        "id": "src\\transformers\\models\\mluke\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.234,
        "file_type": "py",
        "inclusive_size": 15.234,
        "id": "src\\transformers\\models\\mllama\\configuration_mllama.py"
      },
      {
        "type": "file",
        "size": 29.627,
        "file_type": "py",
        "inclusive_size": 29.627,
        "id": "src\\transformers\\models\\mllama\\convert_mllama_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 37.265,
        "file_type": "py",
        "inclusive_size": 37.265,
        "id": "src\\transformers\\models\\mllama\\image_processing_mllama.py"
      },
      {
        "type": "file",
        "size": 15.68,
        "file_type": "py",
        "inclusive_size": 15.68,
        "id": "src\\transformers\\models\\mllama\\image_processing_mllama_fast.py"
      },
      {
        "type": "file",
        "size": 80.077,
        "file_type": "py",
        "inclusive_size": 80.077,
        "id": "src\\transformers\\models\\mllama\\modeling_mllama.py"
      },
      {
        "type": "file",
        "size": 14.026,
        "file_type": "py",
        "inclusive_size": 14.026,
        "id": "src\\transformers\\models\\mllama\\processing_mllama.py"
      },
      {
        "type": "file",
        "size": 1.121,
        "file_type": "py",
        "inclusive_size": 1.121,
        "id": "src\\transformers\\models\\mllama\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.789,
        "file_type": "py",
        "inclusive_size": 5.789,
        "id": "src\\transformers\\models\\mlcd\\configuration_mlcd.py"
      },
      {
        "type": "file",
        "size": 13.425,
        "file_type": "py",
        "inclusive_size": 13.425,
        "id": "src\\transformers\\models\\mlcd\\convert_mlcd_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 24.47,
        "file_type": "py",
        "inclusive_size": 24.47,
        "id": "src\\transformers\\models\\mlcd\\modeling_mlcd.py"
      },
      {
        "type": "file",
        "size": 20.437,
        "file_type": "py",
        "inclusive_size": 20.437,
        "id": "src\\transformers\\models\\mlcd\\modular_mlcd.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\mlcd\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.685,
        "file_type": "py",
        "inclusive_size": 9.685,
        "id": "src\\transformers\\models\\mixtral\\configuration_mixtral.py"
      },
      {
        "type": "file",
        "size": 9.047,
        "file_type": "py",
        "inclusive_size": 9.047,
        "id": "src\\transformers\\models\\mixtral\\convert_mixtral_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 31.379,
        "file_type": "py",
        "inclusive_size": 31.379,
        "id": "src\\transformers\\models\\mixtral\\modeling_mixtral.py"
      },
      {
        "type": "file",
        "size": 18.865,
        "file_type": "py",
        "inclusive_size": 18.865,
        "id": "src\\transformers\\models\\mixtral\\modular_mixtral.py"
      },
      {
        "type": "file",
        "size": 1.015,
        "file_type": "py",
        "inclusive_size": 1.015,
        "id": "src\\transformers\\models\\mixtral\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.922,
        "file_type": "py",
        "inclusive_size": 5.922,
        "id": "src\\transformers\\models\\mistral3\\configuration_mistral3.py"
      },
      {
        "type": "file",
        "size": 11.738,
        "file_type": "py",
        "inclusive_size": 11.738,
        "id": "src\\transformers\\models\\mistral3\\convert_mistral3_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 22.735,
        "file_type": "py",
        "inclusive_size": 22.735,
        "id": "src\\transformers\\models\\mistral3\\modeling_mistral3.py"
      },
      {
        "type": "file",
        "size": 14.003,
        "file_type": "py",
        "inclusive_size": 14.003,
        "id": "src\\transformers\\models\\mistral3\\modular_mistral3.py"
      },
      {
        "type": "file",
        "size": 1.036,
        "file_type": "py",
        "inclusive_size": 1.036,
        "id": "src\\transformers\\models\\mistral3\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.628,
        "file_type": "py",
        "inclusive_size": 8.628,
        "id": "src\\transformers\\models\\mistral\\configuration_mistral.py"
      },
      {
        "type": "file",
        "size": 11.338,
        "file_type": "py",
        "inclusive_size": 11.338,
        "id": "src\\transformers\\models\\mistral\\convert_mistral_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 21.794,
        "file_type": "py",
        "inclusive_size": 21.794,
        "id": "src\\transformers\\models\\mistral\\modeling_mistral.py"
      },
      {
        "type": "file",
        "size": 7.451,
        "file_type": "py",
        "inclusive_size": 7.451,
        "id": "src\\transformers\\models\\mistral\\modular_mistral.py"
      },
      {
        "type": "file",
        "size": 1.015,
        "file_type": "py",
        "inclusive_size": 1.015,
        "id": "src\\transformers\\models\\mistral\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.773,
        "file_type": "py",
        "inclusive_size": 9.773,
        "id": "src\\transformers\\models\\ministral3\\configuration_ministral3.py"
      },
      {
        "type": "file",
        "size": 15.354,
        "file_type": "py",
        "inclusive_size": 15.354,
        "id": "src\\transformers\\models\\ministral3\\convert_ministral3_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 22.471,
        "file_type": "py",
        "inclusive_size": 22.471,
        "id": "src\\transformers\\models\\ministral3\\modeling_ministral3.py"
      },
      {
        "type": "file",
        "size": 4.039,
        "file_type": "py",
        "inclusive_size": 4.039,
        "id": "src\\transformers\\models\\ministral3\\modular_ministral3.py"
      },
      {
        "type": "file",
        "size": 1.021,
        "file_type": "py",
        "inclusive_size": 1.021,
        "id": "src\\transformers\\models\\ministral3\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.8,
        "file_type": "py",
        "inclusive_size": 8.8,
        "id": "src\\transformers\\models\\ministral\\configuration_ministral.py"
      },
      {
        "type": "file",
        "size": 22.613,
        "file_type": "py",
        "inclusive_size": 22.613,
        "id": "src\\transformers\\models\\ministral\\modeling_ministral.py"
      },
      {
        "type": "file",
        "size": 12.296,
        "file_type": "py",
        "inclusive_size": 12.296,
        "id": "src\\transformers\\models\\ministral\\modular_ministral.py"
      },
      {
        "type": "file",
        "size": 1.019,
        "file_type": "py",
        "inclusive_size": 1.019,
        "id": "src\\transformers\\models\\ministral\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.12,
        "file_type": "py",
        "inclusive_size": 10.12,
        "id": "src\\transformers\\models\\minimax_m2\\configuration_minimax_m2.py"
      },
      {
        "type": "file",
        "size": 31.603,
        "file_type": "py",
        "inclusive_size": 31.603,
        "id": "src\\transformers\\models\\minimax_m2\\modeling_minimax_m2.py"
      },
      {
        "type": "file",
        "size": 15.867,
        "file_type": "py",
        "inclusive_size": 15.867,
        "id": "src\\transformers\\models\\minimax_m2\\modular_minimax_m2.py"
      },
      {
        "type": "file",
        "size": 1.002,
        "file_type": "py",
        "inclusive_size": 1.002,
        "id": "src\\transformers\\models\\minimax_m2\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.624,
        "file_type": "py",
        "inclusive_size": 12.624,
        "id": "src\\transformers\\models\\minimax\\configuration_minimax.py"
      },
      {
        "type": "file",
        "size": 41.296,
        "file_type": "py",
        "inclusive_size": 41.296,
        "id": "src\\transformers\\models\\minimax\\modeling_minimax.py"
      },
      {
        "type": "file",
        "size": 29.561,
        "file_type": "py",
        "inclusive_size": 29.561,
        "id": "src\\transformers\\models\\minimax\\modular_minimax.py"
      },
      {
        "type": "file",
        "size": 1.013,
        "file_type": "py",
        "inclusive_size": 1.013,
        "id": "src\\transformers\\models\\minimax\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.582,
        "file_type": "py",
        "inclusive_size": 14.582,
        "id": "src\\transformers\\models\\mimi\\configuration_mimi.py"
      },
      {
        "type": "file",
        "size": 6.793,
        "file_type": "py",
        "inclusive_size": 6.793,
        "id": "src\\transformers\\models\\mimi\\convert_mimi_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 80.922,
        "file_type": "py",
        "inclusive_size": 80.922,
        "id": "src\\transformers\\models\\mimi\\modeling_mimi.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\mimi\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.795,
        "file_type": "py",
        "inclusive_size": 5.795,
        "id": "src\\transformers\\models\\mgp_str\\configuration_mgp_str.py"
      },
      {
        "type": "file",
        "size": 18.435,
        "file_type": "py",
        "inclusive_size": 18.435,
        "id": "src\\transformers\\models\\mgp_str\\modeling_mgp_str.py"
      },
      {
        "type": "file",
        "size": 7.804,
        "file_type": "py",
        "inclusive_size": 7.804,
        "id": "src\\transformers\\models\\mgp_str\\processing_mgp_str.py"
      },
      {
        "type": "file",
        "size": 3.791,
        "file_type": "py",
        "inclusive_size": 3.791,
        "id": "src\\transformers\\models\\mgp_str\\tokenization_mgp_str.py"
      },
      {
        "type": "file",
        "size": 1.073,
        "file_type": "py",
        "inclusive_size": 1.073,
        "id": "src\\transformers\\models\\mgp_str\\__init__.py"
      },
      {
        "type": "file",
        "size": 18.052,
        "file_type": "py",
        "inclusive_size": 18.052,
        "id": "src\\transformers\\models\\metaclip_2\\configuration_metaclip_2.py"
      },
      {
        "type": "file",
        "size": 16.966,
        "file_type": "py",
        "inclusive_size": 16.966,
        "id": "src\\transformers\\models\\metaclip_2\\convert_metaclip_2_to_hf.py"
      },
      {
        "type": "file",
        "size": 51.489,
        "file_type": "py",
        "inclusive_size": 51.489,
        "id": "src\\transformers\\models\\metaclip_2\\modeling_metaclip_2.py"
      },
      {
        "type": "file",
        "size": 34.141,
        "file_type": "py",
        "inclusive_size": 34.141,
        "id": "src\\transformers\\models\\metaclip_2\\modular_metaclip_2.py"
      },
      {
        "type": "file",
        "size": 1.001,
        "file_type": "py",
        "inclusive_size": 1.001,
        "id": "src\\transformers\\models\\metaclip_2\\__init__.py"
      },
      {
        "type": "file",
        "size": 37.65,
        "file_type": "py",
        "inclusive_size": 37.65,
        "id": "src\\transformers\\models\\megatron_gpt2\\checkpoint_reshaping_and_interoperability.py"
      },
      {
        "type": "file",
        "size": 17.656,
        "file_type": "py",
        "inclusive_size": 17.656,
        "id": "src\\transformers\\models\\megatron_gpt2\\convert_megatron_gpt2_checkpoint.py"
      },
      {
        "type": "file",
        "size": 0.0,
        "file_type": "py",
        "inclusive_size": 0.0,
        "id": "src\\transformers\\models\\megatron_gpt2\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.129,
        "file_type": "py",
        "inclusive_size": 6.129,
        "id": "src\\transformers\\models\\megatron_bert\\configuration_megatron_bert.py"
      },
      {
        "type": "file",
        "size": 13.703,
        "file_type": "py",
        "inclusive_size": 13.703,
        "id": "src\\transformers\\models\\megatron_bert\\convert_megatron_bert_checkpoint.py"
      },
      {
        "type": "file",
        "size": 61.412,
        "file_type": "py",
        "inclusive_size": 61.412,
        "id": "src\\transformers\\models\\megatron_bert\\modeling_megatron_bert.py"
      },
      {
        "type": "file",
        "size": 1.007,
        "file_type": "py",
        "inclusive_size": 1.007,
        "id": "src\\transformers\\models\\megatron_bert\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.853,
        "file_type": "py",
        "inclusive_size": 14.853,
        "id": "src\\transformers\\models\\mbart50\\tokenization_mbart50.py"
      },
      {
        "type": "file",
        "size": 0.958,
        "file_type": "py",
        "inclusive_size": 0.958,
        "id": "src\\transformers\\models\\mbart50\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.741,
        "file_type": "py",
        "inclusive_size": 7.741,
        "id": "src\\transformers\\models\\mbart\\configuration_mbart.py"
      },
      {
        "type": "file",
        "size": 3.054,
        "file_type": "py",
        "inclusive_size": 3.054,
        "id": "src\\transformers\\models\\mbart\\convert_mbart_original_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 70.728,
        "file_type": "py",
        "inclusive_size": 70.728,
        "id": "src\\transformers\\models\\mbart\\modeling_mbart.py"
      },
      {
        "type": "file",
        "size": 8.473,
        "file_type": "py",
        "inclusive_size": 8.473,
        "id": "src\\transformers\\models\\mbart\\tokenization_mbart.py"
      },
      {
        "type": "file",
        "size": 1.029,
        "file_type": "py",
        "inclusive_size": 1.029,
        "id": "src\\transformers\\models\\mbart\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.588,
        "file_type": "py",
        "inclusive_size": 9.588,
        "id": "src\\transformers\\models\\maskformer\\configuration_maskformer.py"
      },
      {
        "type": "file",
        "size": 7.238,
        "file_type": "py",
        "inclusive_size": 7.238,
        "id": "src\\transformers\\models\\maskformer\\configuration_maskformer_swin.py"
      },
      {
        "type": "file",
        "size": 32.12,
        "file_type": "py",
        "inclusive_size": 32.12,
        "id": "src\\transformers\\models\\maskformer\\convert_maskformer_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 21.489,
        "file_type": "py",
        "inclusive_size": 21.489,
        "id": "src\\transformers\\models\\maskformer\\convert_maskformer_resnet_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 21.071,
        "file_type": "py",
        "inclusive_size": 21.071,
        "id": "src\\transformers\\models\\maskformer\\convert_maskformer_swin_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 56.442,
        "file_type": "py",
        "inclusive_size": 56.442,
        "id": "src\\transformers\\models\\maskformer\\image_processing_maskformer.py"
      },
      {
        "type": "file",
        "size": 31.339,
        "file_type": "py",
        "inclusive_size": 31.339,
        "id": "src\\transformers\\models\\maskformer\\image_processing_maskformer_fast.py"
      },
      {
        "type": "file",
        "size": 85.187,
        "file_type": "py",
        "inclusive_size": 85.187,
        "id": "src\\transformers\\models\\maskformer\\modeling_maskformer.py"
      },
      {
        "type": "file",
        "size": 37.659,
        "file_type": "py",
        "inclusive_size": 37.659,
        "id": "src\\transformers\\models\\maskformer\\modeling_maskformer_swin.py"
      },
      {
        "type": "file",
        "size": 1.242,
        "file_type": "py",
        "inclusive_size": 1.242,
        "id": "src\\transformers\\models\\maskformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.877,
        "file_type": "py",
        "inclusive_size": 11.877,
        "id": "src\\transformers\\models\\mask2former\\configuration_mask2former.py"
      },
      {
        "type": "file",
        "size": 45.698,
        "file_type": "py",
        "inclusive_size": 45.698,
        "id": "src\\transformers\\models\\mask2former\\convert_mask2former_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 57.877,
        "file_type": "py",
        "inclusive_size": 57.877,
        "id": "src\\transformers\\models\\mask2former\\image_processing_mask2former.py"
      },
      {
        "type": "file",
        "size": 32.348,
        "file_type": "py",
        "inclusive_size": 32.348,
        "id": "src\\transformers\\models\\mask2former\\image_processing_mask2former_fast.py"
      },
      {
        "type": "file",
        "size": 117.588,
        "file_type": "py",
        "inclusive_size": 117.588,
        "id": "src\\transformers\\models\\mask2former\\modeling_mask2former.py"
      },
      {
        "type": "file",
        "size": 15.516,
        "file_type": "py",
        "inclusive_size": 15.516,
        "id": "src\\transformers\\models\\mask2former\\modular_mask2former.py"
      },
      {
        "type": "file",
        "size": 1.104,
        "file_type": "py",
        "inclusive_size": 1.104,
        "id": "src\\transformers\\models\\mask2former\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.203,
        "file_type": "py",
        "inclusive_size": 7.203,
        "id": "src\\transformers\\models\\markuplm\\configuration_markuplm.py"
      },
      {
        "type": "file",
        "size": 6.428,
        "file_type": "py",
        "inclusive_size": 6.428,
        "id": "src\\transformers\\models\\markuplm\\feature_extraction_markuplm.py"
      },
      {
        "type": "file",
        "size": 39.414,
        "file_type": "py",
        "inclusive_size": 39.414,
        "id": "src\\transformers\\models\\markuplm\\modeling_markuplm.py"
      },
      {
        "type": "file",
        "size": 5.71,
        "file_type": "py",
        "inclusive_size": 5.71,
        "id": "src\\transformers\\models\\markuplm\\processing_markuplm.py"
      },
      {
        "type": "file",
        "size": 48.683,
        "file_type": "py",
        "inclusive_size": 48.683,
        "id": "src\\transformers\\models\\markuplm\\tokenization_markuplm.py"
      },
      {
        "type": "file",
        "size": 1.083,
        "file_type": "py",
        "inclusive_size": 1.083,
        "id": "src\\transformers\\models\\markuplm\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.837,
        "file_type": "py",
        "inclusive_size": 7.837,
        "id": "src\\transformers\\models\\marian\\configuration_marian.py"
      },
      {
        "type": "file",
        "size": 36.354,
        "file_type": "py",
        "inclusive_size": 36.354,
        "id": "src\\transformers\\models\\marian\\convert_marian_tatoeba_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 27.082,
        "file_type": "py",
        "inclusive_size": 27.082,
        "id": "src\\transformers\\models\\marian\\convert_marian_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 61.288,
        "file_type": "py",
        "inclusive_size": 61.288,
        "id": "src\\transformers\\models\\marian\\modeling_marian.py"
      },
      {
        "type": "file",
        "size": 18.086,
        "file_type": "py",
        "inclusive_size": 18.086,
        "id": "src\\transformers\\models\\marian\\tokenization_marian.py"
      },
      {
        "type": "file",
        "size": 1.032,
        "file_type": "py",
        "inclusive_size": 1.032,
        "id": "src\\transformers\\models\\marian\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.184,
        "file_type": "py",
        "inclusive_size": 8.184,
        "id": "src\\transformers\\models\\mamba2\\configuration_mamba2.py"
      },
      {
        "type": "file",
        "size": 7.479,
        "file_type": "py",
        "inclusive_size": 7.479,
        "id": "src\\transformers\\models\\mamba2\\convert_mamba2_ssm_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 47.603,
        "file_type": "py",
        "inclusive_size": 47.603,
        "id": "src\\transformers\\models\\mamba2\\modeling_mamba2.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\mamba2\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.54,
        "file_type": "py",
        "inclusive_size": 7.54,
        "id": "src\\transformers\\models\\mamba\\configuration_mamba.py"
      },
      {
        "type": "file",
        "size": 6.433,
        "file_type": "py",
        "inclusive_size": 6.433,
        "id": "src\\transformers\\models\\mamba\\convert_mamba_ssm_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 39.154,
        "file_type": "py",
        "inclusive_size": 39.154,
        "id": "src\\transformers\\models\\mamba\\modeling_mamba.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\mamba\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.131,
        "file_type": "py",
        "inclusive_size": 7.131,
        "id": "src\\transformers\\models\\m2m_100\\configuration_m2m_100.py"
      },
      {
        "type": "file",
        "size": 3.178,
        "file_type": "py",
        "inclusive_size": 3.178,
        "id": "src\\transformers\\models\\m2m_100\\convert_m2m100_original_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 51.568,
        "file_type": "py",
        "inclusive_size": 51.568,
        "id": "src\\transformers\\models\\m2m_100\\modeling_m2m_100.py"
      },
      {
        "type": "file",
        "size": 16.373,
        "file_type": "py",
        "inclusive_size": 16.373,
        "id": "src\\transformers\\models\\m2m_100\\tokenization_m2m_100.py"
      },
      {
        "type": "file",
        "size": 1.035,
        "file_type": "py",
        "inclusive_size": 1.035,
        "id": "src\\transformers\\models\\m2m_100\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.313,
        "file_type": "py",
        "inclusive_size": 9.313,
        "id": "src\\transformers\\models\\lxmert\\configuration_lxmert.py"
      },
      {
        "type": "file",
        "size": 5.078,
        "file_type": "py",
        "inclusive_size": 5.078,
        "id": "src\\transformers\\models\\lxmert\\convert_lxmert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 58.743,
        "file_type": "py",
        "inclusive_size": 58.743,
        "id": "src\\transformers\\models\\lxmert\\modeling_lxmert.py"
      },
      {
        "type": "file",
        "size": 1.067,
        "file_type": "py",
        "inclusive_size": 1.067,
        "id": "src\\transformers\\models\\lxmert\\__init__.py"
      },
      {
        "type": "file",
        "size": 19.489,
        "file_type": "py",
        "inclusive_size": 19.489,
        "id": "src\\transformers\\models\\lw_detr\\configuration_lw_detr.py"
      },
      {
        "type": "file",
        "size": 24.838,
        "file_type": "py",
        "inclusive_size": 24.838,
        "id": "src\\transformers\\models\\lw_detr\\convert_lw_detr_to_hf.py"
      },
      {
        "type": "file",
        "size": 75.342,
        "file_type": "py",
        "inclusive_size": 75.342,
        "id": "src\\transformers\\models\\lw_detr\\modeling_lw_detr.py"
      },
      {
        "type": "file",
        "size": 71.894,
        "file_type": "py",
        "inclusive_size": 71.894,
        "id": "src\\transformers\\models\\lw_detr\\modular_lw_detr.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\lw_detr\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.859,
        "file_type": "py",
        "inclusive_size": 6.859,
        "id": "src\\transformers\\models\\luke\\configuration_luke.py"
      },
      {
        "type": "file",
        "size": 7.463,
        "file_type": "py",
        "inclusive_size": 7.463,
        "id": "src\\transformers\\models\\luke\\convert_luke_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 95.981,
        "file_type": "py",
        "inclusive_size": 95.981,
        "id": "src\\transformers\\models\\luke\\modeling_luke.py"
      },
      {
        "type": "file",
        "size": 82.419,
        "file_type": "py",
        "inclusive_size": 82.419,
        "id": "src\\transformers\\models\\luke\\tokenization_luke.py"
      },
      {
        "type": "file",
        "size": 1.026,
        "file_type": "py",
        "inclusive_size": 1.026,
        "id": "src\\transformers\\models\\luke\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.196,
        "file_type": "py",
        "inclusive_size": 7.196,
        "id": "src\\transformers\\models\\longt5\\configuration_longt5.py"
      },
      {
        "type": "file",
        "size": 92.592,
        "file_type": "py",
        "inclusive_size": 92.592,
        "id": "src\\transformers\\models\\longt5\\modeling_longt5.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\longt5\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.324,
        "file_type": "py",
        "inclusive_size": 6.324,
        "id": "src\\transformers\\models\\longformer\\configuration_longformer.py"
      },
      {
        "type": "file",
        "size": 3.029,
        "file_type": "py",
        "inclusive_size": 3.029,
        "id": "src\\transformers\\models\\longformer\\convert_longformer_original_pytorch_lightning_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 103.166,
        "file_type": "py",
        "inclusive_size": 103.166,
        "id": "src\\transformers\\models\\longformer\\modeling_longformer.py"
      },
      {
        "type": "file",
        "size": 1.088,
        "file_type": "py",
        "inclusive_size": 1.088,
        "id": "src\\transformers\\models\\longformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.915,
        "file_type": "py",
        "inclusive_size": 11.915,
        "id": "src\\transformers\\models\\longcat_flash\\configuration_longcat_flash.py"
      },
      {
        "type": "file",
        "size": 32.084,
        "file_type": "py",
        "inclusive_size": 32.084,
        "id": "src\\transformers\\models\\longcat_flash\\modeling_longcat_flash.py"
      },
      {
        "type": "file",
        "size": 18.097,
        "file_type": "py",
        "inclusive_size": 18.097,
        "id": "src\\transformers\\models\\longcat_flash\\modular_longcat_flash.py"
      },
      {
        "type": "file",
        "size": 1.025,
        "file_type": "py",
        "inclusive_size": 1.025,
        "id": "src\\transformers\\models\\longcat_flash\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.426,
        "file_type": "py",
        "inclusive_size": 8.426,
        "id": "src\\transformers\\models\\llava_onevision\\configuration_llava_onevision.py"
      },
      {
        "type": "file",
        "size": 19.95,
        "file_type": "py",
        "inclusive_size": 19.95,
        "id": "src\\transformers\\models\\llava_onevision\\convert_llava_onevision_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 38.36,
        "file_type": "py",
        "inclusive_size": 38.36,
        "id": "src\\transformers\\models\\llava_onevision\\image_processing_llava_onevision.py"
      },
      {
        "type": "file",
        "size": 14.137,
        "file_type": "py",
        "inclusive_size": 14.137,
        "id": "src\\transformers\\models\\llava_onevision\\image_processing_llava_onevision_fast.py"
      },
      {
        "type": "file",
        "size": 45.299,
        "file_type": "py",
        "inclusive_size": 45.299,
        "id": "src\\transformers\\models\\llava_onevision\\modeling_llava_onevision.py"
      },
      {
        "type": "file",
        "size": 34.179,
        "file_type": "py",
        "inclusive_size": 34.179,
        "id": "src\\transformers\\models\\llava_onevision\\modular_llava_onevision.py"
      },
      {
        "type": "file",
        "size": 14.146,
        "file_type": "py",
        "inclusive_size": 14.146,
        "id": "src\\transformers\\models\\llava_onevision\\processing_llava_onevision.py"
      },
      {
        "type": "file",
        "size": 1.343,
        "file_type": "py",
        "inclusive_size": 1.343,
        "id": "src\\transformers\\models\\llava_onevision\\video_processing_llava_onevision.py"
      },
      {
        "type": "file",
        "size": 1.218,
        "file_type": "py",
        "inclusive_size": 1.218,
        "id": "src\\transformers\\models\\llava_onevision\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.631,
        "file_type": "py",
        "inclusive_size": 8.631,
        "id": "src\\transformers\\models\\llava_next_video\\configuration_llava_next_video.py"
      },
      {
        "type": "file",
        "size": 10.438,
        "file_type": "py",
        "inclusive_size": 10.438,
        "id": "src\\transformers\\models\\llava_next_video\\convert_llava_next_video_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 46.579,
        "file_type": "py",
        "inclusive_size": 46.579,
        "id": "src\\transformers\\models\\llava_next_video\\modeling_llava_next_video.py"
      },
      {
        "type": "file",
        "size": 35.268,
        "file_type": "py",
        "inclusive_size": 35.268,
        "id": "src\\transformers\\models\\llava_next_video\\modular_llava_next_video.py"
      },
      {
        "type": "file",
        "size": 12.147,
        "file_type": "py",
        "inclusive_size": 12.147,
        "id": "src\\transformers\\models\\llava_next_video\\processing_llava_next_video.py"
      },
      {
        "type": "file",
        "size": 1.333,
        "file_type": "py",
        "inclusive_size": 1.333,
        "id": "src\\transformers\\models\\llava_next_video\\video_processing_llava_next_video.py"
      },
      {
        "type": "file",
        "size": 1.113,
        "file_type": "py",
        "inclusive_size": 1.113,
        "id": "src\\transformers\\models\\llava_next_video\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.775,
        "file_type": "py",
        "inclusive_size": 6.775,
        "id": "src\\transformers\\models\\llava_next\\configuration_llava_next.py"
      },
      {
        "type": "file",
        "size": 20.807,
        "file_type": "py",
        "inclusive_size": 20.807,
        "id": "src\\transformers\\models\\llava_next\\convert_llava_next_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 35.817,
        "file_type": "py",
        "inclusive_size": 35.817,
        "id": "src\\transformers\\models\\llava_next\\image_processing_llava_next.py"
      },
      {
        "type": "file",
        "size": 10.102,
        "file_type": "py",
        "inclusive_size": 10.102,
        "id": "src\\transformers\\models\\llava_next\\image_processing_llava_next_fast.py"
      },
      {
        "type": "file",
        "size": 35.62,
        "file_type": "py",
        "inclusive_size": 35.62,
        "id": "src\\transformers\\models\\llava_next\\modeling_llava_next.py"
      },
      {
        "type": "file",
        "size": 10.613,
        "file_type": "py",
        "inclusive_size": 10.613,
        "id": "src\\transformers\\models\\llava_next\\processing_llava_next.py"
      },
      {
        "type": "file",
        "size": 1.141,
        "file_type": "py",
        "inclusive_size": 1.141,
        "id": "src\\transformers\\models\\llava_next\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.353,
        "file_type": "py",
        "inclusive_size": 6.353,
        "id": "src\\transformers\\models\\llava\\configuration_llava.py"
      },
      {
        "type": "file",
        "size": 7.775,
        "file_type": "py",
        "inclusive_size": 7.775,
        "id": "src\\transformers\\models\\llava\\convert_llava_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 20.812,
        "file_type": "py",
        "inclusive_size": 20.812,
        "id": "src\\transformers\\models\\llava\\image_processing_llava.py"
      },
      {
        "type": "file",
        "size": 6.072,
        "file_type": "py",
        "inclusive_size": 6.072,
        "id": "src\\transformers\\models\\llava\\image_processing_llava_fast.py"
      },
      {
        "type": "file",
        "size": 18.817,
        "file_type": "py",
        "inclusive_size": 18.817,
        "id": "src\\transformers\\models\\llava\\modeling_llava.py"
      },
      {
        "type": "file",
        "size": 7.447,
        "file_type": "py",
        "inclusive_size": 7.447,
        "id": "src\\transformers\\models\\llava\\processing_llava.py"
      },
      {
        "type": "file",
        "size": 1.074,
        "file_type": "py",
        "inclusive_size": 1.074,
        "id": "src\\transformers\\models\\llava\\__init__.py"
      },
      {
        "type": "file",
        "size": 20.12,
        "file_type": "py",
        "inclusive_size": 20.12,
        "id": "src\\transformers\\models\\llama4\\configuration_llama4.py"
      },
      {
        "type": "file",
        "size": 38.6,
        "file_type": "py",
        "inclusive_size": 38.6,
        "id": "src\\transformers\\models\\llama4\\convert_llama4_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 17.001,
        "file_type": "py",
        "inclusive_size": 17.001,
        "id": "src\\transformers\\models\\llama4\\image_processing_llama4_fast.py"
      },
      {
        "type": "file",
        "size": 59.896,
        "file_type": "py",
        "inclusive_size": 59.896,
        "id": "src\\transformers\\models\\llama4\\modeling_llama4.py"
      },
      {
        "type": "file",
        "size": 14.177,
        "file_type": "py",
        "inclusive_size": 14.177,
        "id": "src\\transformers\\models\\llama4\\processing_llama4.py"
      },
      {
        "type": "file",
        "size": 1.078,
        "file_type": "py",
        "inclusive_size": 1.078,
        "id": "src\\transformers\\models\\llama4\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.23,
        "file_type": "py",
        "inclusive_size": 9.23,
        "id": "src\\transformers\\models\\llama\\configuration_llama.py"
      },
      {
        "type": "file",
        "size": 23.884,
        "file_type": "py",
        "inclusive_size": 23.884,
        "id": "src\\transformers\\models\\llama\\convert_llama_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 21.765,
        "file_type": "py",
        "inclusive_size": 21.765,
        "id": "src\\transformers\\models\\llama\\modeling_llama.py"
      },
      {
        "type": "file",
        "size": 6.346,
        "file_type": "py",
        "inclusive_size": 6.346,
        "id": "src\\transformers\\models\\llama\\tokenization_llama.py"
      },
      {
        "type": "file",
        "size": 1.029,
        "file_type": "py",
        "inclusive_size": 1.029,
        "id": "src\\transformers\\models\\llama\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.107,
        "file_type": "py",
        "inclusive_size": 6.107,
        "id": "src\\transformers\\models\\lilt\\configuration_lilt.py"
      },
      {
        "type": "file",
        "size": 42.307,
        "file_type": "py",
        "inclusive_size": 42.307,
        "id": "src\\transformers\\models\\lilt\\modeling_lilt.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\lilt\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.817,
        "file_type": "py",
        "inclusive_size": 5.817,
        "id": "src\\transformers\\models\\lighton_ocr\\configuration_lighton_ocr.py"
      },
      {
        "type": "file",
        "size": 20.838,
        "file_type": "py",
        "inclusive_size": 20.838,
        "id": "src\\transformers\\models\\lighton_ocr\\modeling_lighton_ocr.py"
      },
      {
        "type": "file",
        "size": 16.767,
        "file_type": "py",
        "inclusive_size": 16.767,
        "id": "src\\transformers\\models\\lighton_ocr\\modular_lighton_ocr.py"
      },
      {
        "type": "file",
        "size": 10.289,
        "file_type": "py",
        "inclusive_size": 10.289,
        "id": "src\\transformers\\models\\lighton_ocr\\processing_lighton_ocr.py"
      },
      {
        "type": "file",
        "size": 1.071,
        "file_type": "py",
        "inclusive_size": 1.071,
        "id": "src\\transformers\\models\\lighton_ocr\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.135,
        "file_type": "py",
        "inclusive_size": 8.135,
        "id": "src\\transformers\\models\\lightglue\\configuration_lightglue.py"
      },
      {
        "type": "file",
        "size": 11.121,
        "file_type": "py",
        "inclusive_size": 11.121,
        "id": "src\\transformers\\models\\lightglue\\convert_lightglue_to_hf.py"
      },
      {
        "type": "file",
        "size": 22.104,
        "file_type": "py",
        "inclusive_size": 22.104,
        "id": "src\\transformers\\models\\lightglue\\image_processing_lightglue.py"
      },
      {
        "type": "file",
        "size": 13.415,
        "file_type": "py",
        "inclusive_size": 13.415,
        "id": "src\\transformers\\models\\lightglue\\image_processing_lightglue_fast.py"
      },
      {
        "type": "file",
        "size": 43.378,
        "file_type": "py",
        "inclusive_size": 43.378,
        "id": "src\\transformers\\models\\lightglue\\modeling_lightglue.py"
      },
      {
        "type": "file",
        "size": 45.91,
        "file_type": "py",
        "inclusive_size": 45.91,
        "id": "src\\transformers\\models\\lightglue\\modular_lightglue.py"
      },
      {
        "type": "file",
        "size": 1.096,
        "file_type": "py",
        "inclusive_size": 1.096,
        "id": "src\\transformers\\models\\lightglue\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.339,
        "file_type": "py",
        "inclusive_size": 4.339,
        "id": "src\\transformers\\models\\lfm2_vl\\configuration_lfm2_vl.py"
      },
      {
        "type": "file",
        "size": 23.597,
        "file_type": "py",
        "inclusive_size": 23.597,
        "id": "src\\transformers\\models\\lfm2_vl\\image_processing_lfm2_vl_fast.py"
      },
      {
        "type": "file",
        "size": 21.031,
        "file_type": "py",
        "inclusive_size": 21.031,
        "id": "src\\transformers\\models\\lfm2_vl\\modeling_lfm2_vl.py"
      },
      {
        "type": "file",
        "size": 14.944,
        "file_type": "py",
        "inclusive_size": 14.944,
        "id": "src\\transformers\\models\\lfm2_vl\\modular_lfm2_vl.py"
      },
      {
        "type": "file",
        "size": 11.061,
        "file_type": "py",
        "inclusive_size": 11.061,
        "id": "src\\transformers\\models\\lfm2_vl\\processing_lfm2_vl.py"
      },
      {
        "type": "file",
        "size": 1.082,
        "file_type": "py",
        "inclusive_size": 1.082,
        "id": "src\\transformers\\models\\lfm2_vl\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.384,
        "file_type": "py",
        "inclusive_size": 8.384,
        "id": "src\\transformers\\models\\lfm2_moe\\configuration_lfm2_moe.py"
      },
      {
        "type": "file",
        "size": 37.095,
        "file_type": "py",
        "inclusive_size": 37.095,
        "id": "src\\transformers\\models\\lfm2_moe\\modeling_lfm2_moe.py"
      },
      {
        "type": "file",
        "size": 8.763,
        "file_type": "py",
        "inclusive_size": 8.763,
        "id": "src\\transformers\\models\\lfm2_moe\\modular_lfm2_moe.py"
      },
      {
        "type": "file",
        "size": 0.998,
        "file_type": "py",
        "inclusive_size": 0.998,
        "id": "src\\transformers\\models\\lfm2_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.214,
        "file_type": "py",
        "inclusive_size": 8.214,
        "id": "src\\transformers\\models\\lfm2\\configuration_lfm2.py"
      },
      {
        "type": "file",
        "size": 32.659,
        "file_type": "py",
        "inclusive_size": 32.659,
        "id": "src\\transformers\\models\\lfm2\\modeling_lfm2.py"
      },
      {
        "type": "file",
        "size": 20.057,
        "file_type": "py",
        "inclusive_size": 20.057,
        "id": "src\\transformers\\models\\lfm2\\modular_lfm2.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\lfm2\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.132,
        "file_type": "py",
        "inclusive_size": 5.132,
        "id": "src\\transformers\\models\\levit\\configuration_levit.py"
      },
      {
        "type": "file",
        "size": 6.197,
        "file_type": "py",
        "inclusive_size": 6.197,
        "id": "src\\transformers\\models\\levit\\convert_levit_timm_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 16.325,
        "file_type": "py",
        "inclusive_size": 16.325,
        "id": "src\\transformers\\models\\levit\\image_processing_levit.py"
      },
      {
        "type": "file",
        "size": 3.721,
        "file_type": "py",
        "inclusive_size": 3.721,
        "id": "src\\transformers\\models\\levit\\image_processing_levit_fast.py"
      },
      {
        "type": "file",
        "size": 25.355,
        "file_type": "py",
        "inclusive_size": 25.355,
        "id": "src\\transformers\\models\\levit\\modeling_levit.py"
      },
      {
        "type": "file",
        "size": 1.124,
        "file_type": "py",
        "inclusive_size": 1.124,
        "id": "src\\transformers\\models\\levit\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.467,
        "file_type": "py",
        "inclusive_size": 7.467,
        "id": "src\\transformers\\models\\led\\configuration_led.py"
      },
      {
        "type": "file",
        "size": 108.028,
        "file_type": "py",
        "inclusive_size": 108.028,
        "id": "src\\transformers\\models\\led\\modeling_led.py"
      },
      {
        "type": "file",
        "size": 1.067,
        "file_type": "py",
        "inclusive_size": 1.067,
        "id": "src\\transformers\\models\\led\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.801,
        "file_type": "py",
        "inclusive_size": 11.801,
        "id": "src\\transformers\\models\\layoutxlm\\configuration_layoutxlm.py"
      },
      {
        "type": "file",
        "size": 6.425,
        "file_type": "py",
        "inclusive_size": 6.425,
        "id": "src\\transformers\\models\\layoutxlm\\modular_layoutxlm.py"
      },
      {
        "type": "file",
        "size": 5.294,
        "file_type": "py",
        "inclusive_size": 5.294,
        "id": "src\\transformers\\models\\layoutxlm\\processing_layoutxlm.py"
      },
      {
        "type": "file",
        "size": 46.151,
        "file_type": "py",
        "inclusive_size": 46.151,
        "id": "src\\transformers\\models\\layoutxlm\\tokenization_layoutxlm.py"
      },
      {
        "type": "file",
        "size": 1.043,
        "file_type": "py",
        "inclusive_size": 1.043,
        "id": "src\\transformers\\models\\layoutxlm\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.46,
        "file_type": "py",
        "inclusive_size": 8.46,
        "id": "src\\transformers\\models\\layoutlmv3\\configuration_layoutlmv3.py"
      },
      {
        "type": "file",
        "size": 19.353,
        "file_type": "py",
        "inclusive_size": 19.353,
        "id": "src\\transformers\\models\\layoutlmv3\\image_processing_layoutlmv3.py"
      },
      {
        "type": "file",
        "size": 5.003,
        "file_type": "py",
        "inclusive_size": 5.003,
        "id": "src\\transformers\\models\\layoutlmv3\\image_processing_layoutlmv3_fast.py"
      },
      {
        "type": "file",
        "size": 52.299,
        "file_type": "py",
        "inclusive_size": 52.299,
        "id": "src\\transformers\\models\\layoutlmv3\\modeling_layoutlmv3.py"
      },
      {
        "type": "file",
        "size": 5.097,
        "file_type": "py",
        "inclusive_size": 5.097,
        "id": "src\\transformers\\models\\layoutlmv3\\processing_layoutlmv3.py"
      },
      {
        "type": "file",
        "size": 42.66,
        "file_type": "py",
        "inclusive_size": 42.66,
        "id": "src\\transformers\\models\\layoutlmv3\\tokenization_layoutlmv3.py"
      },
      {
        "type": "file",
        "size": 1.233,
        "file_type": "py",
        "inclusive_size": 1.233,
        "id": "src\\transformers\\models\\layoutlmv3\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.112,
        "file_type": "py",
        "inclusive_size": 11.112,
        "id": "src\\transformers\\models\\layoutlmv2\\configuration_layoutlmv2.py"
      },
      {
        "type": "file",
        "size": 14.241,
        "file_type": "py",
        "inclusive_size": 14.241,
        "id": "src\\transformers\\models\\layoutlmv2\\image_processing_layoutlmv2.py"
      },
      {
        "type": "file",
        "size": 4.434,
        "file_type": "py",
        "inclusive_size": 4.434,
        "id": "src\\transformers\\models\\layoutlmv2\\image_processing_layoutlmv2_fast.py"
      },
      {
        "type": "file",
        "size": 60.774,
        "file_type": "py",
        "inclusive_size": 60.774,
        "id": "src\\transformers\\models\\layoutlmv2\\modeling_layoutlmv2.py"
      },
      {
        "type": "file",
        "size": 5.316,
        "file_type": "py",
        "inclusive_size": 5.316,
        "id": "src\\transformers\\models\\layoutlmv2\\processing_layoutlmv2.py"
      },
      {
        "type": "file",
        "size": 42.519,
        "file_type": "py",
        "inclusive_size": 42.519,
        "id": "src\\transformers\\models\\layoutlmv2\\tokenization_layoutlmv2.py"
      },
      {
        "type": "file",
        "size": 1.233,
        "file_type": "py",
        "inclusive_size": 1.233,
        "id": "src\\transformers\\models\\layoutlmv2\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.319,
        "file_type": "py",
        "inclusive_size": 6.319,
        "id": "src\\transformers\\models\\layoutlm\\configuration_layoutlm.py"
      },
      {
        "type": "file",
        "size": 43.893,
        "file_type": "py",
        "inclusive_size": 43.893,
        "id": "src\\transformers\\models\\layoutlm\\modeling_layoutlm.py"
      },
      {
        "type": "file",
        "size": 1.153,
        "file_type": "py",
        "inclusive_size": 1.153,
        "id": "src\\transformers\\models\\layoutlm\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.064,
        "file_type": "py",
        "inclusive_size": 12.064,
        "id": "src\\transformers\\models\\lasr\\configuration_lasr.py"
      },
      {
        "type": "file",
        "size": 12.687,
        "file_type": "py",
        "inclusive_size": 12.687,
        "id": "src\\transformers\\models\\lasr\\feature_extraction_lasr.py"
      },
      {
        "type": "file",
        "size": 31.073,
        "file_type": "py",
        "inclusive_size": 31.073,
        "id": "src\\transformers\\models\\lasr\\modeling_lasr.py"
      },
      {
        "type": "file",
        "size": 23.649,
        "file_type": "py",
        "inclusive_size": 23.649,
        "id": "src\\transformers\\models\\lasr\\modular_lasr.py"
      },
      {
        "type": "file",
        "size": 4.494,
        "file_type": "py",
        "inclusive_size": 4.494,
        "id": "src\\transformers\\models\\lasr\\processing_lasr.py"
      },
      {
        "type": "file",
        "size": 7.929,
        "file_type": "py",
        "inclusive_size": 7.929,
        "id": "src\\transformers\\models\\lasr\\tokenization_lasr.py"
      },
      {
        "type": "file",
        "size": 1.069,
        "file_type": "py",
        "inclusive_size": 1.069,
        "id": "src\\transformers\\models\\lasr\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.843,
        "file_type": "py",
        "inclusive_size": 9.843,
        "id": "src\\transformers\\models\\kyutai_speech_to_text\\configuration_kyutai_speech_to_text.py"
      },
      {
        "type": "file",
        "size": 13.079,
        "file_type": "py",
        "inclusive_size": 13.079,
        "id": "src\\transformers\\models\\kyutai_speech_to_text\\convert_kyutai_speech_to_text_to_hf.py"
      },
      {
        "type": "file",
        "size": 11.643,
        "file_type": "py",
        "inclusive_size": 11.643,
        "id": "src\\transformers\\models\\kyutai_speech_to_text\\feature_extraction_kyutai_speech_to_text.py"
      },
      {
        "type": "file",
        "size": 64.929,
        "file_type": "py",
        "inclusive_size": 64.929,
        "id": "src\\transformers\\models\\kyutai_speech_to_text\\modeling_kyutai_speech_to_text.py"
      },
      {
        "type": "file",
        "size": 23.834,
        "file_type": "py",
        "inclusive_size": 23.834,
        "id": "src\\transformers\\models\\kyutai_speech_to_text\\modular_kyutai_speech_to_text.py"
      },
      {
        "type": "file",
        "size": 1.222,
        "file_type": "py",
        "inclusive_size": 1.222,
        "id": "src\\transformers\\models\\kyutai_speech_to_text\\processing_kyutai_speech_to_text.py"
      },
      {
        "type": "file",
        "size": 1.135,
        "file_type": "py",
        "inclusive_size": 1.135,
        "id": "src\\transformers\\models\\kyutai_speech_to_text\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.01,
        "file_type": "py",
        "inclusive_size": 12.01,
        "id": "src\\transformers\\models\\kosmos2_5\\configuration_kosmos2_5.py"
      },
      {
        "type": "file",
        "size": 3.425,
        "file_type": "py",
        "inclusive_size": 3.425,
        "id": "src\\transformers\\models\\kosmos2_5\\convert_kosmos2_5.py"
      },
      {
        "type": "file",
        "size": 14.856,
        "file_type": "py",
        "inclusive_size": 14.856,
        "id": "src\\transformers\\models\\kosmos2_5\\image_processing_kosmos2_5.py"
      },
      {
        "type": "file",
        "size": 10.925,
        "file_type": "py",
        "inclusive_size": 10.925,
        "id": "src\\transformers\\models\\kosmos2_5\\image_processing_kosmos2_5_fast.py"
      },
      {
        "type": "file",
        "size": 82.608,
        "file_type": "py",
        "inclusive_size": 82.608,
        "id": "src\\transformers\\models\\kosmos2_5\\modeling_kosmos2_5.py"
      },
      {
        "type": "file",
        "size": 5.022,
        "file_type": "py",
        "inclusive_size": 5.022,
        "id": "src\\transformers\\models\\kosmos2_5\\processing_kosmos2_5.py"
      },
      {
        "type": "file",
        "size": 1.164,
        "file_type": "py",
        "inclusive_size": 1.164,
        "id": "src\\transformers\\models\\kosmos2_5\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.316,
        "file_type": "py",
        "inclusive_size": 12.316,
        "id": "src\\transformers\\models\\kosmos2\\configuration_kosmos2.py"
      },
      {
        "type": "file",
        "size": 2.724,
        "file_type": "py",
        "inclusive_size": 2.724,
        "id": "src\\transformers\\models\\kosmos2\\convert_kosmos2_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 80.703,
        "file_type": "py",
        "inclusive_size": 80.703,
        "id": "src\\transformers\\models\\kosmos2\\modeling_kosmos2.py"
      },
      {
        "type": "file",
        "size": 29.686,
        "file_type": "py",
        "inclusive_size": 29.686,
        "id": "src\\transformers\\models\\kosmos2\\processing_kosmos2.py"
      },
      {
        "type": "file",
        "size": 1.033,
        "file_type": "py",
        "inclusive_size": 1.033,
        "id": "src\\transformers\\models\\kosmos2\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.546,
        "file_type": "py",
        "inclusive_size": 7.546,
        "id": "src\\transformers\\models\\jetmoe\\configuration_jetmoe.py"
      },
      {
        "type": "file",
        "size": 35.637,
        "file_type": "py",
        "inclusive_size": 35.637,
        "id": "src\\transformers\\models\\jetmoe\\modeling_jetmoe.py"
      },
      {
        "type": "file",
        "size": 24.225,
        "file_type": "py",
        "inclusive_size": 24.225,
        "id": "src\\transformers\\models\\jetmoe\\modular_jetmoe.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\jetmoe\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.893,
        "file_type": "py",
        "inclusive_size": 14.893,
        "id": "src\\transformers\\models\\janus\\configuration_janus.py"
      },
      {
        "type": "file",
        "size": 19.351,
        "file_type": "py",
        "inclusive_size": 19.351,
        "id": "src\\transformers\\models\\janus\\convert_janus_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 26.112,
        "file_type": "py",
        "inclusive_size": 26.112,
        "id": "src\\transformers\\models\\janus\\image_processing_janus.py"
      },
      {
        "type": "file",
        "size": 8.703,
        "file_type": "py",
        "inclusive_size": 8.703,
        "id": "src\\transformers\\models\\janus\\image_processing_janus_fast.py"
      },
      {
        "type": "file",
        "size": 59.337,
        "file_type": "py",
        "inclusive_size": 59.337,
        "id": "src\\transformers\\models\\janus\\modeling_janus.py"
      },
      {
        "type": "file",
        "size": 79.423,
        "file_type": "py",
        "inclusive_size": 79.423,
        "id": "src\\transformers\\models\\janus\\modular_janus.py"
      },
      {
        "type": "file",
        "size": 7.227,
        "file_type": "py",
        "inclusive_size": 7.227,
        "id": "src\\transformers\\models\\janus\\processing_janus.py"
      },
      {
        "type": "file",
        "size": 1.132,
        "file_type": "py",
        "inclusive_size": 1.132,
        "id": "src\\transformers\\models\\janus\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.937,
        "file_type": "py",
        "inclusive_size": 10.937,
        "id": "src\\transformers\\models\\jamba\\configuration_jamba.py"
      },
      {
        "type": "file",
        "size": 48.986,
        "file_type": "py",
        "inclusive_size": 48.986,
        "id": "src\\transformers\\models\\jamba\\modeling_jamba.py"
      },
      {
        "type": "file",
        "size": 35.556,
        "file_type": "py",
        "inclusive_size": 35.556,
        "id": "src\\transformers\\models\\jamba\\modular_jamba.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\jamba\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.115,
        "file_type": "py",
        "inclusive_size": 7.115,
        "id": "src\\transformers\\models\\jais2\\configuration_jais2.py"
      },
      {
        "type": "file",
        "size": 20.666,
        "file_type": "py",
        "inclusive_size": 20.666,
        "id": "src\\transformers\\models\\jais2\\modeling_jais2.py"
      },
      {
        "type": "file",
        "size": 7.733,
        "file_type": "py",
        "inclusive_size": 7.733,
        "id": "src\\transformers\\models\\jais2\\modular_jais2.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\jais2\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.812,
        "file_type": "py",
        "inclusive_size": 10.812,
        "id": "src\\transformers\\models\\internvl\\configuration_internvl.py"
      },
      {
        "type": "file",
        "size": 18.152,
        "file_type": "py",
        "inclusive_size": 18.152,
        "id": "src\\transformers\\models\\internvl\\convert_internvl_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 38.111,
        "file_type": "py",
        "inclusive_size": 38.111,
        "id": "src\\transformers\\models\\internvl\\modeling_internvl.py"
      },
      {
        "type": "file",
        "size": 25.322,
        "file_type": "py",
        "inclusive_size": 25.322,
        "id": "src\\transformers\\models\\internvl\\modular_internvl.py"
      },
      {
        "type": "file",
        "size": 12.777,
        "file_type": "py",
        "inclusive_size": 12.777,
        "id": "src\\transformers\\models\\internvl\\processing_internvl.py"
      },
      {
        "type": "file",
        "size": 6.385,
        "file_type": "py",
        "inclusive_size": 6.385,
        "id": "src\\transformers\\models\\internvl\\video_processing_internvl.py"
      },
      {
        "type": "file",
        "size": 1.081,
        "file_type": "py",
        "inclusive_size": 1.081,
        "id": "src\\transformers\\models\\internvl\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.826,
        "file_type": "py",
        "inclusive_size": 15.826,
        "id": "src\\transformers\\models\\instructblipvideo\\configuration_instructblipvideo.py"
      },
      {
        "type": "file",
        "size": 13.554,
        "file_type": "py",
        "inclusive_size": 13.554,
        "id": "src\\transformers\\models\\instructblipvideo\\convert_instructblipvideo_original_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 66.665,
        "file_type": "py",
        "inclusive_size": 66.665,
        "id": "src\\transformers\\models\\instructblipvideo\\modeling_instructblipvideo.py"
      },
      {
        "type": "file",
        "size": 27.631,
        "file_type": "py",
        "inclusive_size": 27.631,
        "id": "src\\transformers\\models\\instructblipvideo\\modular_instructblipvideo.py"
      },
      {
        "type": "file",
        "size": 6.953,
        "file_type": "py",
        "inclusive_size": 6.953,
        "id": "src\\transformers\\models\\instructblipvideo\\processing_instructblipvideo.py"
      },
      {
        "type": "file",
        "size": 3.611,
        "file_type": "py",
        "inclusive_size": 3.611,
        "id": "src\\transformers\\models\\instructblipvideo\\video_processing_instructblipvideo.py"
      },
      {
        "type": "file",
        "size": 1.171,
        "file_type": "py",
        "inclusive_size": 1.171,
        "id": "src\\transformers\\models\\instructblipvideo\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.703,
        "file_type": "py",
        "inclusive_size": 14.703,
        "id": "src\\transformers\\models\\instructblip\\configuration_instructblip.py"
      },
      {
        "type": "file",
        "size": 13.445,
        "file_type": "py",
        "inclusive_size": 13.445,
        "id": "src\\transformers\\models\\instructblip\\convert_instructblip_original_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 64.228,
        "file_type": "py",
        "inclusive_size": 64.228,
        "id": "src\\transformers\\models\\instructblip\\modeling_instructblip.py"
      },
      {
        "type": "file",
        "size": 5.403,
        "file_type": "py",
        "inclusive_size": 5.403,
        "id": "src\\transformers\\models\\instructblip\\processing_instructblip.py"
      },
      {
        "type": "file",
        "size": 1.048,
        "file_type": "py",
        "inclusive_size": 1.048,
        "id": "src\\transformers\\models\\instructblip\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.413,
        "file_type": "py",
        "inclusive_size": 12.413,
        "id": "src\\transformers\\models\\informer\\configuration_informer.py"
      },
      {
        "type": "file",
        "size": 95.681,
        "file_type": "py",
        "inclusive_size": 95.681,
        "id": "src\\transformers\\models\\informer\\modeling_informer.py"
      },
      {
        "type": "file",
        "size": 41.118,
        "file_type": "py",
        "inclusive_size": 41.118,
        "id": "src\\transformers\\models\\informer\\modular_informer.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\informer\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.433,
        "file_type": "py",
        "inclusive_size": 6.433,
        "id": "src\\transformers\\models\\imagegpt\\configuration_imagegpt.py"
      },
      {
        "type": "file",
        "size": 7.235,
        "file_type": "py",
        "inclusive_size": 7.235,
        "id": "src\\transformers\\models\\imagegpt\\convert_imagegpt_original_tf2_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 15.516,
        "file_type": "py",
        "inclusive_size": 15.516,
        "id": "src\\transformers\\models\\imagegpt\\image_processing_imagegpt.py"
      },
      {
        "type": "file",
        "size": 7.221,
        "file_type": "py",
        "inclusive_size": 7.221,
        "id": "src\\transformers\\models\\imagegpt\\image_processing_imagegpt_fast.py"
      },
      {
        "type": "file",
        "size": 37.404,
        "file_type": "py",
        "inclusive_size": 37.404,
        "id": "src\\transformers\\models\\imagegpt\\modeling_imagegpt.py"
      },
      {
        "type": "file",
        "size": 1.139,
        "file_type": "py",
        "inclusive_size": 1.139,
        "id": "src\\transformers\\models\\imagegpt\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.247,
        "file_type": "py",
        "inclusive_size": 5.247,
        "id": "src\\transformers\\models\\ijepa\\configuration_ijepa.py"
      },
      {
        "type": "file",
        "size": 10.114,
        "file_type": "py",
        "inclusive_size": 10.114,
        "id": "src\\transformers\\models\\ijepa\\convert_ijepa_to_hf.py"
      },
      {
        "type": "file",
        "size": 20.723,
        "file_type": "py",
        "inclusive_size": 20.723,
        "id": "src\\transformers\\models\\ijepa\\modeling_ijepa.py"
      },
      {
        "type": "file",
        "size": 7.128,
        "file_type": "py",
        "inclusive_size": 7.128,
        "id": "src\\transformers\\models\\ijepa\\modular_ijepa.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\ijepa\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.439,
        "file_type": "py",
        "inclusive_size": 8.439,
        "id": "src\\transformers\\models\\idefics3\\configuration_idefics3.py"
      },
      {
        "type": "file",
        "size": 7.325,
        "file_type": "py",
        "inclusive_size": 7.325,
        "id": "src\\transformers\\models\\idefics3\\convert_idefics3_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 44.253,
        "file_type": "py",
        "inclusive_size": 44.253,
        "id": "src\\transformers\\models\\idefics3\\image_processing_idefics3.py"
      },
      {
        "type": "file",
        "size": 23.475,
        "file_type": "py",
        "inclusive_size": 23.475,
        "id": "src\\transformers\\models\\idefics3\\image_processing_idefics3_fast.py"
      },
      {
        "type": "file",
        "size": 41.681,
        "file_type": "py",
        "inclusive_size": 41.681,
        "id": "src\\transformers\\models\\idefics3\\modeling_idefics3.py"
      },
      {
        "type": "file",
        "size": 14.875,
        "file_type": "py",
        "inclusive_size": 14.875,
        "id": "src\\transformers\\models\\idefics3\\processing_idefics3.py"
      },
      {
        "type": "file",
        "size": 1.131,
        "file_type": "py",
        "inclusive_size": 1.131,
        "id": "src\\transformers\\models\\idefics3\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.124,
        "file_type": "py",
        "inclusive_size": 12.124,
        "id": "src\\transformers\\models\\idefics2\\configuration_idefics2.py"
      },
      {
        "type": "file",
        "size": 6.627,
        "file_type": "py",
        "inclusive_size": 6.627,
        "id": "src\\transformers\\models\\idefics2\\convert_idefics2_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 26.505,
        "file_type": "py",
        "inclusive_size": 26.505,
        "id": "src\\transformers\\models\\idefics2\\image_processing_idefics2.py"
      },
      {
        "type": "file",
        "size": 11.678,
        "file_type": "py",
        "inclusive_size": 11.678,
        "id": "src\\transformers\\models\\idefics2\\image_processing_idefics2_fast.py"
      },
      {
        "type": "file",
        "size": 51.164,
        "file_type": "py",
        "inclusive_size": 51.164,
        "id": "src\\transformers\\models\\idefics2\\modeling_idefics2.py"
      },
      {
        "type": "file",
        "size": 8.093,
        "file_type": "py",
        "inclusive_size": 8.093,
        "id": "src\\transformers\\models\\idefics2\\processing_idefics2.py"
      },
      {
        "type": "file",
        "size": 1.131,
        "file_type": "py",
        "inclusive_size": 1.131,
        "id": "src\\transformers\\models\\idefics2\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.274,
        "file_type": "py",
        "inclusive_size": 15.274,
        "id": "src\\transformers\\models\\idefics\\configuration_idefics.py"
      },
      {
        "type": "file",
        "size": 9.772,
        "file_type": "py",
        "inclusive_size": 9.772,
        "id": "src\\transformers\\models\\idefics\\image_processing_idefics.py"
      },
      {
        "type": "file",
        "size": 57.042,
        "file_type": "py",
        "inclusive_size": 57.042,
        "id": "src\\transformers\\models\\idefics\\modeling_idefics.py"
      },
      {
        "type": "file",
        "size": 9.394,
        "file_type": "py",
        "inclusive_size": 9.394,
        "id": "src\\transformers\\models\\idefics\\perceiver.py"
      },
      {
        "type": "file",
        "size": 16.923,
        "file_type": "py",
        "inclusive_size": 16.923,
        "id": "src\\transformers\\models\\idefics\\processing_idefics.py"
      },
      {
        "type": "file",
        "size": 21.233,
        "file_type": "py",
        "inclusive_size": 21.233,
        "id": "src\\transformers\\models\\idefics\\vision.py"
      },
      {
        "type": "file",
        "size": 1.077,
        "file_type": "py",
        "inclusive_size": 1.077,
        "id": "src\\transformers\\models\\idefics\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.81,
        "file_type": "py",
        "inclusive_size": 5.81,
        "id": "src\\transformers\\models\\ibert\\configuration_ibert.py"
      },
      {
        "type": "file",
        "size": 48.804,
        "file_type": "py",
        "inclusive_size": 48.804,
        "id": "src\\transformers\\models\\ibert\\modeling_ibert.py"
      },
      {
        "type": "file",
        "size": 30.059,
        "file_type": "py",
        "inclusive_size": 30.059,
        "id": "src\\transformers\\models\\ibert\\quant_modules.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\ibert\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.588,
        "file_type": "py",
        "inclusive_size": 10.588,
        "id": "src\\transformers\\models\\hunyuan_v1_moe\\configuration_hunyuan_v1_moe.py"
      },
      {
        "type": "file",
        "size": 27.845,
        "file_type": "py",
        "inclusive_size": 27.845,
        "id": "src\\transformers\\models\\hunyuan_v1_moe\\modeling_hunyuan_v1_moe.py"
      },
      {
        "type": "file",
        "size": 8.231,
        "file_type": "py",
        "inclusive_size": 8.231,
        "id": "src\\transformers\\models\\hunyuan_v1_moe\\modular_hunyuan_v1_moe.py"
      },
      {
        "type": "file",
        "size": 0.403,
        "file_type": "py",
        "inclusive_size": 0.403,
        "id": "src\\transformers\\models\\hunyuan_v1_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.027,
        "file_type": "py",
        "inclusive_size": 8.027,
        "id": "src\\transformers\\models\\hunyuan_v1_dense\\configuration_hunyuan_v1_dense.py"
      },
      {
        "type": "file",
        "size": 23.204,
        "file_type": "py",
        "inclusive_size": 23.204,
        "id": "src\\transformers\\models\\hunyuan_v1_dense\\modeling_hunyuan_v1_dense.py"
      },
      {
        "type": "file",
        "size": 6.232,
        "file_type": "py",
        "inclusive_size": 6.232,
        "id": "src\\transformers\\models\\hunyuan_v1_dense\\modular_hunyuan_v1_dense.py"
      },
      {
        "type": "file",
        "size": 0.442,
        "file_type": "py",
        "inclusive_size": 0.442,
        "id": "src\\transformers\\models\\hunyuan_v1_dense\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.989,
        "file_type": "py",
        "inclusive_size": 14.989,
        "id": "src\\transformers\\models\\hubert\\configuration_hubert.py"
      },
      {
        "type": "file",
        "size": 8.885,
        "file_type": "py",
        "inclusive_size": 8.885,
        "id": "src\\transformers\\models\\hubert\\convert_distilhubert_original_s3prl_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 10.985,
        "file_type": "py",
        "inclusive_size": 10.985,
        "id": "src\\transformers\\models\\hubert\\convert_hubert_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 2.898,
        "file_type": "py",
        "inclusive_size": 2.898,
        "id": "src\\transformers\\models\\hubert\\convert_hubert_original_s3prl_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 51.205,
        "file_type": "py",
        "inclusive_size": 51.205,
        "id": "src\\transformers\\models\\hubert\\modeling_hubert.py"
      },
      {
        "type": "file",
        "size": 11.915,
        "file_type": "py",
        "inclusive_size": 11.915,
        "id": "src\\transformers\\models\\hubert\\modular_hubert.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\hubert\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.304,
        "file_type": "py",
        "inclusive_size": 9.304,
        "id": "src\\transformers\\models\\hiera\\configuration_hiera.py"
      },
      {
        "type": "file",
        "size": 16.666,
        "file_type": "py",
        "inclusive_size": 16.666,
        "id": "src\\transformers\\models\\hiera\\convert_hiera_to_hf.py"
      },
      {
        "type": "file",
        "size": 59.585,
        "file_type": "py",
        "inclusive_size": 59.585,
        "id": "src\\transformers\\models\\hiera\\modeling_hiera.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\hiera\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.808,
        "file_type": "py",
        "inclusive_size": 8.808,
        "id": "src\\transformers\\models\\hgnet_v2\\configuration_hgnet_v2.py"
      },
      {
        "type": "file",
        "size": 18.457,
        "file_type": "py",
        "inclusive_size": 18.457,
        "id": "src\\transformers\\models\\hgnet_v2\\modeling_hgnet_v2.py"
      },
      {
        "type": "file",
        "size": 24.897,
        "file_type": "py",
        "inclusive_size": 24.897,
        "id": "src\\transformers\\models\\hgnet_v2\\modular_hgnet_v2.py"
      },
      {
        "type": "file",
        "size": 0.999,
        "file_type": "py",
        "inclusive_size": 0.999,
        "id": "src\\transformers\\models\\hgnet_v2\\__init__.py"
      },
      {
        "type": "file",
        "size": 3.917,
        "file_type": "py",
        "inclusive_size": 3.917,
        "id": "src\\transformers\\models\\herbert\\tokenization_herbert.py"
      },
      {
        "type": "file",
        "size": 0.958,
        "file_type": "py",
        "inclusive_size": 0.958,
        "id": "src\\transformers\\models\\herbert\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.988,
        "file_type": "py",
        "inclusive_size": 7.988,
        "id": "src\\transformers\\models\\helium\\configuration_helium.py"
      },
      {
        "type": "file",
        "size": 21.918,
        "file_type": "py",
        "inclusive_size": 21.918,
        "id": "src\\transformers\\models\\helium\\modeling_helium.py"
      },
      {
        "type": "file",
        "size": 5.261,
        "file_type": "py",
        "inclusive_size": 5.261,
        "id": "src\\transformers\\models\\helium\\modular_helium.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\helium\\__init__.py"
      },
      {
        "type": "file",
        "size": 17.149,
        "file_type": "py",
        "inclusive_size": 17.149,
        "id": "src\\transformers\\models\\groupvit\\configuration_groupvit.py"
      },
      {
        "type": "file",
        "size": 9.841,
        "file_type": "py",
        "inclusive_size": 9.841,
        "id": "src\\transformers\\models\\groupvit\\convert_groupvit_nvlab_to_hf.py"
      },
      {
        "type": "file",
        "size": 60.187,
        "file_type": "py",
        "inclusive_size": 60.187,
        "id": "src\\transformers\\models\\groupvit\\modeling_groupvit.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\groupvit\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.891,
        "file_type": "py",
        "inclusive_size": 14.891,
        "id": "src\\transformers\\models\\grounding_dino\\configuration_grounding_dino.py"
      },
      {
        "type": "file",
        "size": 25.501,
        "file_type": "py",
        "inclusive_size": 25.501,
        "id": "src\\transformers\\models\\grounding_dino\\convert_grounding_dino_to_hf.py"
      },
      {
        "type": "file",
        "size": 68.499,
        "file_type": "py",
        "inclusive_size": 68.499,
        "id": "src\\transformers\\models\\grounding_dino\\image_processing_grounding_dino.py"
      },
      {
        "type": "file",
        "size": 30.362,
        "file_type": "py",
        "inclusive_size": 30.362,
        "id": "src\\transformers\\models\\grounding_dino\\image_processing_grounding_dino_fast.py"
      },
      {
        "type": "file",
        "size": 130.529,
        "file_type": "py",
        "inclusive_size": 130.529,
        "id": "src\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py"
      },
      {
        "type": "file",
        "size": 5.575,
        "file_type": "py",
        "inclusive_size": 5.575,
        "id": "src\\transformers\\models\\grounding_dino\\modular_grounding_dino.py"
      },
      {
        "type": "file",
        "size": 9.659,
        "file_type": "py",
        "inclusive_size": 9.659,
        "id": "src\\transformers\\models\\grounding_dino\\processing_grounding_dino.py"
      },
      {
        "type": "file",
        "size": 1.161,
        "file_type": "py",
        "inclusive_size": 1.161,
        "id": "src\\transformers\\models\\grounding_dino\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.55,
        "file_type": "py",
        "inclusive_size": 8.55,
        "id": "src\\transformers\\models\\granite_speech\\configuration_granite_speech.py"
      },
      {
        "type": "file",
        "size": 7.349,
        "file_type": "py",
        "inclusive_size": 7.349,
        "id": "src\\transformers\\models\\granite_speech\\feature_extraction_granite_speech.py"
      },
      {
        "type": "file",
        "size": 26.568,
        "file_type": "py",
        "inclusive_size": 26.568,
        "id": "src\\transformers\\models\\granite_speech\\modeling_granite_speech.py"
      },
      {
        "type": "file",
        "size": 4.18,
        "file_type": "py",
        "inclusive_size": 4.18,
        "id": "src\\transformers\\models\\granite_speech\\processing_granite_speech.py"
      },
      {
        "type": "file",
        "size": 1.107,
        "file_type": "py",
        "inclusive_size": 1.107,
        "id": "src\\transformers\\models\\granite_speech\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.715,
        "file_type": "py",
        "inclusive_size": 9.715,
        "id": "src\\transformers\\models\\granitemoeshared\\configuration_granitemoeshared.py"
      },
      {
        "type": "file",
        "size": 35.096,
        "file_type": "py",
        "inclusive_size": 35.096,
        "id": "src\\transformers\\models\\granitemoeshared\\modeling_granitemoeshared.py"
      },
      {
        "type": "file",
        "size": 5.707,
        "file_type": "py",
        "inclusive_size": 5.707,
        "id": "src\\transformers\\models\\granitemoeshared\\modular_granitemoeshared.py"
      },
      {
        "type": "file",
        "size": 1.013,
        "file_type": "py",
        "inclusive_size": 1.013,
        "id": "src\\transformers\\models\\granitemoeshared\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.165,
        "file_type": "py",
        "inclusive_size": 13.165,
        "id": "src\\transformers\\models\\granitemoehybrid\\configuration_granitemoehybrid.py"
      },
      {
        "type": "file",
        "size": 70.942,
        "file_type": "py",
        "inclusive_size": 70.942,
        "id": "src\\transformers\\models\\granitemoehybrid\\modeling_granitemoehybrid.py"
      },
      {
        "type": "file",
        "size": 14.063,
        "file_type": "py",
        "inclusive_size": 14.063,
        "id": "src\\transformers\\models\\granitemoehybrid\\modular_granitemoehybrid.py"
      },
      {
        "type": "file",
        "size": 1.028,
        "file_type": "py",
        "inclusive_size": 1.028,
        "id": "src\\transformers\\models\\granitemoehybrid\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.162,
        "file_type": "py",
        "inclusive_size": 9.162,
        "id": "src\\transformers\\models\\granitemoe\\configuration_granitemoe.py"
      },
      {
        "type": "file",
        "size": 32.318,
        "file_type": "py",
        "inclusive_size": 32.318,
        "id": "src\\transformers\\models\\granitemoe\\modeling_granitemoe.py"
      },
      {
        "type": "file",
        "size": 13.221,
        "file_type": "py",
        "inclusive_size": 13.221,
        "id": "src\\transformers\\models\\granitemoe\\modular_granitemoe.py"
      },
      {
        "type": "file",
        "size": 1.001,
        "file_type": "py",
        "inclusive_size": 1.001,
        "id": "src\\transformers\\models\\granitemoe\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.062,
        "file_type": "py",
        "inclusive_size": 9.062,
        "id": "src\\transformers\\models\\granite\\configuration_granite.py"
      },
      {
        "type": "file",
        "size": 25.893,
        "file_type": "py",
        "inclusive_size": 25.893,
        "id": "src\\transformers\\models\\granite\\modeling_granite.py"
      },
      {
        "type": "file",
        "size": 11.481,
        "file_type": "py",
        "inclusive_size": 11.481,
        "id": "src\\transformers\\models\\granite\\modular_granite.py"
      },
      {
        "type": "file",
        "size": 1.015,
        "file_type": "py",
        "inclusive_size": 1.015,
        "id": "src\\transformers\\models\\granite\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.167,
        "file_type": "py",
        "inclusive_size": 8.167,
        "id": "src\\transformers\\models\\gpt_sw3\\convert_megatron_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 10.021,
        "file_type": "py",
        "inclusive_size": 10.021,
        "id": "src\\transformers\\models\\gpt_sw3\\tokenization_gpt_sw3.py"
      },
      {
        "type": "file",
        "size": 0.958,
        "file_type": "py",
        "inclusive_size": 0.958,
        "id": "src\\transformers\\models\\gpt_sw3\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.919,
        "file_type": "py",
        "inclusive_size": 5.919,
        "id": "src\\transformers\\models\\gpt_oss\\configuration_gpt_oss.py"
      },
      {
        "type": "file",
        "size": 34.897,
        "file_type": "py",
        "inclusive_size": 34.897,
        "id": "src\\transformers\\models\\gpt_oss\\convert_gpt_oss_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 31.327,
        "file_type": "py",
        "inclusive_size": 31.327,
        "id": "src\\transformers\\models\\gpt_oss\\modeling_gpt_oss.py"
      },
      {
        "type": "file",
        "size": 17.842,
        "file_type": "py",
        "inclusive_size": 17.842,
        "id": "src\\transformers\\models\\gpt_oss\\modular_gpt_oss.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\gpt_oss\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.988,
        "file_type": "py",
        "inclusive_size": 6.988,
        "id": "src\\transformers\\models\\gpt_neox_japanese\\configuration_gpt_neox_japanese.py"
      },
      {
        "type": "file",
        "size": 32.259,
        "file_type": "py",
        "inclusive_size": 32.259,
        "id": "src\\transformers\\models\\gpt_neox_japanese\\modeling_gpt_neox_japanese.py"
      },
      {
        "type": "file",
        "size": 16.931,
        "file_type": "py",
        "inclusive_size": 16.931,
        "id": "src\\transformers\\models\\gpt_neox_japanese\\tokenization_gpt_neox_japanese.py"
      },
      {
        "type": "file",
        "size": 1.065,
        "file_type": "py",
        "inclusive_size": 1.065,
        "id": "src\\transformers\\models\\gpt_neox_japanese\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.807,
        "file_type": "py",
        "inclusive_size": 8.807,
        "id": "src\\transformers\\models\\gpt_neox\\configuration_gpt_neox.py"
      },
      {
        "type": "file",
        "size": 33.755,
        "file_type": "py",
        "inclusive_size": 33.755,
        "id": "src\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py"
      },
      {
        "type": "file",
        "size": 28.162,
        "file_type": "py",
        "inclusive_size": 28.162,
        "id": "src\\transformers\\models\\gpt_neox\\modular_gpt_neox.py"
      },
      {
        "type": "file",
        "size": 6.045,
        "file_type": "py",
        "inclusive_size": 6.045,
        "id": "src\\transformers\\models\\gpt_neox\\tokenization_gpt_neox.py"
      },
      {
        "type": "file",
        "size": 1.038,
        "file_type": "py",
        "inclusive_size": 1.038,
        "id": "src\\transformers\\models\\gpt_neox\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.425,
        "file_type": "py",
        "inclusive_size": 9.425,
        "id": "src\\transformers\\models\\gpt_neo\\configuration_gpt_neo.py"
      },
      {
        "type": "file",
        "size": 6.053,
        "file_type": "py",
        "inclusive_size": 6.053,
        "id": "src\\transformers\\models\\gpt_neo\\convert_gpt_neo_mesh_tf_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 45.898,
        "file_type": "py",
        "inclusive_size": 45.898,
        "id": "src\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\gpt_neo\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.553,
        "file_type": "py",
        "inclusive_size": 6.553,
        "id": "src\\transformers\\models\\gpt_bigcode\\configuration_gpt_bigcode.py"
      },
      {
        "type": "file",
        "size": 38.349,
        "file_type": "py",
        "inclusive_size": 38.349,
        "id": "src\\transformers\\models\\gpt_bigcode\\modeling_gpt_bigcode.py"
      },
      {
        "type": "file",
        "size": 1.003,
        "file_type": "py",
        "inclusive_size": 1.003,
        "id": "src\\transformers\\models\\gpt_bigcode\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.492,
        "file_type": "py",
        "inclusive_size": 5.492,
        "id": "src\\transformers\\models\\gptj\\configuration_gptj.py"
      },
      {
        "type": "file",
        "size": 44.113,
        "file_type": "py",
        "inclusive_size": 44.113,
        "id": "src\\transformers\\models\\gptj\\modeling_gptj.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\gptj\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.071,
        "file_type": "py",
        "inclusive_size": 9.071,
        "id": "src\\transformers\\models\\gpt2\\configuration_gpt2.py"
      },
      {
        "type": "file",
        "size": 0.65,
        "file_type": "md",
        "inclusive_size": 0.65,
        "id": "src\\transformers\\models\\gpt2\\CONVERSION.md"
      },
      {
        "type": "file",
        "size": 4.732,
        "file_type": "py",
        "inclusive_size": 4.732,
        "id": "src\\transformers\\models\\gpt2\\convert_gpt2_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 59.984,
        "file_type": "py",
        "inclusive_size": 59.984,
        "id": "src\\transformers\\models\\gpt2\\modeling_gpt2.py"
      },
      {
        "type": "file",
        "size": 5.37,
        "file_type": "py",
        "inclusive_size": 5.37,
        "id": "src\\transformers\\models\\gpt2\\tokenization_gpt2.py"
      },
      {
        "type": "file",
        "size": 1.026,
        "file_type": "py",
        "inclusive_size": 1.026,
        "id": "src\\transformers\\models\\gpt2\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.581,
        "file_type": "py",
        "inclusive_size": 9.581,
        "id": "src\\transformers\\models\\got_ocr2\\configuration_got_ocr2.py"
      },
      {
        "type": "file",
        "size": 9.608,
        "file_type": "py",
        "inclusive_size": 9.608,
        "id": "src\\transformers\\models\\got_ocr2\\convert_got_ocr2_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 26.18,
        "file_type": "py",
        "inclusive_size": 26.18,
        "id": "src\\transformers\\models\\got_ocr2\\image_processing_got_ocr2.py"
      },
      {
        "type": "file",
        "size": 9.453,
        "file_type": "py",
        "inclusive_size": 9.453,
        "id": "src\\transformers\\models\\got_ocr2\\image_processing_got_ocr2_fast.py"
      },
      {
        "type": "file",
        "size": 36.01,
        "file_type": "py",
        "inclusive_size": 36.01,
        "id": "src\\transformers\\models\\got_ocr2\\modeling_got_ocr2.py"
      },
      {
        "type": "file",
        "size": 20.095,
        "file_type": "py",
        "inclusive_size": 20.095,
        "id": "src\\transformers\\models\\got_ocr2\\modular_got_ocr2.py"
      },
      {
        "type": "file",
        "size": 10.45,
        "file_type": "py",
        "inclusive_size": 10.45,
        "id": "src\\transformers\\models\\got_ocr2\\processing_got_ocr2.py"
      },
      {
        "type": "file",
        "size": 1.138,
        "file_type": "py",
        "inclusive_size": 1.138,
        "id": "src\\transformers\\models\\got_ocr2\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.983,
        "file_type": "py",
        "inclusive_size": 5.983,
        "id": "src\\transformers\\models\\glpn\\configuration_glpn.py"
      },
      {
        "type": "file",
        "size": 8.277,
        "file_type": "py",
        "inclusive_size": 8.277,
        "id": "src\\transformers\\models\\glpn\\convert_glpn_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 13.305,
        "file_type": "py",
        "inclusive_size": 13.305,
        "id": "src\\transformers\\models\\glpn\\image_processing_glpn.py"
      },
      {
        "type": "file",
        "size": 5.247,
        "file_type": "py",
        "inclusive_size": 5.247,
        "id": "src\\transformers\\models\\glpn\\image_processing_glpn_fast.py"
      },
      {
        "type": "file",
        "size": 26.384,
        "file_type": "py",
        "inclusive_size": 26.384,
        "id": "src\\transformers\\models\\glpn\\modeling_glpn.py"
      },
      {
        "type": "file",
        "size": 1.119,
        "file_type": "py",
        "inclusive_size": 1.119,
        "id": "src\\transformers\\models\\glpn\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.495,
        "file_type": "py",
        "inclusive_size": 15.495,
        "id": "src\\transformers\\models\\glm_ocr\\configuration_glm_ocr.py"
      },
      {
        "type": "file",
        "size": 75.019,
        "file_type": "py",
        "inclusive_size": 75.019,
        "id": "src\\transformers\\models\\glm_ocr\\modeling_glm_ocr.py"
      },
      {
        "type": "file",
        "size": 18.622,
        "file_type": "py",
        "inclusive_size": 18.622,
        "id": "src\\transformers\\models\\glm_ocr\\modular_glm_ocr.py"
      },
      {
        "type": "file",
        "size": 0.996,
        "file_type": "py",
        "inclusive_size": 0.996,
        "id": "src\\transformers\\models\\glm_ocr\\__init__.py"
      },
      {
        "type": "file",
        "size": 17.537,
        "file_type": "py",
        "inclusive_size": 17.537,
        "id": "src\\transformers\\models\\glm_image\\configuration_glm_image.py"
      },
      {
        "type": "file",
        "size": 25.285,
        "file_type": "py",
        "inclusive_size": 25.285,
        "id": "src\\transformers\\models\\glm_image\\image_processing_glm_image.py"
      },
      {
        "type": "file",
        "size": 12.298,
        "file_type": "py",
        "inclusive_size": 12.298,
        "id": "src\\transformers\\models\\glm_image\\image_processing_glm_image_fast.py"
      },
      {
        "type": "file",
        "size": 75.718,
        "file_type": "py",
        "inclusive_size": 75.718,
        "id": "src\\transformers\\models\\glm_image\\modeling_glm_image.py"
      },
      {
        "type": "file",
        "size": 72.399,
        "file_type": "py",
        "inclusive_size": 72.399,
        "id": "src\\transformers\\models\\glm_image\\modular_glm_image.py"
      },
      {
        "type": "file",
        "size": 12.117,
        "file_type": "py",
        "inclusive_size": 12.117,
        "id": "src\\transformers\\models\\glm_image\\processing_glm_image.py"
      },
      {
        "type": "file",
        "size": 1.137,
        "file_type": "py",
        "inclusive_size": 1.137,
        "id": "src\\transformers\\models\\glm_image\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.835,
        "file_type": "py",
        "inclusive_size": 8.835,
        "id": "src\\transformers\\models\\glmasr\\configuration_glmasr.py"
      },
      {
        "type": "file",
        "size": 6.91,
        "file_type": "py",
        "inclusive_size": 6.91,
        "id": "src\\transformers\\models\\glmasr\\convert_glmasr_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 22.764,
        "file_type": "py",
        "inclusive_size": 22.764,
        "id": "src\\transformers\\models\\glmasr\\modeling_glmasr.py"
      },
      {
        "type": "file",
        "size": 18.781,
        "file_type": "py",
        "inclusive_size": 18.781,
        "id": "src\\transformers\\models\\glmasr\\modular_glmasr.py"
      },
      {
        "type": "file",
        "size": 14.462,
        "file_type": "py",
        "inclusive_size": 14.462,
        "id": "src\\transformers\\models\\glmasr\\processing_glmasr.py"
      },
      {
        "type": "file",
        "size": 1.031,
        "file_type": "py",
        "inclusive_size": 1.031,
        "id": "src\\transformers\\models\\glmasr\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.553,
        "file_type": "py",
        "inclusive_size": 12.553,
        "id": "src\\transformers\\models\\glm4_moe_lite\\configuration_glm4_moe_lite.py"
      },
      {
        "type": "file",
        "size": 33.019,
        "file_type": "py",
        "inclusive_size": 33.019,
        "id": "src\\transformers\\models\\glm4_moe_lite\\modeling_glm4_moe_lite.py"
      },
      {
        "type": "file",
        "size": 13.419,
        "file_type": "py",
        "inclusive_size": 13.419,
        "id": "src\\transformers\\models\\glm4_moe_lite\\modular_glm4_moe_lite.py"
      },
      {
        "type": "file",
        "size": 1.008,
        "file_type": "py",
        "inclusive_size": 1.008,
        "id": "src\\transformers\\models\\glm4_moe_lite\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.997,
        "file_type": "py",
        "inclusive_size": 10.997,
        "id": "src\\transformers\\models\\glm4_moe\\configuration_glm4_moe.py"
      },
      {
        "type": "file",
        "size": 29.171,
        "file_type": "py",
        "inclusive_size": 29.171,
        "id": "src\\transformers\\models\\glm4_moe\\modeling_glm4_moe.py"
      },
      {
        "type": "file",
        "size": 13.319,
        "file_type": "py",
        "inclusive_size": 13.319,
        "id": "src\\transformers\\models\\glm4_moe\\modular_glm4_moe.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\glm4_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 18.794,
        "file_type": "py",
        "inclusive_size": 18.794,
        "id": "src\\transformers\\models\\glm4v_moe\\configuration_glm4v_moe.py"
      },
      {
        "type": "file",
        "size": 33.283,
        "file_type": "py",
        "inclusive_size": 33.283,
        "id": "src\\transformers\\models\\glm4v_moe\\convert_glm4v_moe_mgt_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 89.352,
        "file_type": "py",
        "inclusive_size": 89.352,
        "id": "src\\transformers\\models\\glm4v_moe\\modeling_glm4v_moe.py"
      },
      {
        "type": "file",
        "size": 25.052,
        "file_type": "py",
        "inclusive_size": 25.052,
        "id": "src\\transformers\\models\\glm4v_moe\\modular_glm4v_moe.py"
      },
      {
        "type": "file",
        "size": 0.999,
        "file_type": "py",
        "inclusive_size": 0.999,
        "id": "src\\transformers\\models\\glm4v_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.909,
        "file_type": "py",
        "inclusive_size": 15.909,
        "id": "src\\transformers\\models\\glm4v\\configuration_glm4v.py"
      },
      {
        "type": "file",
        "size": 32.454,
        "file_type": "py",
        "inclusive_size": 32.454,
        "id": "src\\transformers\\models\\glm4v\\convert_glm4v_mgt_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 23.882,
        "file_type": "py",
        "inclusive_size": 23.882,
        "id": "src\\transformers\\models\\glm4v\\image_processing_glm4v.py"
      },
      {
        "type": "file",
        "size": 7.102,
        "file_type": "py",
        "inclusive_size": 7.102,
        "id": "src\\transformers\\models\\glm4v\\image_processing_glm4v_fast.py"
      },
      {
        "type": "file",
        "size": 78.342,
        "file_type": "py",
        "inclusive_size": 78.342,
        "id": "src\\transformers\\models\\glm4v\\modeling_glm4v.py"
      },
      {
        "type": "file",
        "size": 77.38,
        "file_type": "py",
        "inclusive_size": 77.38,
        "id": "src\\transformers\\models\\glm4v\\modular_glm4v.py"
      },
      {
        "type": "file",
        "size": 12.375,
        "file_type": "py",
        "inclusive_size": 12.375,
        "id": "src\\transformers\\models\\glm4v\\processing_glm4v.py"
      },
      {
        "type": "file",
        "size": 9.933,
        "file_type": "py",
        "inclusive_size": 9.933,
        "id": "src\\transformers\\models\\glm4v\\video_processing_glm4v.py"
      },
      {
        "type": "file",
        "size": 1.111,
        "file_type": "py",
        "inclusive_size": 1.111,
        "id": "src\\transformers\\models\\glm4v\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.418,
        "file_type": "py",
        "inclusive_size": 5.418,
        "id": "src\\transformers\\models\\glm46v\\configuration_glm46v.py"
      },
      {
        "type": "file",
        "size": 24.618,
        "file_type": "py",
        "inclusive_size": 24.618,
        "id": "src\\transformers\\models\\glm46v\\image_processing_glm46v.py"
      },
      {
        "type": "file",
        "size": 7.736,
        "file_type": "py",
        "inclusive_size": 7.736,
        "id": "src\\transformers\\models\\glm46v\\image_processing_glm46v_fast.py"
      },
      {
        "type": "file",
        "size": 41.477,
        "file_type": "py",
        "inclusive_size": 41.477,
        "id": "src\\transformers\\models\\glm46v\\modeling_glm46v.py"
      },
      {
        "type": "file",
        "size": 8.758,
        "file_type": "py",
        "inclusive_size": 8.758,
        "id": "src\\transformers\\models\\glm46v\\modular_glm46v.py"
      },
      {
        "type": "file",
        "size": 12.364,
        "file_type": "py",
        "inclusive_size": 12.364,
        "id": "src\\transformers\\models\\glm46v\\processing_glm46v.py"
      },
      {
        "type": "file",
        "size": 11.728,
        "file_type": "py",
        "inclusive_size": 11.728,
        "id": "src\\transformers\\models\\glm46v\\video_processing_glm46v.py"
      },
      {
        "type": "file",
        "size": 1.164,
        "file_type": "py",
        "inclusive_size": 1.164,
        "id": "src\\transformers\\models\\glm46v\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.009,
        "file_type": "py",
        "inclusive_size": 8.009,
        "id": "src\\transformers\\models\\glm4\\configuration_glm4.py"
      },
      {
        "type": "file",
        "size": 7.529,
        "file_type": "py",
        "inclusive_size": 7.529,
        "id": "src\\transformers\\models\\glm4\\convert_glm4_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 23.382,
        "file_type": "py",
        "inclusive_size": 23.382,
        "id": "src\\transformers\\models\\glm4\\modeling_glm4.py"
      },
      {
        "type": "file",
        "size": 5.125,
        "file_type": "py",
        "inclusive_size": 5.125,
        "id": "src\\transformers\\models\\glm4\\modular_glm4.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\glm4\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.993,
        "file_type": "py",
        "inclusive_size": 7.993,
        "id": "src\\transformers\\models\\glm\\configuration_glm.py"
      },
      {
        "type": "file",
        "size": 7.248,
        "file_type": "py",
        "inclusive_size": 7.248,
        "id": "src\\transformers\\models\\glm\\convert_glm_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 22.506,
        "file_type": "py",
        "inclusive_size": 22.506,
        "id": "src\\transformers\\models\\glm\\modeling_glm.py"
      },
      {
        "type": "file",
        "size": 5.526,
        "file_type": "py",
        "inclusive_size": 5.526,
        "id": "src\\transformers\\models\\glm\\modular_glm.py"
      },
      {
        "type": "file",
        "size": 0.987,
        "file_type": "py",
        "inclusive_size": 0.987,
        "id": "src\\transformers\\models\\glm\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.626,
        "file_type": "py",
        "inclusive_size": 9.626,
        "id": "src\\transformers\\models\\git\\configuration_git.py"
      },
      {
        "type": "file",
        "size": 23.174,
        "file_type": "py",
        "inclusive_size": 23.174,
        "id": "src\\transformers\\models\\git\\convert_git_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 59.331,
        "file_type": "py",
        "inclusive_size": 59.331,
        "id": "src\\transformers\\models\\git\\modeling_git.py"
      },
      {
        "type": "file",
        "size": 0.905,
        "file_type": "py",
        "inclusive_size": 0.905,
        "id": "src\\transformers\\models\\git\\processing_git.py"
      },
      {
        "type": "file",
        "size": 1.021,
        "file_type": "py",
        "inclusive_size": 1.021,
        "id": "src\\transformers\\models\\git\\__init__.py"
      },
      {
        "type": "file",
        "size": 35.7,
        "file_type": "py",
        "inclusive_size": 35.7,
        "id": "src\\transformers\\models\\gemma3n\\configuration_gemma3n.py"
      },
      {
        "type": "file",
        "size": 34.897,
        "file_type": "py",
        "inclusive_size": 34.897,
        "id": "src\\transformers\\models\\gemma3n\\convert_gemma3n_weights.py"
      },
      {
        "type": "file",
        "size": 14.875,
        "file_type": "py",
        "inclusive_size": 14.875,
        "id": "src\\transformers\\models\\gemma3n\\feature_extraction_gemma3n.py"
      },
      {
        "type": "file",
        "size": 114.285,
        "file_type": "py",
        "inclusive_size": 114.285,
        "id": "src\\transformers\\models\\gemma3n\\modeling_gemma3n.py"
      },
      {
        "type": "file",
        "size": 129.767,
        "file_type": "py",
        "inclusive_size": 129.767,
        "id": "src\\transformers\\models\\gemma3n\\modular_gemma3n.py"
      },
      {
        "type": "file",
        "size": 6.287,
        "file_type": "py",
        "inclusive_size": 6.287,
        "id": "src\\transformers\\models\\gemma3n\\processing_gemma3n.py"
      },
      {
        "type": "file",
        "size": 1.079,
        "file_type": "py",
        "inclusive_size": 1.079,
        "id": "src\\transformers\\models\\gemma3n\\__init__.py"
      },
      {
        "type": "file",
        "size": 16.971,
        "file_type": "py",
        "inclusive_size": 16.971,
        "id": "src\\transformers\\models\\gemma3\\configuration_gemma3.py"
      },
      {
        "type": "file",
        "size": 28.893,
        "file_type": "py",
        "inclusive_size": 28.893,
        "id": "src\\transformers\\models\\gemma3\\convert_gemma3_weights.py"
      },
      {
        "type": "file",
        "size": 20.426,
        "file_type": "py",
        "inclusive_size": 20.426,
        "id": "src\\transformers\\models\\gemma3\\image_processing_gemma3.py"
      },
      {
        "type": "file",
        "size": 10.099,
        "file_type": "py",
        "inclusive_size": 10.099,
        "id": "src\\transformers\\models\\gemma3\\image_processing_gemma3_fast.py"
      },
      {
        "type": "file",
        "size": 57.834,
        "file_type": "py",
        "inclusive_size": 57.834,
        "id": "src\\transformers\\models\\gemma3\\modeling_gemma3.py"
      },
      {
        "type": "file",
        "size": 52.486,
        "file_type": "py",
        "inclusive_size": 52.486,
        "id": "src\\transformers\\models\\gemma3\\modular_gemma3.py"
      },
      {
        "type": "file",
        "size": 7.321,
        "file_type": "py",
        "inclusive_size": 7.321,
        "id": "src\\transformers\\models\\gemma3\\processing_gemma3.py"
      },
      {
        "type": "file",
        "size": 1.121,
        "file_type": "py",
        "inclusive_size": 1.121,
        "id": "src\\transformers\\models\\gemma3\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.528,
        "file_type": "py",
        "inclusive_size": 10.528,
        "id": "src\\transformers\\models\\gemma2\\configuration_gemma2.py"
      },
      {
        "type": "file",
        "size": 8.188,
        "file_type": "py",
        "inclusive_size": 8.188,
        "id": "src\\transformers\\models\\gemma2\\convert_gemma2_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 24.499,
        "file_type": "py",
        "inclusive_size": 24.499,
        "id": "src\\transformers\\models\\gemma2\\modeling_gemma2.py"
      },
      {
        "type": "file",
        "size": 25.188,
        "file_type": "py",
        "inclusive_size": 25.188,
        "id": "src\\transformers\\models\\gemma2\\modular_gemma2.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\gemma2\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.843,
        "file_type": "py",
        "inclusive_size": 8.843,
        "id": "src\\transformers\\models\\gemma\\configuration_gemma.py"
      },
      {
        "type": "file",
        "size": 7.0,
        "file_type": "py",
        "inclusive_size": 7.0,
        "id": "src\\transformers\\models\\gemma\\convert_gemma_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 22.478,
        "file_type": "py",
        "inclusive_size": 22.478,
        "id": "src\\transformers\\models\\gemma\\modeling_gemma.py"
      },
      {
        "type": "file",
        "size": 14.297,
        "file_type": "py",
        "inclusive_size": 14.297,
        "id": "src\\transformers\\models\\gemma\\modular_gemma.py"
      },
      {
        "type": "file",
        "size": 3.826,
        "file_type": "py",
        "inclusive_size": 3.826,
        "id": "src\\transformers\\models\\gemma\\tokenization_gemma.py"
      },
      {
        "type": "file",
        "size": 1.072,
        "file_type": "py",
        "inclusive_size": 1.072,
        "id": "src\\transformers\\models\\gemma\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.651,
        "file_type": "py",
        "inclusive_size": 8.651,
        "id": "src\\transformers\\models\\fuyu\\configuration_fuyu.py"
      },
      {
        "type": "file",
        "size": 4.124,
        "file_type": "py",
        "inclusive_size": 4.124,
        "id": "src\\transformers\\models\\fuyu\\convert_fuyu_model_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 33.958,
        "file_type": "py",
        "inclusive_size": 33.958,
        "id": "src\\transformers\\models\\fuyu\\image_processing_fuyu.py"
      },
      {
        "type": "file",
        "size": 17.173,
        "file_type": "py",
        "inclusive_size": 17.173,
        "id": "src\\transformers\\models\\fuyu\\image_processing_fuyu_fast.py"
      },
      {
        "type": "file",
        "size": 17.619,
        "file_type": "py",
        "inclusive_size": 17.619,
        "id": "src\\transformers\\models\\fuyu\\modeling_fuyu.py"
      },
      {
        "type": "file",
        "size": 35.522,
        "file_type": "py",
        "inclusive_size": 35.522,
        "id": "src\\transformers\\models\\fuyu\\processing_fuyu.py"
      },
      {
        "type": "file",
        "size": 1.111,
        "file_type": "py",
        "inclusive_size": 1.111,
        "id": "src\\transformers\\models\\fuyu\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.969,
        "file_type": "py",
        "inclusive_size": 7.969,
        "id": "src\\transformers\\models\\funnel\\configuration_funnel.py"
      },
      {
        "type": "file",
        "size": 5.84,
        "file_type": "py",
        "inclusive_size": 5.84,
        "id": "src\\transformers\\models\\funnel\\convert_funnel_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 57.857,
        "file_type": "py",
        "inclusive_size": 57.857,
        "id": "src\\transformers\\models\\funnel\\modeling_funnel.py"
      },
      {
        "type": "file",
        "size": 6.792,
        "file_type": "py",
        "inclusive_size": 6.792,
        "id": "src\\transformers\\models\\funnel\\tokenization_funnel.py"
      },
      {
        "type": "file",
        "size": 1.1,
        "file_type": "py",
        "inclusive_size": 1.1,
        "id": "src\\transformers\\models\\funnel\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.14,
        "file_type": "py",
        "inclusive_size": 10.14,
        "id": "src\\transformers\\models\\fsmt\\configuration_fsmt.py"
      },
      {
        "type": "file",
        "size": 11.249,
        "file_type": "py",
        "inclusive_size": 11.249,
        "id": "src\\transformers\\models\\fsmt\\convert_fsmt_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 46.933,
        "file_type": "py",
        "inclusive_size": 46.933,
        "id": "src\\transformers\\models\\fsmt\\modeling_fsmt.py"
      },
      {
        "type": "file",
        "size": 17.893,
        "file_type": "py",
        "inclusive_size": 17.893,
        "id": "src\\transformers\\models\\fsmt\\tokenization_fsmt.py"
      },
      {
        "type": "file",
        "size": 1.026,
        "file_type": "py",
        "inclusive_size": 1.026,
        "id": "src\\transformers\\models\\fsmt\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.042,
        "file_type": "py",
        "inclusive_size": 8.042,
        "id": "src\\transformers\\models\\focalnet\\configuration_focalnet.py"
      },
      {
        "type": "file",
        "size": 9.451,
        "file_type": "py",
        "inclusive_size": 9.451,
        "id": "src\\transformers\\models\\focalnet\\convert_focalnet_to_hf_format.py"
      },
      {
        "type": "file",
        "size": 36.704,
        "file_type": "py",
        "inclusive_size": 36.704,
        "id": "src\\transformers\\models\\focalnet\\modeling_focalnet.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\focalnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.645,
        "file_type": "py",
        "inclusive_size": 5.645,
        "id": "src\\transformers\\models\\fnet\\configuration_fnet.py"
      },
      {
        "type": "file",
        "size": 6.896,
        "file_type": "py",
        "inclusive_size": 6.896,
        "id": "src\\transformers\\models\\fnet\\convert_fnet_original_flax_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 42.608,
        "file_type": "py",
        "inclusive_size": 42.608,
        "id": "src\\transformers\\models\\fnet\\modeling_fnet.py"
      },
      {
        "type": "file",
        "size": 3.226,
        "file_type": "py",
        "inclusive_size": 3.226,
        "id": "src\\transformers\\models\\fnet\\tokenization_fnet.py"
      },
      {
        "type": "file",
        "size": 1.057,
        "file_type": "py",
        "inclusive_size": 1.057,
        "id": "src\\transformers\\models\\fnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.83,
        "file_type": "py",
        "inclusive_size": 9.83,
        "id": "src\\transformers\\models\\florence2\\configuration_florence2.py"
      },
      {
        "type": "file",
        "size": 18.526,
        "file_type": "py",
        "inclusive_size": 18.526,
        "id": "src\\transformers\\models\\florence2\\convert_florence2_original_pytorch_to_hf.py"
      },
      {
        "type": "file",
        "size": 43.185,
        "file_type": "py",
        "inclusive_size": 43.185,
        "id": "src\\transformers\\models\\florence2\\modeling_florence2.py"
      },
      {
        "type": "file",
        "size": 76.723,
        "file_type": "py",
        "inclusive_size": 76.723,
        "id": "src\\transformers\\models\\florence2\\modular_florence2.py"
      },
      {
        "type": "file",
        "size": 34.33,
        "file_type": "py",
        "inclusive_size": 34.33,
        "id": "src\\transformers\\models\\florence2\\processing_florence2.py"
      },
      {
        "type": "file",
        "size": 1.039,
        "file_type": "py",
        "inclusive_size": 1.039,
        "id": "src\\transformers\\models\\florence2\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.093,
        "file_type": "py",
        "inclusive_size": 10.093,
        "id": "src\\transformers\\models\\flex_olmo\\configuration_flex_olmo.py"
      },
      {
        "type": "file",
        "size": 31.326,
        "file_type": "py",
        "inclusive_size": 31.326,
        "id": "src\\transformers\\models\\flex_olmo\\modeling_flex_olmo.py"
      },
      {
        "type": "file",
        "size": 15.811,
        "file_type": "py",
        "inclusive_size": 15.811,
        "id": "src\\transformers\\models\\flex_olmo\\modular_flex_olmo.py"
      },
      {
        "type": "file",
        "size": 1.0,
        "file_type": "py",
        "inclusive_size": 1.0,
        "id": "src\\transformers\\models\\flex_olmo\\__init__.py"
      },
      {
        "type": "file",
        "size": 32.919,
        "file_type": "py",
        "inclusive_size": 32.919,
        "id": "src\\transformers\\models\\flava\\configuration_flava.py"
      },
      {
        "type": "file",
        "size": 3.432,
        "file_type": "py",
        "inclusive_size": 3.432,
        "id": "src\\transformers\\models\\flava\\convert_dalle_to_flava_codebook.py"
      },
      {
        "type": "file",
        "size": 4.376,
        "file_type": "py",
        "inclusive_size": 4.376,
        "id": "src\\transformers\\models\\flava\\convert_flava_original_pytorch_to_hf.py"
      },
      {
        "type": "file",
        "size": 42.407,
        "file_type": "py",
        "inclusive_size": 42.407,
        "id": "src\\transformers\\models\\flava\\image_processing_flava.py"
      },
      {
        "type": "file",
        "size": 16.408,
        "file_type": "py",
        "inclusive_size": 16.408,
        "id": "src\\transformers\\models\\flava\\image_processing_flava_fast.py"
      },
      {
        "type": "file",
        "size": 89.356,
        "file_type": "py",
        "inclusive_size": 89.356,
        "id": "src\\transformers\\models\\flava\\modeling_flava.py"
      },
      {
        "type": "file",
        "size": 0.974,
        "file_type": "py",
        "inclusive_size": 0.974,
        "id": "src\\transformers\\models\\flava\\processing_flava.py"
      },
      {
        "type": "file",
        "size": 1.16,
        "file_type": "py",
        "inclusive_size": 1.16,
        "id": "src\\transformers\\models\\flava\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.797,
        "file_type": "py",
        "inclusive_size": 10.797,
        "id": "src\\transformers\\models\\flaubert\\configuration_flaubert.py"
      },
      {
        "type": "file",
        "size": 77.702,
        "file_type": "py",
        "inclusive_size": 77.702,
        "id": "src\\transformers\\models\\flaubert\\modeling_flaubert.py"
      },
      {
        "type": "file",
        "size": 20.919,
        "file_type": "py",
        "inclusive_size": 20.919,
        "id": "src\\transformers\\models\\flaubert\\tokenization_flaubert.py"
      },
      {
        "type": "file",
        "size": 1.038,
        "file_type": "py",
        "inclusive_size": 1.038,
        "id": "src\\transformers\\models\\flaubert\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.969,
        "file_type": "py",
        "inclusive_size": 6.969,
        "id": "src\\transformers\\models\\fast_vlm\\configuration_fast_vlm.py"
      },
      {
        "type": "file",
        "size": 9.822,
        "file_type": "py",
        "inclusive_size": 9.822,
        "id": "src\\transformers\\models\\fast_vlm\\convert_fastvlm_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 19.477,
        "file_type": "py",
        "inclusive_size": 19.477,
        "id": "src\\transformers\\models\\fast_vlm\\modeling_fast_vlm.py"
      },
      {
        "type": "file",
        "size": 16.686,
        "file_type": "py",
        "inclusive_size": 16.686,
        "id": "src\\transformers\\models\\fast_vlm\\modular_fast_vlm.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\fast_vlm\\__init__.py"
      },
      {
        "type": "file",
        "size": 24.797,
        "file_type": "py",
        "inclusive_size": 24.797,
        "id": "src\\transformers\\models\\fastspeech2_conformer\\configuration_fastspeech2_conformer.py"
      },
      {
        "type": "file",
        "size": 8.951,
        "file_type": "py",
        "inclusive_size": 8.951,
        "id": "src\\transformers\\models\\fastspeech2_conformer\\convert_fastspeech2_conformer_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 5.443,
        "file_type": "py",
        "inclusive_size": 5.443,
        "id": "src\\transformers\\models\\fastspeech2_conformer\\convert_hifigan.py"
      },
      {
        "type": "file",
        "size": 3.46,
        "file_type": "py",
        "inclusive_size": 3.46,
        "id": "src\\transformers\\models\\fastspeech2_conformer\\convert_model_with_hifigan.py"
      },
      {
        "type": "file",
        "size": 70.272,
        "file_type": "py",
        "inclusive_size": 70.272,
        "id": "src\\transformers\\models\\fastspeech2_conformer\\modeling_fastspeech2_conformer.py"
      },
      {
        "type": "file",
        "size": 6.256,
        "file_type": "py",
        "inclusive_size": 6.256,
        "id": "src\\transformers\\models\\fastspeech2_conformer\\tokenization_fastspeech2_conformer.py"
      },
      {
        "type": "file",
        "size": 1.077,
        "file_type": "py",
        "inclusive_size": 1.077,
        "id": "src\\transformers\\models\\fastspeech2_conformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.955,
        "file_type": "py",
        "inclusive_size": 8.955,
        "id": "src\\transformers\\models\\falcon_mamba\\configuration_falcon_mamba.py"
      },
      {
        "type": "file",
        "size": 42.353,
        "file_type": "py",
        "inclusive_size": 42.353,
        "id": "src\\transformers\\models\\falcon_mamba\\modeling_falcon_mamba.py"
      },
      {
        "type": "file",
        "size": 26.234,
        "file_type": "py",
        "inclusive_size": 26.234,
        "id": "src\\transformers\\models\\falcon_mamba\\modular_falcon_mamba.py"
      },
      {
        "type": "file",
        "size": 1.005,
        "file_type": "py",
        "inclusive_size": 1.005,
        "id": "src\\transformers\\models\\falcon_mamba\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.045,
        "file_type": "py",
        "inclusive_size": 15.045,
        "id": "src\\transformers\\models\\falcon_h1\\configuration_falcon_h1.py"
      },
      {
        "type": "file",
        "size": 5.853,
        "file_type": "py",
        "inclusive_size": 5.853,
        "id": "src\\transformers\\models\\falcon_h1\\convert_mamba_ssm_checkpoint.py"
      },
      {
        "type": "file",
        "size": 73.844,
        "file_type": "py",
        "inclusive_size": 73.844,
        "id": "src\\transformers\\models\\falcon_h1\\modeling_falcon_h1.py"
      },
      {
        "type": "file",
        "size": 59.339,
        "file_type": "py",
        "inclusive_size": 59.339,
        "id": "src\\transformers\\models\\falcon_h1\\modular_falcon_h1.py"
      },
      {
        "type": "file",
        "size": 1.012,
        "file_type": "py",
        "inclusive_size": 1.012,
        "id": "src\\transformers\\models\\falcon_h1\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.808,
        "file_type": "py",
        "inclusive_size": 8.808,
        "id": "src\\transformers\\models\\falcon\\configuration_falcon.py"
      },
      {
        "type": "file",
        "size": 3.061,
        "file_type": "py",
        "inclusive_size": 3.061,
        "id": "src\\transformers\\models\\falcon\\convert_custom_code_checkpoint.py"
      },
      {
        "type": "file",
        "size": 63.164,
        "file_type": "py",
        "inclusive_size": 63.164,
        "id": "src\\transformers\\models\\falcon\\modeling_falcon.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\falcon\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.229,
        "file_type": "py",
        "inclusive_size": 10.229,
        "id": "src\\transformers\\models\\exaone4\\configuration_exaone4.py"
      },
      {
        "type": "file",
        "size": 24.715,
        "file_type": "py",
        "inclusive_size": 24.715,
        "id": "src\\transformers\\models\\exaone4\\modeling_exaone4.py"
      },
      {
        "type": "file",
        "size": 21.202,
        "file_type": "py",
        "inclusive_size": 21.202,
        "id": "src\\transformers\\models\\exaone4\\modular_exaone4.py"
      },
      {
        "type": "file",
        "size": 1.018,
        "file_type": "py",
        "inclusive_size": 1.018,
        "id": "src\\transformers\\models\\exaone4\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.484,
        "file_type": "py",
        "inclusive_size": 14.484,
        "id": "src\\transformers\\models\\evolla\\configuration_evolla.py"
      },
      {
        "type": "file",
        "size": 63.11,
        "file_type": "py",
        "inclusive_size": 63.11,
        "id": "src\\transformers\\models\\evolla\\modeling_evolla.py"
      },
      {
        "type": "file",
        "size": 37.51,
        "file_type": "py",
        "inclusive_size": 37.51,
        "id": "src\\transformers\\models\\evolla\\modular_evolla.py"
      },
      {
        "type": "file",
        "size": 8.231,
        "file_type": "py",
        "inclusive_size": 8.231,
        "id": "src\\transformers\\models\\evolla\\processing_evolla.py"
      },
      {
        "type": "file",
        "size": 1.03,
        "file_type": "py",
        "inclusive_size": 1.03,
        "id": "src\\transformers\\models\\evolla\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.671,
        "file_type": "py",
        "inclusive_size": 13.671,
        "id": "src\\transformers\\models\\esm\\configuration_esm.py"
      },
      {
        "type": "file",
        "size": 18.453,
        "file_type": "py",
        "inclusive_size": 18.453,
        "id": "src\\transformers\\models\\esm\\convert_esm.py"
      },
      {
        "type": "file",
        "size": 39.29,
        "file_type": "py",
        "inclusive_size": 39.29,
        "id": "src\\transformers\\models\\esm\\modeling_esm.py"
      },
      {
        "type": "file",
        "size": 85.61,
        "file_type": "py",
        "inclusive_size": 85.61,
        "id": "src\\transformers\\models\\esm\\modeling_esmfold.py"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 125.586,
        "id": "src\\transformers\\models\\esm\\openfold_utils"
      },
      {
        "type": "file",
        "size": 5.331,
        "file_type": "py",
        "inclusive_size": 5.331,
        "id": "src\\transformers\\models\\esm\\tokenization_esm.py"
      },
      {
        "type": "file",
        "size": 1.059,
        "file_type": "py",
        "inclusive_size": 1.059,
        "id": "src\\transformers\\models\\esm\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.357,
        "file_type": "py",
        "inclusive_size": 14.357,
        "id": "src\\transformers\\models\\esm\\openfold_utils\\chunk_utils.py"
      },
      {
        "type": "file",
        "size": 3.688,
        "file_type": "py",
        "inclusive_size": 3.688,
        "id": "src\\transformers\\models\\esm\\openfold_utils\\data_transforms.py"
      },
      {
        "type": "file",
        "size": 8.355,
        "file_type": "py",
        "inclusive_size": 8.355,
        "id": "src\\transformers\\models\\esm\\openfold_utils\\feats.py"
      },
      {
        "type": "file",
        "size": 3.661,
        "file_type": "py",
        "inclusive_size": 3.661,
        "id": "src\\transformers\\models\\esm\\openfold_utils\\loss.py"
      },
      {
        "type": "file",
        "size": 11.482,
        "file_type": "py",
        "inclusive_size": 11.482,
        "id": "src\\transformers\\models\\esm\\openfold_utils\\protein.py"
      },
      {
        "type": "file",
        "size": 37.875,
        "file_type": "py",
        "inclusive_size": 37.875,
        "id": "src\\transformers\\models\\esm\\openfold_utils\\residue_constants.py"
      },
      {
        "type": "file",
        "size": 41.006,
        "file_type": "py",
        "inclusive_size": 41.006,
        "id": "src\\transformers\\models\\esm\\openfold_utils\\rigid_utils.py"
      },
      {
        "type": "file",
        "size": 4.716,
        "file_type": "py",
        "inclusive_size": 4.716,
        "id": "src\\transformers\\models\\esm\\openfold_utils\\tensor_utils.py"
      },
      {
        "type": "file",
        "size": 0.446,
        "file_type": "py",
        "inclusive_size": 0.446,
        "id": "src\\transformers\\models\\esm\\openfold_utils\\__init__.py"
      },
      {
        "type": "file",
        "size": 16.961,
        "file_type": "py",
        "inclusive_size": 16.961,
        "id": "src\\transformers\\models\\ernie4_5_vl_moe\\configuration_ernie4_5_vl_moe.py"
      },
      {
        "type": "file",
        "size": 13.119,
        "file_type": "py",
        "inclusive_size": 13.119,
        "id": "src\\transformers\\models\\ernie4_5_vl_moe\\convert_ernie4_5_vl_moe_to_hf.py"
      },
      {
        "type": "file",
        "size": 23.377,
        "file_type": "py",
        "inclusive_size": 23.377,
        "id": "src\\transformers\\models\\ernie4_5_vl_moe\\image_processing_ernie4_5_vl_moe.py"
      },
      {
        "type": "file",
        "size": 10.147,
        "file_type": "py",
        "inclusive_size": 10.147,
        "id": "src\\transformers\\models\\ernie4_5_vl_moe\\image_processing_ernie4_5_vl_moe_fast.py"
      },
      {
        "type": "file",
        "size": 89.027,
        "file_type": "py",
        "inclusive_size": 89.027,
        "id": "src\\transformers\\models\\ernie4_5_vl_moe\\modeling_ernie4_5_vl_moe.py"
      },
      {
        "type": "file",
        "size": 89.129,
        "file_type": "py",
        "inclusive_size": 89.129,
        "id": "src\\transformers\\models\\ernie4_5_vl_moe\\modular_ernie4_5_vl_moe.py"
      },
      {
        "type": "file",
        "size": 12.798,
        "file_type": "py",
        "inclusive_size": 12.798,
        "id": "src\\transformers\\models\\ernie4_5_vl_moe\\processing_ernie4_5_vl_moe.py"
      },
      {
        "type": "file",
        "size": 25.862,
        "file_type": "py",
        "inclusive_size": 25.862,
        "id": "src\\transformers\\models\\ernie4_5_vl_moe\\video_processing_ernie4_5_vl_moe.py"
      },
      {
        "type": "file",
        "size": 1.241,
        "file_type": "py",
        "inclusive_size": 1.241,
        "id": "src\\transformers\\models\\ernie4_5_vl_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.356,
        "file_type": "py",
        "inclusive_size": 10.356,
        "id": "src\\transformers\\models\\ernie4_5_moe\\configuration_ernie4_5_moe.py"
      },
      {
        "type": "file",
        "size": 33.004,
        "file_type": "py",
        "inclusive_size": 33.004,
        "id": "src\\transformers\\models\\ernie4_5_moe\\modeling_ernie4_5_moe.py"
      },
      {
        "type": "file",
        "size": 12.98,
        "file_type": "py",
        "inclusive_size": 12.98,
        "id": "src\\transformers\\models\\ernie4_5_moe\\modular_ernie4_5_moe.py"
      },
      {
        "type": "file",
        "size": 1.005,
        "file_type": "py",
        "inclusive_size": 1.005,
        "id": "src\\transformers\\models\\ernie4_5_moe\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.82,
        "file_type": "py",
        "inclusive_size": 7.82,
        "id": "src\\transformers\\models\\ernie4_5\\configuration_ernie4_5.py"
      },
      {
        "type": "file",
        "size": 3.266,
        "file_type": "py",
        "inclusive_size": 3.266,
        "id": "src\\transformers\\models\\ernie4_5\\convert_ernie4_5_tokenizer.py"
      },
      {
        "type": "file",
        "size": 21.582,
        "file_type": "py",
        "inclusive_size": 21.582,
        "id": "src\\transformers\\models\\ernie4_5\\modeling_ernie4_5.py"
      },
      {
        "type": "file",
        "size": 5.569,
        "file_type": "py",
        "inclusive_size": 5.569,
        "id": "src\\transformers\\models\\ernie4_5\\modular_ernie4_5.py"
      },
      {
        "type": "file",
        "size": 0.997,
        "file_type": "py",
        "inclusive_size": 0.997,
        "id": "src\\transformers\\models\\ernie4_5\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.247,
        "file_type": "py",
        "inclusive_size": 7.247,
        "id": "src\\transformers\\models\\ernie\\configuration_ernie.py"
      },
      {
        "type": "file",
        "size": 64.323,
        "file_type": "py",
        "inclusive_size": 64.323,
        "id": "src\\transformers\\models\\ernie\\modeling_ernie.py"
      },
      {
        "type": "file",
        "size": 38.452,
        "file_type": "py",
        "inclusive_size": 38.452,
        "id": "src\\transformers\\models\\ernie\\modular_ernie.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\ernie\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.054,
        "file_type": "py",
        "inclusive_size": 8.054,
        "id": "src\\transformers\\models\\eomt\\configuration_eomt.py"
      },
      {
        "type": "file",
        "size": 12.088,
        "file_type": "py",
        "inclusive_size": 12.088,
        "id": "src\\transformers\\models\\eomt\\convert_eomt_to_hf.py"
      },
      {
        "type": "file",
        "size": 40.587,
        "file_type": "py",
        "inclusive_size": 40.587,
        "id": "src\\transformers\\models\\eomt\\image_processing_eomt.py"
      },
      {
        "type": "file",
        "size": 22.466,
        "file_type": "py",
        "inclusive_size": 22.466,
        "id": "src\\transformers\\models\\eomt\\image_processing_eomt_fast.py"
      },
      {
        "type": "file",
        "size": 54.621,
        "file_type": "py",
        "inclusive_size": 54.621,
        "id": "src\\transformers\\models\\eomt\\modeling_eomt.py"
      },
      {
        "type": "file",
        "size": 25.855,
        "file_type": "py",
        "inclusive_size": 25.855,
        "id": "src\\transformers\\models\\eomt\\modular_eomt.py"
      },
      {
        "type": "file",
        "size": 1.076,
        "file_type": "py",
        "inclusive_size": 1.076,
        "id": "src\\transformers\\models\\eomt\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.007,
        "file_type": "py",
        "inclusive_size": 5.007,
        "id": "src\\transformers\\models\\encoder_decoder\\configuration_encoder_decoder.py"
      },
      {
        "type": "file",
        "size": 22.527,
        "file_type": "py",
        "inclusive_size": 22.527,
        "id": "src\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py"
      },
      {
        "type": "file",
        "size": 1.011,
        "file_type": "py",
        "inclusive_size": 1.011,
        "id": "src\\transformers\\models\\encoder_decoder\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.656,
        "file_type": "py",
        "inclusive_size": 8.656,
        "id": "src\\transformers\\models\\encodec\\configuration_encodec.py"
      },
      {
        "type": "file",
        "size": 15.265,
        "file_type": "py",
        "inclusive_size": 15.265,
        "id": "src\\transformers\\models\\encodec\\convert_encodec_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 9.784,
        "file_type": "py",
        "inclusive_size": 9.784,
        "id": "src\\transformers\\models\\encodec\\feature_extraction_encodec.py"
      },
      {
        "type": "file",
        "size": 35.224,
        "file_type": "py",
        "inclusive_size": 35.224,
        "id": "src\\transformers\\models\\encodec\\modeling_encodec.py"
      },
      {
        "type": "file",
        "size": 1.041,
        "file_type": "py",
        "inclusive_size": 1.041,
        "id": "src\\transformers\\models\\encodec\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.481,
        "file_type": "py",
        "inclusive_size": 13.481,
        "id": "src\\transformers\\models\\emu3\\configuration_emu3.py"
      },
      {
        "type": "file",
        "size": 16.686,
        "file_type": "py",
        "inclusive_size": 16.686,
        "id": "src\\transformers\\models\\emu3\\convert_emu3_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 27.178,
        "file_type": "py",
        "inclusive_size": 27.178,
        "id": "src\\transformers\\models\\emu3\\image_processing_emu3.py"
      },
      {
        "type": "file",
        "size": 67.591,
        "file_type": "py",
        "inclusive_size": 67.591,
        "id": "src\\transformers\\models\\emu3\\modeling_emu3.py"
      },
      {
        "type": "file",
        "size": 47.783,
        "file_type": "py",
        "inclusive_size": 47.783,
        "id": "src\\transformers\\models\\emu3\\modular_emu3.py"
      },
      {
        "type": "file",
        "size": 11.488,
        "file_type": "py",
        "inclusive_size": 11.488,
        "id": "src\\transformers\\models\\emu3\\processing_emu3.py"
      },
      {
        "type": "file",
        "size": 1.07,
        "file_type": "py",
        "inclusive_size": 1.07,
        "id": "src\\transformers\\models\\emu3\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.088,
        "file_type": "py",
        "inclusive_size": 8.088,
        "id": "src\\transformers\\models\\electra\\configuration_electra.py"
      },
      {
        "type": "file",
        "size": 6.46,
        "file_type": "py",
        "inclusive_size": 6.46,
        "id": "src\\transformers\\models\\electra\\convert_electra_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 58.439,
        "file_type": "py",
        "inclusive_size": 58.439,
        "id": "src\\transformers\\models\\electra\\modeling_electra.py"
      },
      {
        "type": "file",
        "size": 1.07,
        "file_type": "py",
        "inclusive_size": 1.07,
        "id": "src\\transformers\\models\\electra\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.076,
        "file_type": "py",
        "inclusive_size": 7.076,
        "id": "src\\transformers\\models\\efficientnet\\configuration_efficientnet.py"
      },
      {
        "type": "file",
        "size": 12.808,
        "file_type": "py",
        "inclusive_size": 12.808,
        "id": "src\\transformers\\models\\efficientnet\\convert_efficientnet_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 18.47,
        "file_type": "py",
        "inclusive_size": 18.47,
        "id": "src\\transformers\\models\\efficientnet\\image_processing_efficientnet.py"
      },
      {
        "type": "file",
        "size": 7.18,
        "file_type": "py",
        "inclusive_size": 7.18,
        "id": "src\\transformers\\models\\efficientnet\\image_processing_efficientnet_fast.py"
      },
      {
        "type": "file",
        "size": 20.366,
        "file_type": "py",
        "inclusive_size": 20.366,
        "id": "src\\transformers\\models\\efficientnet\\modeling_efficientnet.py"
      },
      {
        "type": "file",
        "size": 1.108,
        "file_type": "py",
        "inclusive_size": 1.108,
        "id": "src\\transformers\\models\\efficientnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.406,
        "file_type": "py",
        "inclusive_size": 9.406,
        "id": "src\\transformers\\models\\efficientloftr\\configuration_efficientloftr.py"
      },
      {
        "type": "file",
        "size": 12.445,
        "file_type": "py",
        "inclusive_size": 12.445,
        "id": "src\\transformers\\models\\efficientloftr\\convert_efficientloftr_to_hf.py"
      },
      {
        "type": "file",
        "size": 21.509,
        "file_type": "py",
        "inclusive_size": 21.509,
        "id": "src\\transformers\\models\\efficientloftr\\image_processing_efficientloftr.py"
      },
      {
        "type": "file",
        "size": 12.433,
        "file_type": "py",
        "inclusive_size": 12.433,
        "id": "src\\transformers\\models\\efficientloftr\\image_processing_efficientloftr_fast.py"
      },
      {
        "type": "file",
        "size": 61.42,
        "file_type": "py",
        "inclusive_size": 61.42,
        "id": "src\\transformers\\models\\efficientloftr\\modeling_efficientloftr.py"
      },
      {
        "type": "file",
        "size": 3.263,
        "file_type": "py",
        "inclusive_size": 3.263,
        "id": "src\\transformers\\models\\efficientloftr\\modular_efficientloftr.py"
      },
      {
        "type": "file",
        "size": 1.116,
        "file_type": "py",
        "inclusive_size": 1.116,
        "id": "src\\transformers\\models\\efficientloftr\\__init__.py"
      },
      {
        "type": "file",
        "size": 23.952,
        "file_type": "py",
        "inclusive_size": 23.952,
        "id": "src\\transformers\\models\\edgetam_video\\configuration_edgetam_video.py"
      },
      {
        "type": "file",
        "size": 13.975,
        "file_type": "py",
        "inclusive_size": 13.975,
        "id": "src\\transformers\\models\\edgetam_video\\convert_edgetam_video_to_hf.py"
      },
      {
        "type": "file",
        "size": 146.532,
        "file_type": "py",
        "inclusive_size": 146.532,
        "id": "src\\transformers\\models\\edgetam_video\\modeling_edgetam_video.py"
      },
      {
        "type": "file",
        "size": 71.148,
        "file_type": "py",
        "inclusive_size": 71.148,
        "id": "src\\transformers\\models\\edgetam_video\\modular_edgetam_video.py"
      },
      {
        "type": "file",
        "size": 1.008,
        "file_type": "py",
        "inclusive_size": 1.008,
        "id": "src\\transformers\\models\\edgetam_video\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.422,
        "file_type": "py",
        "inclusive_size": 15.422,
        "id": "src\\transformers\\models\\edgetam\\configuration_edgetam.py"
      },
      {
        "type": "file",
        "size": 11.354,
        "file_type": "py",
        "inclusive_size": 11.354,
        "id": "src\\transformers\\models\\edgetam\\convert_edgetam_to_hf.py"
      },
      {
        "type": "file",
        "size": 58.743,
        "file_type": "py",
        "inclusive_size": 58.743,
        "id": "src\\transformers\\models\\edgetam\\modeling_edgetam.py"
      },
      {
        "type": "file",
        "size": 9.23,
        "file_type": "py",
        "inclusive_size": 9.23,
        "id": "src\\transformers\\models\\edgetam\\modular_edgetam.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\edgetam\\__init__.py"
      },
      {
        "type": "file",
        "size": 21.937,
        "file_type": "py",
        "inclusive_size": 21.937,
        "id": "src\\transformers\\models\\d_fine\\configuration_d_fine.py"
      },
      {
        "type": "file",
        "size": 38.806,
        "file_type": "py",
        "inclusive_size": 38.806,
        "id": "src\\transformers\\models\\d_fine\\convert_d_fine_original_pytorch_checkpoint_to_hf.py"
      },
      {
        "type": "file",
        "size": 105.549,
        "file_type": "py",
        "inclusive_size": 105.549,
        "id": "src\\transformers\\models\\d_fine\\modeling_d_fine.py"
      },
      {
        "type": "file",
        "size": 57.117,
        "file_type": "py",
        "inclusive_size": 57.117,
        "id": "src\\transformers\\models\\d_fine\\modular_d_fine.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\d_fine\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.98,
        "file_type": "py",
        "inclusive_size": 13.98,
        "id": "src\\transformers\\models\\dpt\\configuration_dpt.py"
      },
      {
        "type": "file",
        "size": 16.979,
        "file_type": "py",
        "inclusive_size": 16.979,
        "id": "src\\transformers\\models\\dpt\\convert_dinov2_depth_to_hf.py"
      },
      {
        "type": "file",
        "size": 14.42,
        "file_type": "py",
        "inclusive_size": 14.42,
        "id": "src\\transformers\\models\\dpt\\convert_dpt_beit_to_hf.py"
      },
      {
        "type": "file",
        "size": 13.061,
        "file_type": "py",
        "inclusive_size": 13.061,
        "id": "src\\transformers\\models\\dpt\\convert_dpt_hybrid_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 15.271,
        "file_type": "py",
        "inclusive_size": 15.271,
        "id": "src\\transformers\\models\\dpt\\convert_dpt_swinv2_to_hf.py"
      },
      {
        "type": "file",
        "size": 11.596,
        "file_type": "py",
        "inclusive_size": 11.596,
        "id": "src\\transformers\\models\\dpt\\convert_dpt_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 32.105,
        "file_type": "py",
        "inclusive_size": 32.105,
        "id": "src\\transformers\\models\\dpt\\image_processing_dpt.py"
      },
      {
        "type": "file",
        "size": 15.628,
        "file_type": "py",
        "inclusive_size": 15.628,
        "id": "src\\transformers\\models\\dpt\\image_processing_dpt_fast.py"
      },
      {
        "type": "file",
        "size": 48.299,
        "file_type": "py",
        "inclusive_size": 48.299,
        "id": "src\\transformers\\models\\dpt\\modeling_dpt.py"
      },
      {
        "type": "file",
        "size": 10.479,
        "file_type": "py",
        "inclusive_size": 10.479,
        "id": "src\\transformers\\models\\dpt\\modular_dpt.py"
      },
      {
        "type": "file",
        "size": 1.114,
        "file_type": "py",
        "inclusive_size": 1.114,
        "id": "src\\transformers\\models\\dpt\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.489,
        "file_type": "py",
        "inclusive_size": 6.489,
        "id": "src\\transformers\\models\\dpr\\configuration_dpr.py"
      },
      {
        "type": "file",
        "size": 6.165,
        "file_type": "py",
        "inclusive_size": 6.165,
        "id": "src\\transformers\\models\\dpr\\convert_dpr_original_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 21.848,
        "file_type": "py",
        "inclusive_size": 21.848,
        "id": "src\\transformers\\models\\dpr\\modeling_dpr.py"
      },
      {
        "type": "file",
        "size": 16.128,
        "file_type": "py",
        "inclusive_size": 16.128,
        "id": "src\\transformers\\models\\dpr\\tokenization_dpr.py"
      },
      {
        "type": "file",
        "size": 16.02,
        "file_type": "py",
        "inclusive_size": 16.02,
        "id": "src\\transformers\\models\\dpr\\tokenization_dpr_fast.py"
      },
      {
        "type": "file",
        "size": 1.064,
        "file_type": "py",
        "inclusive_size": 1.064,
        "id": "src\\transformers\\models\\dpr\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.716,
        "file_type": "py",
        "inclusive_size": 10.716,
        "id": "src\\transformers\\models\\dots1\\configuration_dots1.py"
      },
      {
        "type": "file",
        "size": 29.529,
        "file_type": "py",
        "inclusive_size": 29.529,
        "id": "src\\transformers\\models\\dots1\\modeling_dots1.py"
      },
      {
        "type": "file",
        "size": 4.55,
        "file_type": "py",
        "inclusive_size": 4.55,
        "id": "src\\transformers\\models\\dots1\\modular_dots1.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\dots1\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.77,
        "file_type": "py",
        "inclusive_size": 5.77,
        "id": "src\\transformers\\models\\donut\\configuration_donut_swin.py"
      },
      {
        "type": "file",
        "size": 9.323,
        "file_type": "py",
        "inclusive_size": 9.323,
        "id": "src\\transformers\\models\\donut\\convert_donut_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 22.228,
        "file_type": "py",
        "inclusive_size": 22.228,
        "id": "src\\transformers\\models\\donut\\image_processing_donut.py"
      },
      {
        "type": "file",
        "size": 9.177,
        "file_type": "py",
        "inclusive_size": 9.177,
        "id": "src\\transformers\\models\\donut\\image_processing_donut_fast.py"
      },
      {
        "type": "file",
        "size": 42.295,
        "file_type": "py",
        "inclusive_size": 42.295,
        "id": "src\\transformers\\models\\donut\\modeling_donut_swin.py"
      },
      {
        "type": "file",
        "size": 5.287,
        "file_type": "py",
        "inclusive_size": 5.287,
        "id": "src\\transformers\\models\\donut\\processing_donut.py"
      },
      {
        "type": "file",
        "size": 1.17,
        "file_type": "py",
        "inclusive_size": 1.17,
        "id": "src\\transformers\\models\\donut\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.251,
        "file_type": "py",
        "inclusive_size": 11.251,
        "id": "src\\transformers\\models\\doge\\configuration_doge.py"
      },
      {
        "type": "file",
        "size": 4.692,
        "file_type": "py",
        "inclusive_size": 4.692,
        "id": "src\\transformers\\models\\doge\\convert_doge_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 37.058,
        "file_type": "py",
        "inclusive_size": 37.058,
        "id": "src\\transformers\\models\\doge\\modeling_doge.py"
      },
      {
        "type": "file",
        "size": 34.214,
        "file_type": "py",
        "inclusive_size": 34.214,
        "id": "src\\transformers\\models\\doge\\modular_doge.py"
      },
      {
        "type": "file",
        "size": 1.009,
        "file_type": "py",
        "inclusive_size": 1.009,
        "id": "src\\transformers\\models\\doge\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.128,
        "file_type": "py",
        "inclusive_size": 9.128,
        "id": "src\\transformers\\models\\dit\\convert_dit_unilm_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 0.0,
        "file_type": "py",
        "inclusive_size": 0.0,
        "id": "src\\transformers\\models\\dit\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.645,
        "file_type": "py",
        "inclusive_size": 5.645,
        "id": "src\\transformers\\models\\distilbert\\configuration_distilbert.py"
      },
      {
        "type": "file",
        "size": 39.874,
        "file_type": "py",
        "inclusive_size": 39.874,
        "id": "src\\transformers\\models\\distilbert\\modeling_distilbert.py"
      },
      {
        "type": "file",
        "size": 1.716,
        "file_type": "py",
        "inclusive_size": 1.716,
        "id": "src\\transformers\\models\\distilbert\\tokenization_distilbert.py"
      },
      {
        "type": "file",
        "size": 1.087,
        "file_type": "py",
        "inclusive_size": 1.087,
        "id": "src\\transformers\\models\\distilbert\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.062,
        "file_type": "py",
        "inclusive_size": 9.062,
        "id": "src\\transformers\\models\\dinov3_vit\\configuration_dinov3_vit.py"
      },
      {
        "type": "file",
        "size": 12.698,
        "file_type": "py",
        "inclusive_size": 12.698,
        "id": "src\\transformers\\models\\dinov3_vit\\convert_dinov3_vit_to_hf.py"
      },
      {
        "type": "file",
        "size": 3.785,
        "file_type": "py",
        "inclusive_size": 3.785,
        "id": "src\\transformers\\models\\dinov3_vit\\image_processing_dinov3_vit_fast.py"
      },
      {
        "type": "file",
        "size": 24.514,
        "file_type": "py",
        "inclusive_size": 24.514,
        "id": "src\\transformers\\models\\dinov3_vit\\modeling_dinov3_vit.py"
      },
      {
        "type": "file",
        "size": 19.717,
        "file_type": "py",
        "inclusive_size": 19.717,
        "id": "src\\transformers\\models\\dinov3_vit\\modular_dinov3_vit.py"
      },
      {
        "type": "file",
        "size": 1.053,
        "file_type": "py",
        "inclusive_size": 1.053,
        "id": "src\\transformers\\models\\dinov3_vit\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.639,
        "file_type": "py",
        "inclusive_size": 5.639,
        "id": "src\\transformers\\models\\dinov3_convnext\\configuration_dinov3_convnext.py"
      },
      {
        "type": "file",
        "size": 8.617,
        "file_type": "py",
        "inclusive_size": 8.617,
        "id": "src\\transformers\\models\\dinov3_convnext\\convert_dinov3_convnext_to_hf.py"
      },
      {
        "type": "file",
        "size": 11.71,
        "file_type": "py",
        "inclusive_size": 11.71,
        "id": "src\\transformers\\models\\dinov3_convnext\\modeling_dinov3_convnext.py"
      },
      {
        "type": "file",
        "size": 1.011,
        "file_type": "py",
        "inclusive_size": 1.011,
        "id": "src\\transformers\\models\\dinov3_convnext\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.619,
        "file_type": "py",
        "inclusive_size": 8.619,
        "id": "src\\transformers\\models\\dinov2_with_registers\\configuration_dinov2_with_registers.py"
      },
      {
        "type": "file",
        "size": 12.443,
        "file_type": "py",
        "inclusive_size": 12.443,
        "id": "src\\transformers\\models\\dinov2_with_registers\\convert_dinov2_with_registers_to_hf.py"
      },
      {
        "type": "file",
        "size": 27.61,
        "file_type": "py",
        "inclusive_size": 27.61,
        "id": "src\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py"
      },
      {
        "type": "file",
        "size": 19.014,
        "file_type": "py",
        "inclusive_size": 19.014,
        "id": "src\\transformers\\models\\dinov2_with_registers\\modular_dinov2_with_registers.py"
      },
      {
        "type": "file",
        "size": 1.023,
        "file_type": "py",
        "inclusive_size": 1.023,
        "id": "src\\transformers\\models\\dinov2_with_registers\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.719,
        "file_type": "py",
        "inclusive_size": 7.719,
        "id": "src\\transformers\\models\\dinov2\\configuration_dinov2.py"
      },
      {
        "type": "file",
        "size": 11.934,
        "file_type": "py",
        "inclusive_size": 11.934,
        "id": "src\\transformers\\models\\dinov2\\convert_dinov2_to_hf.py"
      },
      {
        "type": "file",
        "size": 25.316,
        "file_type": "py",
        "inclusive_size": 25.316,
        "id": "src\\transformers\\models\\dinov2\\modeling_dinov2.py"
      },
      {
        "type": "file",
        "size": 0.993,
        "file_type": "py",
        "inclusive_size": 0.993,
        "id": "src\\transformers\\models\\dinov2\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.341,
        "file_type": "py",
        "inclusive_size": 7.341,
        "id": "src\\transformers\\models\\dinat\\configuration_dinat.py"
      },
      {
        "type": "file",
        "size": 31.442,
        "file_type": "py",
        "inclusive_size": 31.442,
        "id": "src\\transformers\\models\\dinat\\modeling_dinat.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\dinat\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.812,
        "file_type": "py",
        "inclusive_size": 7.812,
        "id": "src\\transformers\\models\\diffllama\\configuration_diffllama.py"
      },
      {
        "type": "file",
        "size": 35.754,
        "file_type": "py",
        "inclusive_size": 35.754,
        "id": "src\\transformers\\models\\diffllama\\modeling_diffllama.py"
      },
      {
        "type": "file",
        "size": 19.858,
        "file_type": "py",
        "inclusive_size": 19.858,
        "id": "src\\transformers\\models\\diffllama\\modular_diffllama.py"
      },
      {
        "type": "file",
        "size": 1.004,
        "file_type": "py",
        "inclusive_size": 1.004,
        "id": "src\\transformers\\models\\diffllama\\__init__.py"
      },
      {
        "type": "file",
        "size": 1.556,
        "file_type": "py",
        "inclusive_size": 1.556,
        "id": "src\\transformers\\models\\dialogpt\\convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 0.0,
        "file_type": "py",
        "inclusive_size": 0.0,
        "id": "src\\transformers\\models\\dialogpt\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.262,
        "file_type": "py",
        "inclusive_size": 14.262,
        "id": "src\\transformers\\models\\dia\\configuration_dia.py"
      },
      {
        "type": "file",
        "size": 7.383,
        "file_type": "py",
        "inclusive_size": 7.383,
        "id": "src\\transformers\\models\\dia\\convert_dia_to_hf.py"
      },
      {
        "type": "file",
        "size": 8.353,
        "file_type": "py",
        "inclusive_size": 8.353,
        "id": "src\\transformers\\models\\dia\\feature_extraction_dia.py"
      },
      {
        "type": "file",
        "size": 22.01,
        "file_type": "py",
        "inclusive_size": 22.01,
        "id": "src\\transformers\\models\\dia\\generation_dia.py"
      },
      {
        "type": "file",
        "size": 40.54,
        "file_type": "py",
        "inclusive_size": 40.54,
        "id": "src\\transformers\\models\\dia\\modeling_dia.py"
      },
      {
        "type": "file",
        "size": 30.179,
        "file_type": "py",
        "inclusive_size": 30.179,
        "id": "src\\transformers\\models\\dia\\modular_dia.py"
      },
      {
        "type": "file",
        "size": 20.782,
        "file_type": "py",
        "inclusive_size": 20.782,
        "id": "src\\transformers\\models\\dia\\processing_dia.py"
      },
      {
        "type": "file",
        "size": 4.489,
        "file_type": "py",
        "inclusive_size": 4.489,
        "id": "src\\transformers\\models\\dia\\tokenization_dia.py"
      },
      {
        "type": "file",
        "size": 1.133,
        "file_type": "py",
        "inclusive_size": 1.133,
        "id": "src\\transformers\\models\\dia\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.458,
        "file_type": "py",
        "inclusive_size": 12.458,
        "id": "src\\transformers\\models\\detr\\configuration_detr.py"
      },
      {
        "type": "file",
        "size": 13.604,
        "file_type": "py",
        "inclusive_size": 13.604,
        "id": "src\\transformers\\models\\detr\\convert_detr_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 19.037,
        "file_type": "py",
        "inclusive_size": 19.037,
        "id": "src\\transformers\\models\\detr\\convert_detr_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 76.482,
        "file_type": "py",
        "inclusive_size": 76.482,
        "id": "src\\transformers\\models\\detr\\image_processing_detr.py"
      },
      {
        "type": "file",
        "size": 40.187,
        "file_type": "py",
        "inclusive_size": 40.187,
        "id": "src\\transformers\\models\\detr\\image_processing_detr_fast.py"
      },
      {
        "type": "file",
        "size": 78.102,
        "file_type": "py",
        "inclusive_size": 78.102,
        "id": "src\\transformers\\models\\detr\\modeling_detr.py"
      },
      {
        "type": "file",
        "size": 1.12,
        "file_type": "py",
        "inclusive_size": 1.12,
        "id": "src\\transformers\\models\\detr\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.66,
        "file_type": "py",
        "inclusive_size": 10.66,
        "id": "src\\transformers\\models\\depth_pro\\configuration_depth_pro.py"
      },
      {
        "type": "file",
        "size": 10.795,
        "file_type": "py",
        "inclusive_size": 10.795,
        "id": "src\\transformers\\models\\depth_pro\\convert_depth_pro_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 18.546,
        "file_type": "py",
        "inclusive_size": 18.546,
        "id": "src\\transformers\\models\\depth_pro\\image_processing_depth_pro.py"
      },
      {
        "type": "file",
        "size": 6.532,
        "file_type": "py",
        "inclusive_size": 6.532,
        "id": "src\\transformers\\models\\depth_pro\\image_processing_depth_pro_fast.py"
      },
      {
        "type": "file",
        "size": 42.429,
        "file_type": "py",
        "inclusive_size": 42.429,
        "id": "src\\transformers\\models\\depth_pro\\modeling_depth_pro.py"
      },
      {
        "type": "file",
        "size": 1.096,
        "file_type": "py",
        "inclusive_size": 1.096,
        "id": "src\\transformers\\models\\depth_pro\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.402,
        "file_type": "py",
        "inclusive_size": 7.402,
        "id": "src\\transformers\\models\\depth_anything\\configuration_depth_anything.py"
      },
      {
        "type": "file",
        "size": 17.931,
        "file_type": "py",
        "inclusive_size": 17.931,
        "id": "src\\transformers\\models\\depth_anything\\convert_depth_anything_to_hf.py"
      },
      {
        "type": "file",
        "size": 9.944,
        "file_type": "py",
        "inclusive_size": 9.944,
        "id": "src\\transformers\\models\\depth_anything\\convert_distill_any_depth_to_hf.py"
      },
      {
        "type": "file",
        "size": 16.136,
        "file_type": "py",
        "inclusive_size": 16.136,
        "id": "src\\transformers\\models\\depth_anything\\modeling_depth_anything.py"
      },
      {
        "type": "file",
        "size": 1.009,
        "file_type": "py",
        "inclusive_size": 1.009,
        "id": "src\\transformers\\models\\depth_anything\\__init__.py"
      },
      {
        "type": "file",
        "size": 0.988,
        "file_type": "py",
        "inclusive_size": 0.988,
        "id": "src\\transformers\\models\\deprecated\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.624,
        "file_type": "py",
        "inclusive_size": 5.624,
        "id": "src\\transformers\\models\\deit\\configuration_deit.py"
      },
      {
        "type": "file",
        "size": 9.268,
        "file_type": "py",
        "inclusive_size": 9.268,
        "id": "src\\transformers\\models\\deit\\convert_deit_timm_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 14.925,
        "file_type": "py",
        "inclusive_size": 14.925,
        "id": "src\\transformers\\models\\deit\\image_processing_deit.py"
      },
      {
        "type": "file",
        "size": 1.384,
        "file_type": "py",
        "inclusive_size": 1.384,
        "id": "src\\transformers\\models\\deit\\image_processing_deit_fast.py"
      },
      {
        "type": "file",
        "size": 30.255,
        "file_type": "py",
        "inclusive_size": 30.255,
        "id": "src\\transformers\\models\\deit\\modeling_deit.py"
      },
      {
        "type": "file",
        "size": 1.119,
        "file_type": "py",
        "inclusive_size": 1.119,
        "id": "src\\transformers\\models\\deit\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.688,
        "file_type": "py",
        "inclusive_size": 14.688,
        "id": "src\\transformers\\models\\deformable_detr\\configuration_deformable_detr.py"
      },
      {
        "type": "file",
        "size": 9.493,
        "file_type": "py",
        "inclusive_size": 9.493,
        "id": "src\\transformers\\models\\deformable_detr\\convert_deformable_detr_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 67.106,
        "file_type": "py",
        "inclusive_size": 67.106,
        "id": "src\\transformers\\models\\deformable_detr\\image_processing_deformable_detr.py"
      },
      {
        "type": "file",
        "size": 28.524,
        "file_type": "py",
        "inclusive_size": 28.524,
        "id": "src\\transformers\\models\\deformable_detr\\image_processing_deformable_detr_fast.py"
      },
      {
        "type": "file",
        "size": 88.176,
        "file_type": "py",
        "inclusive_size": 88.176,
        "id": "src\\transformers\\models\\deformable_detr\\modeling_deformable_detr.py"
      },
      {
        "type": "file",
        "size": 3.707,
        "file_type": "py",
        "inclusive_size": 3.707,
        "id": "src\\transformers\\models\\deformable_detr\\modular_deformable_detr.py"
      },
      {
        "type": "file",
        "size": 1.176,
        "file_type": "py",
        "inclusive_size": 1.176,
        "id": "src\\transformers\\models\\deformable_detr\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.686,
        "file_type": "py",
        "inclusive_size": 5.686,
        "id": "src\\transformers\\models\\deepseek_vl_hybrid\\configuration_deepseek_vl_hybrid.py"
      },
      {
        "type": "file",
        "size": 16.56,
        "file_type": "py",
        "inclusive_size": 16.56,
        "id": "src\\transformers\\models\\deepseek_vl_hybrid\\convert_deepseek_vl_hybrid_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 28.071,
        "file_type": "py",
        "inclusive_size": 28.071,
        "id": "src\\transformers\\models\\deepseek_vl_hybrid\\image_processing_deepseek_vl_hybrid.py"
      },
      {
        "type": "file",
        "size": 12.788,
        "file_type": "py",
        "inclusive_size": 12.788,
        "id": "src\\transformers\\models\\deepseek_vl_hybrid\\image_processing_deepseek_vl_hybrid_fast.py"
      },
      {
        "type": "file",
        "size": 25.319,
        "file_type": "py",
        "inclusive_size": 25.319,
        "id": "src\\transformers\\models\\deepseek_vl_hybrid\\modeling_deepseek_vl_hybrid.py"
      },
      {
        "type": "file",
        "size": 49.618,
        "file_type": "py",
        "inclusive_size": 49.618,
        "id": "src\\transformers\\models\\deepseek_vl_hybrid\\modular_deepseek_vl_hybrid.py"
      },
      {
        "type": "file",
        "size": 5.552,
        "file_type": "py",
        "inclusive_size": 5.552,
        "id": "src\\transformers\\models\\deepseek_vl_hybrid\\processing_deepseek_vl_hybrid.py"
      },
      {
        "type": "file",
        "size": 1.257,
        "file_type": "py",
        "inclusive_size": 1.257,
        "id": "src\\transformers\\models\\deepseek_vl_hybrid\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.777,
        "file_type": "py",
        "inclusive_size": 4.777,
        "id": "src\\transformers\\models\\deepseek_vl\\configuration_deepseek_vl.py"
      },
      {
        "type": "file",
        "size": 13.008,
        "file_type": "py",
        "inclusive_size": 13.008,
        "id": "src\\transformers\\models\\deepseek_vl\\convert_deepseek_vl_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 21.55,
        "file_type": "py",
        "inclusive_size": 21.55,
        "id": "src\\transformers\\models\\deepseek_vl\\image_processing_deepseek_vl.py"
      },
      {
        "type": "file",
        "size": 7.703,
        "file_type": "py",
        "inclusive_size": 7.703,
        "id": "src\\transformers\\models\\deepseek_vl\\image_processing_deepseek_vl_fast.py"
      },
      {
        "type": "file",
        "size": 15.315,
        "file_type": "py",
        "inclusive_size": 15.315,
        "id": "src\\transformers\\models\\deepseek_vl\\modeling_deepseek_vl.py"
      },
      {
        "type": "file",
        "size": 10.922,
        "file_type": "py",
        "inclusive_size": 10.922,
        "id": "src\\transformers\\models\\deepseek_vl\\modular_deepseek_vl.py"
      },
      {
        "type": "file",
        "size": 5.389,
        "file_type": "py",
        "inclusive_size": 5.389,
        "id": "src\\transformers\\models\\deepseek_vl\\processing_deepseek_vl.py"
      },
      {
        "type": "file",
        "size": 1.162,
        "file_type": "py",
        "inclusive_size": 1.162,
        "id": "src\\transformers\\models\\deepseek_vl\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.957,
        "file_type": "py",
        "inclusive_size": 12.957,
        "id": "src\\transformers\\models\\deepseek_v3\\configuration_deepseek_v3.py"
      },
      {
        "type": "file",
        "size": 32.341,
        "file_type": "py",
        "inclusive_size": 32.341,
        "id": "src\\transformers\\models\\deepseek_v3\\modeling_deepseek_v3.py"
      },
      {
        "type": "file",
        "size": 14.186,
        "file_type": "py",
        "inclusive_size": 14.186,
        "id": "src\\transformers\\models\\deepseek_v3\\modular_deepseek_v3.py"
      },
      {
        "type": "file",
        "size": 1.008,
        "file_type": "py",
        "inclusive_size": 1.008,
        "id": "src\\transformers\\models\\deepseek_v3\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.087,
        "file_type": "py",
        "inclusive_size": 12.087,
        "id": "src\\transformers\\models\\deepseek_v2\\configuration_deepseek_v2.py"
      },
      {
        "type": "file",
        "size": 28.101,
        "file_type": "py",
        "inclusive_size": 28.101,
        "id": "src\\transformers\\models\\deepseek_v2\\modeling_deepseek_v2.py"
      },
      {
        "type": "file",
        "size": 21.847,
        "file_type": "py",
        "inclusive_size": 21.847,
        "id": "src\\transformers\\models\\deepseek_v2\\modular_deepseek_v2.py"
      },
      {
        "type": "file",
        "size": 1.005,
        "file_type": "py",
        "inclusive_size": 1.005,
        "id": "src\\transformers\\models\\deepseek_v2\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.05,
        "file_type": "py",
        "inclusive_size": 7.05,
        "id": "src\\transformers\\models\\decision_transformer\\configuration_decision_transformer.py"
      },
      {
        "type": "file",
        "size": 35.889,
        "file_type": "py",
        "inclusive_size": 35.889,
        "id": "src\\transformers\\models\\decision_transformer\\modeling_decision_transformer.py"
      },
      {
        "type": "file",
        "size": 1.021,
        "file_type": "py",
        "inclusive_size": 1.021,
        "id": "src\\transformers\\models\\decision_transformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.455,
        "file_type": "py",
        "inclusive_size": 7.455,
        "id": "src\\transformers\\models\\deberta_v2\\configuration_deberta_v2.py"
      },
      {
        "type": "file",
        "size": 55.656,
        "file_type": "py",
        "inclusive_size": 55.656,
        "id": "src\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py"
      },
      {
        "type": "file",
        "size": 6.634,
        "file_type": "py",
        "inclusive_size": 6.634,
        "id": "src\\transformers\\models\\deberta_v2\\tokenization_deberta_v2.py"
      },
      {
        "type": "file",
        "size": 1.044,
        "file_type": "py",
        "inclusive_size": 1.044,
        "id": "src\\transformers\\models\\deberta_v2\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.383,
        "file_type": "py",
        "inclusive_size": 7.383,
        "id": "src\\transformers\\models\\deberta\\configuration_deberta.py"
      },
      {
        "type": "file",
        "size": 47.708,
        "file_type": "py",
        "inclusive_size": 47.708,
        "id": "src\\transformers\\models\\deberta\\modeling_deberta.py"
      },
      {
        "type": "file",
        "size": 7.794,
        "file_type": "py",
        "inclusive_size": 7.794,
        "id": "src\\transformers\\models\\deberta\\tokenization_deberta.py"
      },
      {
        "type": "file",
        "size": 1.035,
        "file_type": "py",
        "inclusive_size": 1.035,
        "id": "src\\transformers\\models\\deberta\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.159,
        "file_type": "py",
        "inclusive_size": 10.159,
        "id": "src\\transformers\\models\\dbrx\\configuration_dbrx.py"
      },
      {
        "type": "file",
        "size": 32.856,
        "file_type": "py",
        "inclusive_size": 32.856,
        "id": "src\\transformers\\models\\dbrx\\modeling_dbrx.py"
      },
      {
        "type": "file",
        "size": 22.424,
        "file_type": "py",
        "inclusive_size": 22.424,
        "id": "src\\transformers\\models\\dbrx\\modular_dbrx.py"
      },
      {
        "type": "file",
        "size": 0.989,
        "file_type": "py",
        "inclusive_size": 0.989,
        "id": "src\\transformers\\models\\dbrx\\__init__.py"
      },
      {
        "type": "file",
        "size": 16.372,
        "file_type": "py",
        "inclusive_size": 16.372,
        "id": "src\\transformers\\models\\data2vec\\configuration_data2vec_audio.py"
      },
      {
        "type": "file",
        "size": 6.279,
        "file_type": "py",
        "inclusive_size": 6.279,
        "id": "src\\transformers\\models\\data2vec\\configuration_data2vec_text.py"
      },
      {
        "type": "file",
        "size": 8.656,
        "file_type": "py",
        "inclusive_size": 8.656,
        "id": "src\\transformers\\models\\data2vec\\configuration_data2vec_vision.py"
      },
      {
        "type": "file",
        "size": 10.868,
        "file_type": "py",
        "inclusive_size": 10.868,
        "id": "src\\transformers\\models\\data2vec\\convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 9.568,
        "file_type": "py",
        "inclusive_size": 9.568,
        "id": "src\\transformers\\models\\data2vec\\convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 15.227,
        "file_type": "py",
        "inclusive_size": 15.227,
        "id": "src\\transformers\\models\\data2vec\\convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 55.846,
        "file_type": "py",
        "inclusive_size": 55.846,
        "id": "src\\transformers\\models\\data2vec\\modeling_data2vec_audio.py"
      },
      {
        "type": "file",
        "size": 50.738,
        "file_type": "py",
        "inclusive_size": 50.738,
        "id": "src\\transformers\\models\\data2vec\\modeling_data2vec_text.py"
      },
      {
        "type": "file",
        "size": 53.828,
        "file_type": "py",
        "inclusive_size": 53.828,
        "id": "src\\transformers\\models\\data2vec\\modeling_data2vec_vision.py"
      },
      {
        "type": "file",
        "size": 9.673,
        "file_type": "py",
        "inclusive_size": 9.673,
        "id": "src\\transformers\\models\\data2vec\\modular_data2vec_audio.py"
      },
      {
        "type": "file",
        "size": 23.111,
        "file_type": "py",
        "inclusive_size": 23.111,
        "id": "src\\transformers\\models\\data2vec\\modular_data2vec_text.py"
      },
      {
        "type": "file",
        "size": 1.191,
        "file_type": "py",
        "inclusive_size": 1.191,
        "id": "src\\transformers\\models\\data2vec\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.566,
        "file_type": "py",
        "inclusive_size": 4.566,
        "id": "src\\transformers\\models\\dac\\configuration_dac.py"
      },
      {
        "type": "file",
        "size": 10.173,
        "file_type": "py",
        "inclusive_size": 10.173,
        "id": "src\\transformers\\models\\dac\\convert_dac_checkpoint.py"
      },
      {
        "type": "file",
        "size": 7.911,
        "file_type": "py",
        "inclusive_size": 7.911,
        "id": "src\\transformers\\models\\dac\\feature_extraction_dac.py"
      },
      {
        "type": "file",
        "size": 28.856,
        "file_type": "py",
        "inclusive_size": 28.856,
        "id": "src\\transformers\\models\\dac\\modeling_dac.py"
      },
      {
        "type": "file",
        "size": 1.029,
        "file_type": "py",
        "inclusive_size": 1.029,
        "id": "src\\transformers\\models\\dac\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.828,
        "file_type": "py",
        "inclusive_size": 13.828,
        "id": "src\\transformers\\models\\dab_detr\\configuration_dab_detr.py"
      },
      {
        "type": "file",
        "size": 12.31,
        "file_type": "py",
        "inclusive_size": 12.31,
        "id": "src\\transformers\\models\\dab_detr\\convert_dab_detr_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 74.396,
        "file_type": "py",
        "inclusive_size": 74.396,
        "id": "src\\transformers\\models\\dab_detr\\modeling_dab_detr.py"
      },
      {
        "type": "file",
        "size": 0.998,
        "file_type": "py",
        "inclusive_size": 0.998,
        "id": "src\\transformers\\models\\dab_detr\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.612,
        "file_type": "py",
        "inclusive_size": 9.612,
        "id": "src\\transformers\\models\\cwm\\configuration_cwm.py"
      },
      {
        "type": "file",
        "size": 22.219,
        "file_type": "py",
        "inclusive_size": 22.219,
        "id": "src\\transformers\\models\\cwm\\modeling_cwm.py"
      },
      {
        "type": "file",
        "size": 12.633,
        "file_type": "py",
        "inclusive_size": 12.633,
        "id": "src\\transformers\\models\\cwm\\modular_cwm.py"
      },
      {
        "type": "file",
        "size": 0.988,
        "file_type": "py",
        "inclusive_size": 0.988,
        "id": "src\\transformers\\models\\cwm\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.669,
        "file_type": "py",
        "inclusive_size": 6.669,
        "id": "src\\transformers\\models\\cvt\\configuration_cvt.py"
      },
      {
        "type": "file",
        "size": 13.521,
        "file_type": "py",
        "inclusive_size": 13.521,
        "id": "src\\transformers\\models\\cvt\\convert_cvt_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 24.148,
        "file_type": "py",
        "inclusive_size": 24.148,
        "id": "src\\transformers\\models\\cvt\\modeling_cvt.py"
      },
      {
        "type": "file",
        "size": 0.987,
        "file_type": "py",
        "inclusive_size": 0.987,
        "id": "src\\transformers\\models\\cvt\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.271,
        "file_type": "py",
        "inclusive_size": 5.271,
        "id": "src\\transformers\\models\\ctrl\\configuration_ctrl.py"
      },
      {
        "type": "file",
        "size": 27.314,
        "file_type": "py",
        "inclusive_size": 27.314,
        "id": "src\\transformers\\models\\ctrl\\modeling_ctrl.py"
      },
      {
        "type": "file",
        "size": 6.855,
        "file_type": "py",
        "inclusive_size": 6.855,
        "id": "src\\transformers\\models\\ctrl\\tokenization_ctrl.py"
      },
      {
        "type": "file",
        "size": 1.026,
        "file_type": "py",
        "inclusive_size": 1.026,
        "id": "src\\transformers\\models\\ctrl\\__init__.py"
      },
      {
        "type": "file",
        "size": 18.093,
        "file_type": "py",
        "inclusive_size": 18.093,
        "id": "src\\transformers\\models\\csm\\configuration_csm.py"
      },
      {
        "type": "file",
        "size": 13.481,
        "file_type": "py",
        "inclusive_size": 13.481,
        "id": "src\\transformers\\models\\csm\\convert_csm.py"
      },
      {
        "type": "file",
        "size": 25.722,
        "file_type": "py",
        "inclusive_size": 25.722,
        "id": "src\\transformers\\models\\csm\\generation_csm.py"
      },
      {
        "type": "file",
        "size": 51.807,
        "file_type": "py",
        "inclusive_size": 51.807,
        "id": "src\\transformers\\models\\csm\\modeling_csm.py"
      },
      {
        "type": "file",
        "size": 35.234,
        "file_type": "py",
        "inclusive_size": 35.234,
        "id": "src\\transformers\\models\\csm\\modular_csm.py"
      },
      {
        "type": "file",
        "size": 13.71,
        "file_type": "py",
        "inclusive_size": 13.71,
        "id": "src\\transformers\\models\\csm\\processing_csm.py"
      },
      {
        "type": "file",
        "size": 1.021,
        "file_type": "py",
        "inclusive_size": 1.021,
        "id": "src\\transformers\\models\\csm\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.334,
        "file_type": "py",
        "inclusive_size": 5.334,
        "id": "src\\transformers\\models\\cpmant\\configuration_cpmant.py"
      },
      {
        "type": "file",
        "size": 32.343,
        "file_type": "py",
        "inclusive_size": 32.343,
        "id": "src\\transformers\\models\\cpmant\\modeling_cpmant.py"
      },
      {
        "type": "file",
        "size": 7.993,
        "file_type": "py",
        "inclusive_size": 7.993,
        "id": "src\\transformers\\models\\cpmant\\tokenization_cpmant.py"
      },
      {
        "type": "file",
        "size": 1.032,
        "file_type": "py",
        "inclusive_size": 1.032,
        "id": "src\\transformers\\models\\cpmant\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.822,
        "file_type": "py",
        "inclusive_size": 13.822,
        "id": "src\\transformers\\models\\cpm\\tokenization_cpm.py"
      },
      {
        "type": "file",
        "size": 9.917,
        "file_type": "py",
        "inclusive_size": 9.917,
        "id": "src\\transformers\\models\\cpm\\tokenization_cpm_fast.py"
      },
      {
        "type": "file",
        "size": 0.954,
        "file_type": "py",
        "inclusive_size": 0.954,
        "id": "src\\transformers\\models\\cpm\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.549,
        "file_type": "py",
        "inclusive_size": 5.549,
        "id": "src\\transformers\\models\\convnextv2\\configuration_convnextv2.py"
      },
      {
        "type": "file",
        "size": 12.511,
        "file_type": "py",
        "inclusive_size": 12.511,
        "id": "src\\transformers\\models\\convnextv2\\convert_convnextv2_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 17.683,
        "file_type": "py",
        "inclusive_size": 17.683,
        "id": "src\\transformers\\models\\convnextv2\\modeling_convnextv2.py"
      },
      {
        "type": "file",
        "size": 1.001,
        "file_type": "py",
        "inclusive_size": 1.001,
        "id": "src\\transformers\\models\\convnextv2\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.616,
        "file_type": "py",
        "inclusive_size": 5.616,
        "id": "src\\transformers\\models\\convnext\\configuration_convnext.py"
      },
      {
        "type": "file",
        "size": 10.145,
        "file_type": "py",
        "inclusive_size": 10.145,
        "id": "src\\transformers\\models\\convnext\\convert_convnext_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 15.989,
        "file_type": "py",
        "inclusive_size": 15.989,
        "id": "src\\transformers\\models\\convnext\\image_processing_convnext.py"
      },
      {
        "type": "file",
        "size": 6.611,
        "file_type": "py",
        "inclusive_size": 6.611,
        "id": "src\\transformers\\models\\convnext\\image_processing_convnext_fast.py"
      },
      {
        "type": "file",
        "size": 16.201,
        "file_type": "py",
        "inclusive_size": 16.201,
        "id": "src\\transformers\\models\\convnext\\modeling_convnext.py"
      },
      {
        "type": "file",
        "size": 1.139,
        "file_type": "py",
        "inclusive_size": 1.139,
        "id": "src\\transformers\\models\\convnext\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.358,
        "file_type": "py",
        "inclusive_size": 6.358,
        "id": "src\\transformers\\models\\convbert\\configuration_convbert.py"
      },
      {
        "type": "file",
        "size": 8.146,
        "file_type": "py",
        "inclusive_size": 8.146,
        "id": "src\\transformers\\models\\convbert\\convert_convbert_original_tf1_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 48.94,
        "file_type": "py",
        "inclusive_size": 48.94,
        "id": "src\\transformers\\models\\convbert\\modeling_convbert.py"
      },
      {
        "type": "file",
        "size": 1.092,
        "file_type": "py",
        "inclusive_size": 1.092,
        "id": "src\\transformers\\models\\convbert\\tokenization_convbert.py"
      },
      {
        "type": "file",
        "size": 1.118,
        "file_type": "py",
        "inclusive_size": 1.118,
        "id": "src\\transformers\\models\\convbert\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.733,
        "file_type": "py",
        "inclusive_size": 12.733,
        "id": "src\\transformers\\models\\conditional_detr\\configuration_conditional_detr.py"
      },
      {
        "type": "file",
        "size": 15.963,
        "file_type": "py",
        "inclusive_size": 15.963,
        "id": "src\\transformers\\models\\conditional_detr\\convert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 79.605,
        "file_type": "py",
        "inclusive_size": 79.605,
        "id": "src\\transformers\\models\\conditional_detr\\image_processing_conditional_detr.py"
      },
      {
        "type": "file",
        "size": 40.6,
        "file_type": "py",
        "inclusive_size": 40.6,
        "id": "src\\transformers\\models\\conditional_detr\\image_processing_conditional_detr_fast.py"
      },
      {
        "type": "file",
        "size": 93.444,
        "file_type": "py",
        "inclusive_size": 93.444,
        "id": "src\\transformers\\models\\conditional_detr\\modeling_conditional_detr.py"
      },
      {
        "type": "file",
        "size": 5.79,
        "file_type": "py",
        "inclusive_size": 5.79,
        "id": "src\\transformers\\models\\conditional_detr\\modular_conditional_detr.py"
      },
      {
        "type": "file",
        "size": 1.179,
        "file_type": "py",
        "inclusive_size": 1.179,
        "id": "src\\transformers\\models\\conditional_detr\\__init__.py"
      },
      {
        "type": "file",
        "size": 3.791,
        "file_type": "py",
        "inclusive_size": 3.791,
        "id": "src\\transformers\\models\\colqwen2\\configuration_colqwen2.py"
      },
      {
        "type": "file",
        "size": 8.342,
        "file_type": "py",
        "inclusive_size": 8.342,
        "id": "src\\transformers\\models\\colqwen2\\convert_colqwen2_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 10.99,
        "file_type": "py",
        "inclusive_size": 10.99,
        "id": "src\\transformers\\models\\colqwen2\\modeling_colqwen2.py"
      },
      {
        "type": "file",
        "size": 16.076,
        "file_type": "py",
        "inclusive_size": 16.076,
        "id": "src\\transformers\\models\\colqwen2\\modular_colqwen2.py"
      },
      {
        "type": "file",
        "size": 16.763,
        "file_type": "py",
        "inclusive_size": 16.763,
        "id": "src\\transformers\\models\\colqwen2\\processing_colqwen2.py"
      },
      {
        "type": "file",
        "size": 1.036,
        "file_type": "py",
        "inclusive_size": 1.036,
        "id": "src\\transformers\\models\\colqwen2\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.285,
        "file_type": "py",
        "inclusive_size": 4.285,
        "id": "src\\transformers\\models\\colpali\\configuration_colpali.py"
      },
      {
        "type": "file",
        "size": 7.979,
        "file_type": "py",
        "inclusive_size": 7.979,
        "id": "src\\transformers\\models\\colpali\\convert_colpali_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 8.6,
        "file_type": "py",
        "inclusive_size": 8.6,
        "id": "src\\transformers\\models\\colpali\\modeling_colpali.py"
      },
      {
        "type": "file",
        "size": 12.861,
        "file_type": "py",
        "inclusive_size": 12.861,
        "id": "src\\transformers\\models\\colpali\\modular_colpali.py"
      },
      {
        "type": "file",
        "size": 16.492,
        "file_type": "py",
        "inclusive_size": 16.492,
        "id": "src\\transformers\\models\\colpali\\processing_colpali.py"
      },
      {
        "type": "file",
        "size": 1.033,
        "file_type": "py",
        "inclusive_size": 1.033,
        "id": "src\\transformers\\models\\colpali\\__init__.py"
      },
      {
        "type": "file",
        "size": 3.735,
        "file_type": "py",
        "inclusive_size": 3.735,
        "id": "src\\transformers\\models\\cohere2_vision\\configuration_cohere2_vision.py"
      },
      {
        "type": "file",
        "size": 13.569,
        "file_type": "py",
        "inclusive_size": 13.569,
        "id": "src\\transformers\\models\\cohere2_vision\\image_processing_cohere2_vision_fast.py"
      },
      {
        "type": "file",
        "size": 17.834,
        "file_type": "py",
        "inclusive_size": 17.834,
        "id": "src\\transformers\\models\\cohere2_vision\\modeling_cohere2_vision.py"
      },
      {
        "type": "file",
        "size": 14.388,
        "file_type": "py",
        "inclusive_size": 14.388,
        "id": "src\\transformers\\models\\cohere2_vision\\modular_cohere2_vision.py"
      },
      {
        "type": "file",
        "size": 7.239,
        "file_type": "py",
        "inclusive_size": 7.239,
        "id": "src\\transformers\\models\\cohere2_vision\\processing_cohere2_vision.py"
      },
      {
        "type": "file",
        "size": 1.11,
        "file_type": "py",
        "inclusive_size": 1.11,
        "id": "src\\transformers\\models\\cohere2_vision\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.022,
        "file_type": "py",
        "inclusive_size": 10.022,
        "id": "src\\transformers\\models\\cohere2\\configuration_cohere2.py"
      },
      {
        "type": "file",
        "size": 24.672,
        "file_type": "py",
        "inclusive_size": 24.672,
        "id": "src\\transformers\\models\\cohere2\\modeling_cohere2.py"
      },
      {
        "type": "file",
        "size": 18.574,
        "file_type": "py",
        "inclusive_size": 18.574,
        "id": "src\\transformers\\models\\cohere2\\modular_cohere2.py"
      },
      {
        "type": "file",
        "size": 1.011,
        "file_type": "py",
        "inclusive_size": 1.011,
        "id": "src\\transformers\\models\\cohere2\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.545,
        "file_type": "py",
        "inclusive_size": 8.545,
        "id": "src\\transformers\\models\\cohere\\configuration_cohere.py"
      },
      {
        "type": "file",
        "size": 25.292,
        "file_type": "py",
        "inclusive_size": 25.292,
        "id": "src\\transformers\\models\\cohere\\modeling_cohere.py"
      },
      {
        "type": "file",
        "size": 15.635,
        "file_type": "py",
        "inclusive_size": 15.635,
        "id": "src\\transformers\\models\\cohere\\modular_cohere.py"
      },
      {
        "type": "file",
        "size": 19.625,
        "file_type": "py",
        "inclusive_size": 19.625,
        "id": "src\\transformers\\models\\cohere\\tokenization_cohere.py"
      },
      {
        "type": "file",
        "size": 1.032,
        "file_type": "py",
        "inclusive_size": 1.032,
        "id": "src\\transformers\\models\\cohere\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.116,
        "file_type": "py",
        "inclusive_size": 15.116,
        "id": "src\\transformers\\models\\code_llama\\tokenization_code_llama.py"
      },
      {
        "type": "file",
        "size": 0.961,
        "file_type": "py",
        "inclusive_size": 0.961,
        "id": "src\\transformers\\models\\code_llama\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.215,
        "file_type": "py",
        "inclusive_size": 6.215,
        "id": "src\\transformers\\models\\codegen\\configuration_codegen.py"
      },
      {
        "type": "file",
        "size": 27.484,
        "file_type": "py",
        "inclusive_size": 27.484,
        "id": "src\\transformers\\models\\codegen\\modeling_codegen.py"
      },
      {
        "type": "file",
        "size": 8.512,
        "file_type": "py",
        "inclusive_size": 8.512,
        "id": "src\\transformers\\models\\codegen\\tokenization_codegen.py"
      },
      {
        "type": "file",
        "size": 1.114,
        "file_type": "py",
        "inclusive_size": 1.114,
        "id": "src\\transformers\\models\\codegen\\__init__.py"
      },
      {
        "type": "file",
        "size": 19.781,
        "file_type": "py",
        "inclusive_size": 19.781,
        "id": "src\\transformers\\models\\clvp\\configuration_clvp.py"
      },
      {
        "type": "file",
        "size": 9.349,
        "file_type": "py",
        "inclusive_size": 9.349,
        "id": "src\\transformers\\models\\clvp\\convert_clvp_to_hf.py"
      },
      {
        "type": "file",
        "size": 10.851,
        "file_type": "py",
        "inclusive_size": 10.851,
        "id": "src\\transformers\\models\\clvp\\feature_extraction_clvp.py"
      },
      {
        "type": "file",
        "size": 84.802,
        "file_type": "py",
        "inclusive_size": 84.802,
        "id": "src\\transformers\\models\\clvp\\modeling_clvp.py"
      },
      {
        "type": "file",
        "size": 8.918,
        "file_type": "py",
        "inclusive_size": 8.918,
        "id": "src\\transformers\\models\\clvp\\number_normalizer.py"
      },
      {
        "type": "file",
        "size": 1.345,
        "file_type": "py",
        "inclusive_size": 1.345,
        "id": "src\\transformers\\models\\clvp\\processing_clvp.py"
      },
      {
        "type": "file",
        "size": 10.181,
        "file_type": "py",
        "inclusive_size": 10.181,
        "id": "src\\transformers\\models\\clvp\\tokenization_clvp.py"
      },
      {
        "type": "file",
        "size": 1.104,
        "file_type": "py",
        "inclusive_size": 1.104,
        "id": "src\\transformers\\models\\clvp\\__init__.py"
      },
      {
        "type": "file",
        "size": 19.076,
        "file_type": "py",
        "inclusive_size": 19.076,
        "id": "src\\transformers\\models\\clipseg\\configuration_clipseg.py"
      },
      {
        "type": "file",
        "size": 11.164,
        "file_type": "py",
        "inclusive_size": 11.164,
        "id": "src\\transformers\\models\\clipseg\\convert_clipseg_original_pytorch_to_hf.py"
      },
      {
        "type": "file",
        "size": 57.823,
        "file_type": "py",
        "inclusive_size": 57.823,
        "id": "src\\transformers\\models\\clipseg\\modeling_clipseg.py"
      },
      {
        "type": "file",
        "size": 3.932,
        "file_type": "py",
        "inclusive_size": 3.932,
        "id": "src\\transformers\\models\\clipseg\\processing_clipseg.py"
      },
      {
        "type": "file",
        "size": 1.033,
        "file_type": "py",
        "inclusive_size": 1.033,
        "id": "src\\transformers\\models\\clipseg\\__init__.py"
      },
      {
        "type": "file",
        "size": 17.366,
        "file_type": "py",
        "inclusive_size": 17.366,
        "id": "src\\transformers\\models\\clip\\configuration_clip.py"
      },
      {
        "type": "file",
        "size": 5.554,
        "file_type": "py",
        "inclusive_size": 5.554,
        "id": "src\\transformers\\models\\clip\\convert_clip_original_pytorch_to_hf.py"
      },
      {
        "type": "file",
        "size": 16.611,
        "file_type": "py",
        "inclusive_size": 16.611,
        "id": "src\\transformers\\models\\clip\\image_processing_clip.py"
      },
      {
        "type": "file",
        "size": 1.805,
        "file_type": "py",
        "inclusive_size": 1.805,
        "id": "src\\transformers\\models\\clip\\image_processing_clip_fast.py"
      },
      {
        "type": "file",
        "size": 43.617,
        "file_type": "py",
        "inclusive_size": 43.617,
        "id": "src\\transformers\\models\\clip\\modeling_clip.py"
      },
      {
        "type": "file",
        "size": 0.928,
        "file_type": "py",
        "inclusive_size": 0.928,
        "id": "src\\transformers\\models\\clip\\processing_clip.py"
      },
      {
        "type": "file",
        "size": 5.202,
        "file_type": "py",
        "inclusive_size": 5.202,
        "id": "src\\transformers\\models\\clip\\tokenization_clip.py"
      },
      {
        "type": "file",
        "size": 1.191,
        "file_type": "py",
        "inclusive_size": 1.191,
        "id": "src\\transformers\\models\\clip\\__init__.py"
      },
      {
        "type": "file",
        "size": 17.366,
        "file_type": "py",
        "inclusive_size": 17.366,
        "id": "src\\transformers\\models\\clap\\configuration_clap.py"
      },
      {
        "type": "file",
        "size": 5.136,
        "file_type": "py",
        "inclusive_size": 5.136,
        "id": "src\\transformers\\models\\clap\\convert_clap_original_pytorch_to_hf.py"
      },
      {
        "type": "file",
        "size": 18.699,
        "file_type": "py",
        "inclusive_size": 18.699,
        "id": "src\\transformers\\models\\clap\\feature_extraction_clap.py"
      },
      {
        "type": "file",
        "size": 79.391,
        "file_type": "py",
        "inclusive_size": 79.391,
        "id": "src\\transformers\\models\\clap\\modeling_clap.py"
      },
      {
        "type": "file",
        "size": 0.961,
        "file_type": "py",
        "inclusive_size": 0.961,
        "id": "src\\transformers\\models\\clap\\processing_clap.py"
      },
      {
        "type": "file",
        "size": 1.067,
        "file_type": "py",
        "inclusive_size": 1.067,
        "id": "src\\transformers\\models\\clap\\__init__.py"
      },
      {
        "type": "file",
        "size": 18.012,
        "file_type": "py",
        "inclusive_size": 18.012,
        "id": "src\\transformers\\models\\chinese_clip\\configuration_chinese_clip.py"
      },
      {
        "type": "file",
        "size": 5.053,
        "file_type": "py",
        "inclusive_size": 5.053,
        "id": "src\\transformers\\models\\chinese_clip\\convert_chinese_clip_original_pytorch_to_hf.py"
      },
      {
        "type": "file",
        "size": 15.15,
        "file_type": "py",
        "inclusive_size": 15.15,
        "id": "src\\transformers\\models\\chinese_clip\\image_processing_chinese_clip.py"
      },
      {
        "type": "file",
        "size": 1.332,
        "file_type": "py",
        "inclusive_size": 1.332,
        "id": "src\\transformers\\models\\chinese_clip\\image_processing_chinese_clip_fast.py"
      },
      {
        "type": "file",
        "size": 49.924,
        "file_type": "py",
        "inclusive_size": 49.924,
        "id": "src\\transformers\\models\\chinese_clip\\modeling_chinese_clip.py"
      },
      {
        "type": "file",
        "size": 0.995,
        "file_type": "py",
        "inclusive_size": 0.995,
        "id": "src\\transformers\\models\\chinese_clip\\processing_chinese_clip.py"
      },
      {
        "type": "file",
        "size": 1.202,
        "file_type": "py",
        "inclusive_size": 1.202,
        "id": "src\\transformers\\models\\chinese_clip\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.1,
        "file_type": "py",
        "inclusive_size": 12.1,
        "id": "src\\transformers\\models\\chameleon\\configuration_chameleon.py"
      },
      {
        "type": "file",
        "size": 20.38,
        "file_type": "py",
        "inclusive_size": 20.38,
        "id": "src\\transformers\\models\\chameleon\\convert_chameleon_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 16.519,
        "file_type": "py",
        "inclusive_size": 16.519,
        "id": "src\\transformers\\models\\chameleon\\image_processing_chameleon.py"
      },
      {
        "type": "file",
        "size": 4.032,
        "file_type": "py",
        "inclusive_size": 4.032,
        "id": "src\\transformers\\models\\chameleon\\image_processing_chameleon_fast.py"
      },
      {
        "type": "file",
        "size": 52.26,
        "file_type": "py",
        "inclusive_size": 52.26,
        "id": "src\\transformers\\models\\chameleon\\modeling_chameleon.py"
      },
      {
        "type": "file",
        "size": 7.136,
        "file_type": "py",
        "inclusive_size": 7.136,
        "id": "src\\transformers\\models\\chameleon\\processing_chameleon.py"
      },
      {
        "type": "file",
        "size": 1.136,
        "file_type": "py",
        "inclusive_size": 1.136,
        "id": "src\\transformers\\models\\chameleon\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.611,
        "file_type": "py",
        "inclusive_size": 6.611,
        "id": "src\\transformers\\models\\canine\\configuration_canine.py"
      },
      {
        "type": "file",
        "size": 6.111,
        "file_type": "py",
        "inclusive_size": 6.111,
        "id": "src\\transformers\\models\\canine\\convert_canine_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 58.72,
        "file_type": "py",
        "inclusive_size": 58.72,
        "id": "src\\transformers\\models\\canine\\modeling_canine.py"
      },
      {
        "type": "file",
        "size": 5.932,
        "file_type": "py",
        "inclusive_size": 5.932,
        "id": "src\\transformers\\models\\canine\\tokenization_canine.py"
      },
      {
        "type": "file",
        "size": 1.032,
        "file_type": "py",
        "inclusive_size": 1.032,
        "id": "src\\transformers\\models\\canine\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.19,
        "file_type": "py",
        "inclusive_size": 6.19,
        "id": "src\\transformers\\models\\camembert\\configuration_camembert.py"
      },
      {
        "type": "file",
        "size": 54.828,
        "file_type": "py",
        "inclusive_size": 54.828,
        "id": "src\\transformers\\models\\camembert\\modeling_camembert.py"
      },
      {
        "type": "file",
        "size": 22.686,
        "file_type": "py",
        "inclusive_size": 22.686,
        "id": "src\\transformers\\models\\camembert\\modular_camembert.py"
      },
      {
        "type": "file",
        "size": 7.321,
        "file_type": "py",
        "inclusive_size": 7.321,
        "id": "src\\transformers\\models\\camembert\\tokenization_camembert.py"
      },
      {
        "type": "file",
        "size": 1.041,
        "file_type": "py",
        "inclusive_size": 1.041,
        "id": "src\\transformers\\models\\camembert\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.681,
        "file_type": "py",
        "inclusive_size": 6.681,
        "id": "src\\transformers\\models\\byt5\\convert_byt5_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 9.992,
        "file_type": "py",
        "inclusive_size": 9.992,
        "id": "src\\transformers\\models\\byt5\\tokenization_byt5.py"
      },
      {
        "type": "file",
        "size": 0.955,
        "file_type": "py",
        "inclusive_size": 0.955,
        "id": "src\\transformers\\models\\byt5\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.872,
        "file_type": "py",
        "inclusive_size": 6.872,
        "id": "src\\transformers\\models\\bros\\configuration_bros.py"
      },
      {
        "type": "file",
        "size": 4.857,
        "file_type": "py",
        "inclusive_size": 4.857,
        "id": "src\\transformers\\models\\bros\\convert_bros_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 44.196,
        "file_type": "py",
        "inclusive_size": 44.196,
        "id": "src\\transformers\\models\\bros\\modeling_bros.py"
      },
      {
        "type": "file",
        "size": 1.467,
        "file_type": "py",
        "inclusive_size": 1.467,
        "id": "src\\transformers\\models\\bros\\processing_bros.py"
      },
      {
        "type": "file",
        "size": 1.024,
        "file_type": "py",
        "inclusive_size": 1.024,
        "id": "src\\transformers\\models\\bros\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.058,
        "file_type": "py",
        "inclusive_size": 14.058,
        "id": "src\\transformers\\models\\bridgetower\\configuration_bridgetower.py"
      },
      {
        "type": "file",
        "size": 25.812,
        "file_type": "py",
        "inclusive_size": 25.812,
        "id": "src\\transformers\\models\\bridgetower\\image_processing_bridgetower.py"
      },
      {
        "type": "file",
        "size": 9.583,
        "file_type": "py",
        "inclusive_size": 9.583,
        "id": "src\\transformers\\models\\bridgetower\\image_processing_bridgetower_fast.py"
      },
      {
        "type": "file",
        "size": 81.884,
        "file_type": "py",
        "inclusive_size": 81.884,
        "id": "src\\transformers\\models\\bridgetower\\modeling_bridgetower.py"
      },
      {
        "type": "file",
        "size": 1.617,
        "file_type": "py",
        "inclusive_size": 1.617,
        "id": "src\\transformers\\models\\bridgetower\\processing_bridgetower.py"
      },
      {
        "type": "file",
        "size": 1.146,
        "file_type": "py",
        "inclusive_size": 1.146,
        "id": "src\\transformers\\models\\bridgetower\\__init__.py"
      },
      {
        "type": "file",
        "size": 19.735,
        "file_type": "py",
        "inclusive_size": 19.735,
        "id": "src\\transformers\\models\\blt\\configuration_blt.py"
      },
      {
        "type": "file",
        "size": 18.113,
        "file_type": "py",
        "inclusive_size": 18.113,
        "id": "src\\transformers\\models\\blt\\convert_blt_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 64.248,
        "file_type": "py",
        "inclusive_size": 64.248,
        "id": "src\\transformers\\models\\blt\\modeling_blt.py"
      },
      {
        "type": "file",
        "size": 47.738,
        "file_type": "py",
        "inclusive_size": 47.738,
        "id": "src\\transformers\\models\\blt\\modular_blt.py"
      },
      {
        "type": "file",
        "size": 1.023,
        "file_type": "py",
        "inclusive_size": 1.023,
        "id": "src\\transformers\\models\\blt\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.436,
        "file_type": "py",
        "inclusive_size": 6.436,
        "id": "src\\transformers\\models\\bloom\\configuration_bloom.py"
      },
      {
        "type": "file",
        "size": 10.241,
        "file_type": "py",
        "inclusive_size": 10.241,
        "id": "src\\transformers\\models\\bloom\\convert_bloom_original_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 48.705,
        "file_type": "py",
        "inclusive_size": 48.705,
        "id": "src\\transformers\\models\\bloom\\modeling_bloom.py"
      },
      {
        "type": "file",
        "size": 1.029,
        "file_type": "py",
        "inclusive_size": 1.029,
        "id": "src\\transformers\\models\\bloom\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.65,
        "file_type": "py",
        "inclusive_size": 14.65,
        "id": "src\\transformers\\models\\blip_2\\configuration_blip_2.py"
      },
      {
        "type": "file",
        "size": 16.827,
        "file_type": "py",
        "inclusive_size": 16.827,
        "id": "src\\transformers\\models\\blip_2\\convert_blip_2_original_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 87.718,
        "file_type": "py",
        "inclusive_size": 87.718,
        "id": "src\\transformers\\models\\blip_2\\modeling_blip_2.py"
      },
      {
        "type": "file",
        "size": 4.813,
        "file_type": "py",
        "inclusive_size": 4.813,
        "id": "src\\transformers\\models\\blip_2\\processing_blip_2.py"
      },
      {
        "type": "file",
        "size": 1.03,
        "file_type": "py",
        "inclusive_size": 1.03,
        "id": "src\\transformers\\models\\blip_2\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.73,
        "file_type": "py",
        "inclusive_size": 14.73,
        "id": "src\\transformers\\models\\blip\\configuration_blip.py"
      },
      {
        "type": "file",
        "size": 7.008,
        "file_type": "py",
        "inclusive_size": 7.008,
        "id": "src\\transformers\\models\\blip\\convert_blip_original_pytorch_to_hf.py"
      },
      {
        "type": "file",
        "size": 14.916,
        "file_type": "py",
        "inclusive_size": 14.916,
        "id": "src\\transformers\\models\\blip\\image_processing_blip.py"
      },
      {
        "type": "file",
        "size": 1.297,
        "file_type": "py",
        "inclusive_size": 1.297,
        "id": "src\\transformers\\models\\blip\\image_processing_blip_fast.py"
      },
      {
        "type": "file",
        "size": 51.369,
        "file_type": "py",
        "inclusive_size": 51.369,
        "id": "src\\transformers\\models\\blip\\modeling_blip.py"
      },
      {
        "type": "file",
        "size": 39.467,
        "file_type": "py",
        "inclusive_size": 39.467,
        "id": "src\\transformers\\models\\blip\\modeling_blip_text.py"
      },
      {
        "type": "file",
        "size": 3.061,
        "file_type": "py",
        "inclusive_size": 3.061,
        "id": "src\\transformers\\models\\blip\\processing_blip.py"
      },
      {
        "type": "file",
        "size": 1.149,
        "file_type": "py",
        "inclusive_size": 1.149,
        "id": "src\\transformers\\models\\blip\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.678,
        "file_type": "py",
        "inclusive_size": 7.678,
        "id": "src\\transformers\\models\\blenderbot_small\\configuration_blenderbot_small.py"
      },
      {
        "type": "file",
        "size": 55.511,
        "file_type": "py",
        "inclusive_size": 55.511,
        "id": "src\\transformers\\models\\blenderbot_small\\modeling_blenderbot_small.py"
      },
      {
        "type": "file",
        "size": 7.944,
        "file_type": "py",
        "inclusive_size": 7.944,
        "id": "src\\transformers\\models\\blenderbot_small\\tokenization_blenderbot_small.py"
      },
      {
        "type": "file",
        "size": 1.116,
        "file_type": "py",
        "inclusive_size": 1.116,
        "id": "src\\transformers\\models\\blenderbot_small\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.696,
        "file_type": "py",
        "inclusive_size": 7.696,
        "id": "src\\transformers\\models\\blenderbot\\configuration_blenderbot.py"
      },
      {
        "type": "file",
        "size": 3.706,
        "file_type": "py",
        "inclusive_size": 3.706,
        "id": "src\\transformers\\models\\blenderbot\\convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 55.581,
        "file_type": "py",
        "inclusive_size": 55.581,
        "id": "src\\transformers\\models\\blenderbot\\modeling_blenderbot.py"
      },
      {
        "type": "file",
        "size": 7.161,
        "file_type": "py",
        "inclusive_size": 7.161,
        "id": "src\\transformers\\models\\blenderbot\\tokenization_blenderbot.py"
      },
      {
        "type": "file",
        "size": 1.044,
        "file_type": "py",
        "inclusive_size": 1.044,
        "id": "src\\transformers\\models\\blenderbot\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.226,
        "file_type": "py",
        "inclusive_size": 7.226,
        "id": "src\\transformers\\models\\bitnet\\configuration_bitnet.py"
      },
      {
        "type": "file",
        "size": 22.46,
        "file_type": "py",
        "inclusive_size": 22.46,
        "id": "src\\transformers\\models\\bitnet\\modeling_bitnet.py"
      },
      {
        "type": "file",
        "size": 5.672,
        "file_type": "py",
        "inclusive_size": 5.672,
        "id": "src\\transformers\\models\\bitnet\\modular_bitnet.py"
      },
      {
        "type": "file",
        "size": 1.018,
        "file_type": "py",
        "inclusive_size": 1.018,
        "id": "src\\transformers\\models\\bitnet\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.28,
        "file_type": "py",
        "inclusive_size": 6.28,
        "id": "src\\transformers\\models\\bit\\configuration_bit.py"
      },
      {
        "type": "file",
        "size": 5.999,
        "file_type": "py",
        "inclusive_size": 5.999,
        "id": "src\\transformers\\models\\bit\\convert_bit_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 15.514,
        "file_type": "py",
        "inclusive_size": 15.514,
        "id": "src\\transformers\\models\\bit\\image_processing_bit.py"
      },
      {
        "type": "file",
        "size": 1.312,
        "file_type": "py",
        "inclusive_size": 1.312,
        "id": "src\\transformers\\models\\bit\\image_processing_bit_fast.py"
      },
      {
        "type": "file",
        "size": 28.356,
        "file_type": "py",
        "inclusive_size": 28.356,
        "id": "src\\transformers\\models\\bit\\modeling_bit.py"
      },
      {
        "type": "file",
        "size": 1.072,
        "file_type": "py",
        "inclusive_size": 1.072,
        "id": "src\\transformers\\models\\bit\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.446,
        "file_type": "py",
        "inclusive_size": 6.446,
        "id": "src\\transformers\\models\\biogpt\\configuration_biogpt.py"
      },
      {
        "type": "file",
        "size": 10.569,
        "file_type": "py",
        "inclusive_size": 10.569,
        "id": "src\\transformers\\models\\biogpt\\convert_biogpt_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 34.016,
        "file_type": "py",
        "inclusive_size": 34.016,
        "id": "src\\transformers\\models\\biogpt\\modeling_biogpt.py"
      },
      {
        "type": "file",
        "size": 25.502,
        "file_type": "py",
        "inclusive_size": 25.502,
        "id": "src\\transformers\\models\\biogpt\\modular_biogpt.py"
      },
      {
        "type": "file",
        "size": 12.106,
        "file_type": "py",
        "inclusive_size": 12.106,
        "id": "src\\transformers\\models\\biogpt\\tokenization_biogpt.py"
      },
      {
        "type": "file",
        "size": 1.032,
        "file_type": "py",
        "inclusive_size": 1.032,
        "id": "src\\transformers\\models\\biogpt\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.528,
        "file_type": "py",
        "inclusive_size": 7.528,
        "id": "src\\transformers\\models\\big_bird\\configuration_big_bird.py"
      },
      {
        "type": "file",
        "size": 9.734,
        "file_type": "py",
        "inclusive_size": 9.734,
        "id": "src\\transformers\\models\\big_bird\\convert_bigbird_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 119.417,
        "file_type": "py",
        "inclusive_size": 119.417,
        "id": "src\\transformers\\models\\big_bird\\modeling_big_bird.py"
      },
      {
        "type": "file",
        "size": 7.521,
        "file_type": "py",
        "inclusive_size": 7.521,
        "id": "src\\transformers\\models\\big_bird\\tokenization_big_bird.py"
      },
      {
        "type": "file",
        "size": 1.038,
        "file_type": "py",
        "inclusive_size": 1.038,
        "id": "src\\transformers\\models\\big_bird\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.7,
        "file_type": "py",
        "inclusive_size": 8.7,
        "id": "src\\transformers\\models\\bigbird_pegasus\\configuration_bigbird_pegasus.py"
      },
      {
        "type": "file",
        "size": 6.211,
        "file_type": "py",
        "inclusive_size": 6.211,
        "id": "src\\transformers\\models\\bigbird_pegasus\\convert_bigbird_pegasus_tf_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 125.552,
        "file_type": "py",
        "inclusive_size": 125.552,
        "id": "src\\transformers\\models\\bigbird_pegasus\\modeling_bigbird_pegasus.py"
      },
      {
        "type": "file",
        "size": 1.011,
        "file_type": "py",
        "inclusive_size": 1.011,
        "id": "src\\transformers\\models\\bigbird_pegasus\\__init__.py"
      },
      {
        "type": "file",
        "size": 35.322,
        "file_type": "py",
        "inclusive_size": 35.322,
        "id": "src\\transformers\\models\\bert_japanese\\tokenization_bert_japanese.py"
      },
      {
        "type": "file",
        "size": 0.964,
        "file_type": "py",
        "inclusive_size": 0.964,
        "id": "src\\transformers\\models\\bert_japanese\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.384,
        "file_type": "py",
        "inclusive_size": 6.384,
        "id": "src\\transformers\\models\\bert_generation\\configuration_bert_generation.py"
      },
      {
        "type": "file",
        "size": 30.689,
        "file_type": "py",
        "inclusive_size": 30.689,
        "id": "src\\transformers\\models\\bert_generation\\modeling_bert_generation.py"
      },
      {
        "type": "file",
        "size": 4.342,
        "file_type": "py",
        "inclusive_size": 4.342,
        "id": "src\\transformers\\models\\bert_generation\\tokenization_bert_generation.py"
      },
      {
        "type": "file",
        "size": 1.059,
        "file_type": "py",
        "inclusive_size": 1.059,
        "id": "src\\transformers\\models\\bert_generation\\__init__.py"
      },
      {
        "type": "file",
        "size": 24.387,
        "file_type": "py",
        "inclusive_size": 24.387,
        "id": "src\\transformers\\models\\bertweet\\tokenization_bertweet.py"
      },
      {
        "type": "file",
        "size": 0.959,
        "file_type": "py",
        "inclusive_size": 0.959,
        "id": "src\\transformers\\models\\bertweet\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.247,
        "file_type": "py",
        "inclusive_size": 6.247,
        "id": "src\\transformers\\models\\bert\\configuration_bert.py"
      },
      {
        "type": "file",
        "size": 10.491,
        "file_type": "py",
        "inclusive_size": 10.491,
        "id": "src\\transformers\\models\\bert\\convert_bert_original_tf2_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 2.143,
        "file_type": "py",
        "inclusive_size": 2.143,
        "id": "src\\transformers\\models\\bert\\convert_bert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 7.61,
        "file_type": "py",
        "inclusive_size": 7.61,
        "id": "src\\transformers\\models\\bert\\convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 57.131,
        "file_type": "py",
        "inclusive_size": 57.131,
        "id": "src\\transformers\\models\\bert\\modeling_bert.py"
      },
      {
        "type": "file",
        "size": 6.077,
        "file_type": "py",
        "inclusive_size": 6.077,
        "id": "src\\transformers\\models\\bert\\tokenization_bert.py"
      },
      {
        "type": "file",
        "size": 19.727,
        "file_type": "py",
        "inclusive_size": 19.727,
        "id": "src\\transformers\\models\\bert\\tokenization_bert_legacy.py"
      },
      {
        "type": "file",
        "size": 1.026,
        "file_type": "py",
        "inclusive_size": 1.026,
        "id": "src\\transformers\\models\\bert\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.591,
        "file_type": "py",
        "inclusive_size": 10.591,
        "id": "src\\transformers\\models\\beit\\configuration_beit.py"
      },
      {
        "type": "file",
        "size": 16.629,
        "file_type": "py",
        "inclusive_size": 16.629,
        "id": "src\\transformers\\models\\beit\\convert_beit_unilm_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 23.997,
        "file_type": "py",
        "inclusive_size": 23.997,
        "id": "src\\transformers\\models\\beit\\image_processing_beit.py"
      },
      {
        "type": "file",
        "size": 8.257,
        "file_type": "py",
        "inclusive_size": 8.257,
        "id": "src\\transformers\\models\\beit\\image_processing_beit_fast.py"
      },
      {
        "type": "file",
        "size": 59.799,
        "file_type": "py",
        "inclusive_size": 59.799,
        "id": "src\\transformers\\models\\beit\\modeling_beit.py"
      },
      {
        "type": "file",
        "size": 1.119,
        "file_type": "py",
        "inclusive_size": 1.119,
        "id": "src\\transformers\\models\\beit\\__init__.py"
      },
      {
        "type": "file",
        "size": 14.255,
        "file_type": "py",
        "inclusive_size": 14.255,
        "id": "src\\transformers\\models\\bartpho\\tokenization_bartpho.py"
      },
      {
        "type": "file",
        "size": 0.958,
        "file_type": "py",
        "inclusive_size": 0.958,
        "id": "src\\transformers\\models\\bartpho\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.111,
        "file_type": "py",
        "inclusive_size": 6.111,
        "id": "src\\transformers\\models\\barthez\\tokenization_barthez.py"
      },
      {
        "type": "file",
        "size": 0.958,
        "file_type": "py",
        "inclusive_size": 0.958,
        "id": "src\\transformers\\models\\barthez\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.621,
        "file_type": "py",
        "inclusive_size": 7.621,
        "id": "src\\transformers\\models\\bart\\configuration_bart.py"
      },
      {
        "type": "file",
        "size": 6.058,
        "file_type": "py",
        "inclusive_size": 6.058,
        "id": "src\\transformers\\models\\bart\\convert_bart_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 71.349,
        "file_type": "py",
        "inclusive_size": 71.349,
        "id": "src\\transformers\\models\\bart\\modeling_bart.py"
      },
      {
        "type": "file",
        "size": 0.827,
        "file_type": "py",
        "inclusive_size": 0.827,
        "id": "src\\transformers\\models\\bart\\tokenization_bart.py"
      },
      {
        "type": "file",
        "size": 1.07,
        "file_type": "py",
        "inclusive_size": 1.07,
        "id": "src\\transformers\\models\\bart\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.627,
        "file_type": "py",
        "inclusive_size": 11.627,
        "id": "src\\transformers\\models\\bark\\configuration_bark.py"
      },
      {
        "type": "file",
        "size": 9.32,
        "file_type": "py",
        "inclusive_size": 9.32,
        "id": "src\\transformers\\models\\bark\\convert_suno_to_hf.py"
      },
      {
        "type": "file",
        "size": 14.89,
        "file_type": "py",
        "inclusive_size": 14.89,
        "id": "src\\transformers\\models\\bark\\generation_configuration_bark.py"
      },
      {
        "type": "file",
        "size": 69.624,
        "file_type": "py",
        "inclusive_size": 69.624,
        "id": "src\\transformers\\models\\bark\\modeling_bark.py"
      },
      {
        "type": "file",
        "size": 14.097,
        "file_type": "py",
        "inclusive_size": 14.097,
        "id": "src\\transformers\\models\\bark\\processing_bark.py"
      },
      {
        "type": "file",
        "size": 1.024,
        "file_type": "py",
        "inclusive_size": 1.024,
        "id": "src\\transformers\\models\\bark\\__init__.py"
      },
      {
        "type": "file",
        "size": 11.683,
        "file_type": "py",
        "inclusive_size": 11.683,
        "id": "src\\transformers\\models\\bamba\\configuration_bamba.py"
      },
      {
        "type": "file",
        "size": 9.712,
        "file_type": "py",
        "inclusive_size": 9.712,
        "id": "src\\transformers\\models\\bamba\\convert_mamba_ssm_checkpoint.py"
      },
      {
        "type": "file",
        "size": 69.637,
        "file_type": "py",
        "inclusive_size": 69.637,
        "id": "src\\transformers\\models\\bamba\\modeling_bamba.py"
      },
      {
        "type": "file",
        "size": 53.14,
        "file_type": "py",
        "inclusive_size": 53.14,
        "id": "src\\transformers\\models\\bamba\\modular_bamba.py"
      },
      {
        "type": "file",
        "size": 1.04,
        "file_type": "py",
        "inclusive_size": 1.04,
        "id": "src\\transformers\\models\\bamba\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.98,
        "file_type": "py",
        "inclusive_size": 4.98,
        "id": "src\\transformers\\models\\aya_vision\\configuration_aya_vision.py"
      },
      {
        "type": "file",
        "size": 20.585,
        "file_type": "py",
        "inclusive_size": 20.585,
        "id": "src\\transformers\\models\\aya_vision\\modeling_aya_vision.py"
      },
      {
        "type": "file",
        "size": 11.361,
        "file_type": "py",
        "inclusive_size": 11.361,
        "id": "src\\transformers\\models\\aya_vision\\modular_aya_vision.py"
      },
      {
        "type": "file",
        "size": 9.272,
        "file_type": "py",
        "inclusive_size": 9.272,
        "id": "src\\transformers\\models\\aya_vision\\processing_aya_vision.py"
      },
      {
        "type": "file",
        "size": 1.042,
        "file_type": "py",
        "inclusive_size": 1.042,
        "id": "src\\transformers\\models\\aya_vision\\__init__.py"
      },
      {
        "type": "file",
        "size": 12.136,
        "file_type": "py",
        "inclusive_size": 12.136,
        "id": "src\\transformers\\models\\autoformer\\configuration_autoformer.py"
      },
      {
        "type": "file",
        "size": 100.032,
        "file_type": "py",
        "inclusive_size": 100.032,
        "id": "src\\transformers\\models\\autoformer\\modeling_autoformer.py"
      },
      {
        "type": "file",
        "size": 1.001,
        "file_type": "py",
        "inclusive_size": 1.001,
        "id": "src\\transformers\\models\\autoformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 31.425,
        "file_type": "py",
        "inclusive_size": 31.425,
        "id": "src\\transformers\\models\\auto\\auto_factory.py"
      },
      {
        "type": "file",
        "size": 57.955,
        "file_type": "py",
        "inclusive_size": 57.955,
        "id": "src\\transformers\\models\\auto\\configuration_auto.py"
      },
      {
        "type": "file",
        "size": 18.858,
        "file_type": "py",
        "inclusive_size": 18.858,
        "id": "src\\transformers\\models\\auto\\feature_extraction_auto.py"
      },
      {
        "type": "file",
        "size": 39.065,
        "file_type": "py",
        "inclusive_size": 39.065,
        "id": "src\\transformers\\models\\auto\\image_processing_auto.py"
      },
      {
        "type": "file",
        "size": 92.181,
        "file_type": "py",
        "inclusive_size": 92.181,
        "id": "src\\transformers\\models\\auto\\modeling_auto.py"
      },
      {
        "type": "file",
        "size": 21.118,
        "file_type": "py",
        "inclusive_size": 21.118,
        "id": "src\\transformers\\models\\auto\\processing_auto.py"
      },
      {
        "type": "file",
        "size": 42.248,
        "file_type": "py",
        "inclusive_size": 42.248,
        "id": "src\\transformers\\models\\auto\\tokenization_auto.py"
      },
      {
        "type": "file",
        "size": 20.36,
        "file_type": "py",
        "inclusive_size": 20.36,
        "id": "src\\transformers\\models\\auto\\video_processing_auto.py"
      },
      {
        "type": "file",
        "size": 1.218,
        "file_type": "py",
        "inclusive_size": 1.218,
        "id": "src\\transformers\\models\\auto\\__init__.py"
      },
      {
        "type": "file",
        "size": 5.886,
        "file_type": "py",
        "inclusive_size": 5.886,
        "id": "src\\transformers\\models\\audio_spectrogram_transformer\\configuration_audio_spectrogram_transformer.py"
      },
      {
        "type": "file",
        "size": 11.104,
        "file_type": "py",
        "inclusive_size": 11.104,
        "id": "src\\transformers\\models\\audio_spectrogram_transformer\\convert_audio_spectrogram_transformer_original_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 9.795,
        "file_type": "py",
        "inclusive_size": 9.795,
        "id": "src\\transformers\\models\\audio_spectrogram_transformer\\feature_extraction_audio_spectrogram_transformer.py"
      },
      {
        "type": "file",
        "size": 17.922,
        "file_type": "py",
        "inclusive_size": 17.922,
        "id": "src\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py"
      },
      {
        "type": "file",
        "size": 1.107,
        "file_type": "py",
        "inclusive_size": 1.107,
        "id": "src\\transformers\\models\\audio_spectrogram_transformer\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.983,
        "file_type": "py",
        "inclusive_size": 8.983,
        "id": "src\\transformers\\models\\audioflamingo3\\configuration_audioflamingo3.py"
      },
      {
        "type": "file",
        "size": 10.132,
        "file_type": "py",
        "inclusive_size": 10.132,
        "id": "src\\transformers\\models\\audioflamingo3\\convert_audioflamingo3_to_hf.py"
      },
      {
        "type": "file",
        "size": 26.494,
        "file_type": "py",
        "inclusive_size": 26.494,
        "id": "src\\transformers\\models\\audioflamingo3\\modeling_audioflamingo3.py"
      },
      {
        "type": "file",
        "size": 12.988,
        "file_type": "py",
        "inclusive_size": 12.988,
        "id": "src\\transformers\\models\\audioflamingo3\\modular_audioflamingo3.py"
      },
      {
        "type": "file",
        "size": 13.547,
        "file_type": "py",
        "inclusive_size": 13.547,
        "id": "src\\transformers\\models\\audioflamingo3\\processing_audioflamingo3.py"
      },
      {
        "type": "file",
        "size": 1.085,
        "file_type": "py",
        "inclusive_size": 1.085,
        "id": "src\\transformers\\models\\audioflamingo3\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.316,
        "file_type": "py",
        "inclusive_size": 13.316,
        "id": "src\\transformers\\models\\aria\\configuration_aria.py"
      },
      {
        "type": "file",
        "size": 5.755,
        "file_type": "py",
        "inclusive_size": 5.755,
        "id": "src\\transformers\\models\\aria\\convert_aria_weights_to_hf.py"
      },
      {
        "type": "file",
        "size": 24.588,
        "file_type": "py",
        "inclusive_size": 24.588,
        "id": "src\\transformers\\models\\aria\\image_processing_aria.py"
      },
      {
        "type": "file",
        "size": 51.265,
        "file_type": "py",
        "inclusive_size": 51.265,
        "id": "src\\transformers\\models\\aria\\modeling_aria.py"
      },
      {
        "type": "file",
        "size": 66.405,
        "file_type": "py",
        "inclusive_size": 66.405,
        "id": "src\\transformers\\models\\aria\\modular_aria.py"
      },
      {
        "type": "file",
        "size": 8.925,
        "file_type": "py",
        "inclusive_size": 8.925,
        "id": "src\\transformers\\models\\aria\\processing_aria.py"
      },
      {
        "type": "file",
        "size": 1.066,
        "file_type": "py",
        "inclusive_size": 1.066,
        "id": "src\\transformers\\models\\aria\\__init__.py"
      },
      {
        "type": "file",
        "size": 9.096,
        "file_type": "py",
        "inclusive_size": 9.096,
        "id": "src\\transformers\\models\\arcee\\configuration_arcee.py"
      },
      {
        "type": "file",
        "size": 22.294,
        "file_type": "py",
        "inclusive_size": 22.294,
        "id": "src\\transformers\\models\\arcee\\modeling_arcee.py"
      },
      {
        "type": "file",
        "size": 8.866,
        "file_type": "py",
        "inclusive_size": 8.866,
        "id": "src\\transformers\\models\\arcee\\modular_arcee.py"
      },
      {
        "type": "file",
        "size": 1.009,
        "file_type": "py",
        "inclusive_size": 1.009,
        "id": "src\\transformers\\models\\arcee\\__init__.py"
      },
      {
        "type": "file",
        "size": 8.76,
        "file_type": "py",
        "inclusive_size": 8.76,
        "id": "src\\transformers\\models\\apertus\\configuration_apertus.py"
      },
      {
        "type": "file",
        "size": 22.346,
        "file_type": "py",
        "inclusive_size": 22.346,
        "id": "src\\transformers\\models\\apertus\\modeling_apertus.py"
      },
      {
        "type": "file",
        "size": 14.481,
        "file_type": "py",
        "inclusive_size": 14.481,
        "id": "src\\transformers\\models\\apertus\\modular_apertus.py"
      },
      {
        "type": "file",
        "size": 1.256,
        "file_type": "py",
        "inclusive_size": 1.256,
        "id": "src\\transformers\\models\\apertus\\__init__.py"
      },
      {
        "type": "file",
        "size": 17.672,
        "file_type": "py",
        "inclusive_size": 17.672,
        "id": "src\\transformers\\models\\altclip\\configuration_altclip.py"
      },
      {
        "type": "file",
        "size": 55.87,
        "file_type": "py",
        "inclusive_size": 55.87,
        "id": "src\\transformers\\models\\altclip\\modeling_altclip.py"
      },
      {
        "type": "file",
        "size": 1.0,
        "file_type": "py",
        "inclusive_size": 1.0,
        "id": "src\\transformers\\models\\altclip\\processing_altclip.py"
      },
      {
        "type": "file",
        "size": 1.033,
        "file_type": "py",
        "inclusive_size": 1.033,
        "id": "src\\transformers\\models\\altclip\\__init__.py"
      },
      {
        "type": "file",
        "size": 15.449,
        "file_type": "py",
        "inclusive_size": 15.449,
        "id": "src\\transformers\\models\\align\\configuration_align.py"
      },
      {
        "type": "file",
        "size": 15.556,
        "file_type": "py",
        "inclusive_size": 15.556,
        "id": "src\\transformers\\models\\align\\convert_align_tf_to_hf.py"
      },
      {
        "type": "file",
        "size": 48.692,
        "file_type": "py",
        "inclusive_size": 48.692,
        "id": "src\\transformers\\models\\align\\modeling_align.py"
      },
      {
        "type": "file",
        "size": 1.237,
        "file_type": "py",
        "inclusive_size": 1.237,
        "id": "src\\transformers\\models\\align\\processing_align.py"
      },
      {
        "type": "file",
        "size": 1.027,
        "file_type": "py",
        "inclusive_size": 1.027,
        "id": "src\\transformers\\models\\align\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.845,
        "file_type": "py",
        "inclusive_size": 6.845,
        "id": "src\\transformers\\models\\albert\\configuration_albert.py"
      },
      {
        "type": "file",
        "size": 2.146,
        "file_type": "py",
        "inclusive_size": 2.146,
        "id": "src\\transformers\\models\\albert\\convert_albert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "type": "file",
        "size": 38.551,
        "file_type": "py",
        "inclusive_size": 38.551,
        "id": "src\\transformers\\models\\albert\\modeling_albert.py"
      },
      {
        "type": "file",
        "size": 7.737,
        "file_type": "py",
        "inclusive_size": 7.737,
        "id": "src\\transformers\\models\\albert\\tokenization_albert.py"
      },
      {
        "type": "file",
        "size": 1.032,
        "file_type": "py",
        "inclusive_size": 1.032,
        "id": "src\\transformers\\models\\albert\\__init__.py"
      },
      {
        "type": "file",
        "size": 13.52,
        "file_type": "py",
        "inclusive_size": 13.52,
        "id": "src\\transformers\\models\\aimv2\\configuration_aimv2.py"
      },
      {
        "type": "file",
        "size": 10.096,
        "file_type": "py",
        "inclusive_size": 10.096,
        "id": "src\\transformers\\models\\aimv2\\convert_aimv2_original_pytorch_to_hf.py"
      },
      {
        "type": "file",
        "size": 29.074,
        "file_type": "py",
        "inclusive_size": 29.074,
        "id": "src\\transformers\\models\\aimv2\\modeling_aimv2.py"
      },
      {
        "type": "file",
        "size": 28.136,
        "file_type": "py",
        "inclusive_size": 28.136,
        "id": "src\\transformers\\models\\aimv2\\modular_aimv2.py"
      },
      {
        "type": "file",
        "size": 0.991,
        "file_type": "py",
        "inclusive_size": 0.991,
        "id": "src\\transformers\\models\\aimv2\\__init__.py"
      },
      {
        "type": "file",
        "size": 10.837,
        "file_type": "py",
        "inclusive_size": 10.837,
        "id": "src\\transformers\\models\\afmoe\\configuration_afmoe.py"
      },
      {
        "type": "file",
        "size": 30.158,
        "file_type": "py",
        "inclusive_size": 30.158,
        "id": "src\\transformers\\models\\afmoe\\modeling_afmoe.py"
      },
      {
        "type": "file",
        "size": 18.113,
        "file_type": "py",
        "inclusive_size": 18.113,
        "id": "src\\transformers\\models\\afmoe\\modular_afmoe.py"
      },
      {
        "type": "file",
        "size": 1.009,
        "file_type": "py",
        "inclusive_size": 1.009,
        "id": "src\\transformers\\models\\afmoe\\__init__.py"
      },
      {
        "type": "file",
        "size": 7.321,
        "file_type": "py",
        "inclusive_size": 7.321,
        "id": "src\\transformers\\loss\\loss_deformable_detr.py"
      },
      {
        "type": "file",
        "size": 15.466,
        "file_type": "py",
        "inclusive_size": 15.466,
        "id": "src\\transformers\\loss\\loss_d_fine.py"
      },
      {
        "type": "file",
        "size": 24.347,
        "file_type": "py",
        "inclusive_size": 24.347,
        "id": "src\\transformers\\loss\\loss_for_object_detection.py"
      },
      {
        "type": "file",
        "size": 11.19,
        "file_type": "py",
        "inclusive_size": 11.19,
        "id": "src\\transformers\\loss\\loss_grounding_dino.py"
      },
      {
        "type": "file",
        "size": 15.459,
        "file_type": "py",
        "inclusive_size": 15.459,
        "id": "src\\transformers\\loss\\loss_lw_detr.py"
      },
      {
        "type": "file",
        "size": 21.956,
        "file_type": "py",
        "inclusive_size": 21.956,
        "id": "src\\transformers\\loss\\loss_rt_detr.py"
      },
      {
        "type": "file",
        "size": 7.061,
        "file_type": "py",
        "inclusive_size": 7.061,
        "id": "src\\transformers\\loss\\loss_utils.py"
      },
      {
        "type": "file",
        "size": 0.606,
        "file_type": "py",
        "inclusive_size": 0.606,
        "id": "src\\transformers\\loss\\__init__.py"
      },
      {
        "type": "file",
        "size": 38.995,
        "file_type": "py",
        "inclusive_size": 38.995,
        "id": "src\\transformers\\integrations\\accelerate.py"
      },
      {
        "type": "file",
        "size": 3.013,
        "file_type": "py",
        "inclusive_size": 3.013,
        "id": "src\\transformers\\integrations\\aqlm.py"
      },
      {
        "type": "file",
        "size": 4.891,
        "file_type": "py",
        "inclusive_size": 4.891,
        "id": "src\\transformers\\integrations\\awq.py"
      },
      {
        "type": "file",
        "size": 13.373,
        "file_type": "py",
        "inclusive_size": 13.373,
        "id": "src\\transformers\\integrations\\bitnet.py"
      },
      {
        "type": "file",
        "size": 14.469,
        "file_type": "py",
        "inclusive_size": 14.469,
        "id": "src\\transformers\\integrations\\bitsandbytes.py"
      },
      {
        "type": "file",
        "size": 29.014,
        "file_type": "py",
        "inclusive_size": 29.014,
        "id": "src\\transformers\\integrations\\deepspeed.py"
      },
      {
        "type": "file",
        "size": 3.334,
        "file_type": "py",
        "inclusive_size": 3.334,
        "id": "src\\transformers\\integrations\\eager_paged.py"
      },
      {
        "type": "file",
        "size": 4.849,
        "file_type": "py",
        "inclusive_size": 4.849,
        "id": "src\\transformers\\integrations\\eetq.py"
      },
      {
        "type": "file",
        "size": 46.297,
        "file_type": "py",
        "inclusive_size": 46.297,
        "id": "src\\transformers\\integrations\\executorch.py"
      },
      {
        "type": "file",
        "size": 14.493,
        "file_type": "py",
        "inclusive_size": 14.493,
        "id": "src\\transformers\\integrations\\fbgemm_fp8.py"
      },
      {
        "type": "file",
        "size": 34.284,
        "file_type": "py",
        "inclusive_size": 34.284,
        "id": "src\\transformers\\integrations\\finegrained_fp8.py"
      },
      {
        "type": "file",
        "size": 3.441,
        "file_type": "py",
        "inclusive_size": 3.441,
        "id": "src\\transformers\\integrations\\flash_attention.py"
      },
      {
        "type": "file",
        "size": 3.581,
        "file_type": "py",
        "inclusive_size": 3.581,
        "id": "src\\transformers\\integrations\\flash_paged.py"
      },
      {
        "type": "file",
        "size": 13.331,
        "file_type": "py",
        "inclusive_size": 13.331,
        "id": "src\\transformers\\integrations\\flex_attention.py"
      },
      {
        "type": "file",
        "size": 5.712,
        "file_type": "py",
        "inclusive_size": 5.712,
        "id": "src\\transformers\\integrations\\fp_quant.py"
      },
      {
        "type": "file",
        "size": 1.545,
        "file_type": "py",
        "inclusive_size": 1.545,
        "id": "src\\transformers\\integrations\\fsdp.py"
      },
      {
        "type": "file",
        "size": 30.843,
        "file_type": "py",
        "inclusive_size": 30.843,
        "id": "src\\transformers\\integrations\\ggml.py"
      },
      {
        "type": "file",
        "size": 30.612,
        "file_type": "py",
        "inclusive_size": 30.612,
        "id": "src\\transformers\\integrations\\higgs.py"
      },
      {
        "type": "file",
        "size": 5.075,
        "file_type": "py",
        "inclusive_size": 5.075,
        "id": "src\\transformers\\integrations\\hqq.py"
      },
      {
        "type": "file",
        "size": 16.539,
        "file_type": "py",
        "inclusive_size": 16.539,
        "id": "src\\transformers\\integrations\\hub_kernels.py"
      },
      {
        "type": "file",
        "size": 107.501,
        "file_type": "py",
        "inclusive_size": 107.501,
        "id": "src\\transformers\\integrations\\integration_utils.py"
      },
      {
        "type": "file",
        "size": 4.714,
        "file_type": "py",
        "inclusive_size": 4.714,
        "id": "src\\transformers\\integrations\\mistral.py"
      },
      {
        "type": "file",
        "size": 14.91,
        "file_type": "py",
        "inclusive_size": 14.91,
        "id": "src\\transformers\\integrations\\moe.py"
      },
      {
        "type": "file",
        "size": 25.37,
        "file_type": "py",
        "inclusive_size": 25.37,
        "id": "src\\transformers\\integrations\\mxfp4.py"
      },
      {
        "type": "file",
        "size": 4.189,
        "file_type": "py",
        "inclusive_size": 4.189,
        "id": "src\\transformers\\integrations\\npu_flash_attention.py"
      },
      {
        "type": "file",
        "size": 47.606,
        "file_type": "py",
        "inclusive_size": 47.606,
        "id": "src\\transformers\\integrations\\peft.py"
      },
      {
        "type": "file",
        "size": 4.878,
        "file_type": "py",
        "inclusive_size": 4.878,
        "id": "src\\transformers\\integrations\\quanto.py"
      },
      {
        "type": "file",
        "size": 2.236,
        "file_type": "py",
        "inclusive_size": 2.236,
        "id": "src\\transformers\\integrations\\quark.py"
      },
      {
        "type": "file",
        "size": 4.851,
        "file_type": "py",
        "inclusive_size": 4.851,
        "id": "src\\transformers\\integrations\\sdpa_attention.py"
      },
      {
        "type": "file",
        "size": 2.421,
        "file_type": "py",
        "inclusive_size": 2.421,
        "id": "src\\transformers\\integrations\\sdpa_paged.py"
      },
      {
        "type": "file",
        "size": 3.306,
        "file_type": "py",
        "inclusive_size": 3.306,
        "id": "src\\transformers\\integrations\\spqr.py"
      },
      {
        "type": "file",
        "size": 55.898,
        "file_type": "py",
        "inclusive_size": 55.898,
        "id": "src\\transformers\\integrations\\tensor_parallel.py"
      },
      {
        "type": "file",
        "size": 1.993,
        "file_type": "py",
        "inclusive_size": 1.993,
        "id": "src\\transformers\\integrations\\tiktoken.py"
      },
      {
        "type": "file",
        "size": 12.196,
        "file_type": "py",
        "inclusive_size": 12.196,
        "id": "src\\transformers\\integrations\\torchao.py"
      },
      {
        "type": "file",
        "size": 1.394,
        "file_type": "py",
        "inclusive_size": 1.394,
        "id": "src\\transformers\\integrations\\tpu.py"
      },
      {
        "type": "file",
        "size": 3.623,
        "file_type": "py",
        "inclusive_size": 3.623,
        "id": "src\\transformers\\integrations\\vptq.py"
      },
      {
        "type": "file",
        "size": 9.664,
        "file_type": "py",
        "inclusive_size": 9.664,
        "id": "src\\transformers\\integrations\\__init__.py"
      },
      {
        "type": "file",
        "size": 66.202,
        "file_type": "py",
        "inclusive_size": 66.202,
        "id": "src\\transformers\\generation\\candidate_generator.py"
      },
      {
        "type": "file",
        "size": 78.959,
        "file_type": "py",
        "inclusive_size": 78.959,
        "id": "src\\transformers\\generation\\configuration_utils.py"
      },
      {
        "type": "dir",
        "size": 4096,
        "file_type": "dir",
        "inclusive_size": 166.967,
        "id": "src\\transformers\\generation\\continuous_batching"
      },
      {
        "type": "file",
        "size": 150.048,
        "file_type": "py",
        "inclusive_size": 150.048,
        "id": "src\\transformers\\generation\\logits_process.py"
      },
      {
        "type": "file",
        "size": 28.871,
        "file_type": "py",
        "inclusive_size": 28.871,
        "id": "src\\transformers\\generation\\stopping_criteria.py"
      },
      {
        "type": "file",
        "size": 12.97,
        "file_type": "py",
        "inclusive_size": 12.97,
        "id": "src\\transformers\\generation\\streamers.py"
      },
      {
        "type": "file",
        "size": 214.247,
        "file_type": "py",
        "inclusive_size": 214.247,
        "id": "src\\transformers\\generation\\utils.py"
      },
      {
        "type": "file",
        "size": 24.565,
        "file_type": "py",
        "inclusive_size": 24.565,
        "id": "src\\transformers\\generation\\watermarking.py"
      },
      {
        "type": "file",
        "size": 6.83,
        "file_type": "py",
        "inclusive_size": 6.83,
        "id": "src\\transformers\\generation\\__init__.py"
      },
      {
        "type": "file",
        "size": 38.904,
        "file_type": "py",
        "inclusive_size": 38.904,
        "id": "src\\transformers\\generation\\continuous_batching\\cache.py"
      },
      {
        "type": "file",
        "size": 26.205,
        "file_type": "py",
        "inclusive_size": 26.205,
        "id": "src\\transformers\\generation\\continuous_batching\\cache_manager.py"
      },
      {
        "type": "file",
        "size": 66.65,
        "file_type": "py",
        "inclusive_size": 66.65,
        "id": "src\\transformers\\generation\\continuous_batching\\continuous_api.py"
      },
      {
        "type": "file",
        "size": 14.478,
        "file_type": "py",
        "inclusive_size": 14.478,
        "id": "src\\transformers\\generation\\continuous_batching\\requests.py"
      },
      {
        "type": "file",
        "size": 19.703,
        "file_type": "py",
        "inclusive_size": 19.703,
        "id": "src\\transformers\\generation\\continuous_batching\\scheduler.py"
      },
      {
        "type": "file",
        "size": 1.027,
        "file_type": "py",
        "inclusive_size": 1.027,
        "id": "src\\transformers\\generation\\continuous_batching\\__init__.py"
      },
      {
        "type": "file",
        "size": 4.386,
        "file_type": "py",
        "inclusive_size": 4.386,
        "id": "src\\transformers\\distributed\\configuration_utils.py"
      },
      {
        "type": "file",
        "size": 0.978,
        "file_type": "py",
        "inclusive_size": 0.978,
        "id": "src\\transformers\\distributed\\__init__.py"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 15.955,
        "id": "src\\transformers\\data\\datasets"
      },
      {
        "type": "file",
        "size": 69.69,
        "file_type": "py",
        "inclusive_size": 69.69,
        "id": "src\\transformers\\data\\data_collator.py"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 33.324999999999996,
        "id": "src\\transformers\\data\\metrics"
      },
      {
        "type": "dir",
        "size": 0,
        "file_type": "dir",
        "inclusive_size": 67.572,
        "id": "src\\transformers\\data\\processors"
      },
      {
        "type": "file",
        "size": 1.49,
        "file_type": "py",
        "inclusive_size": 1.49,
        "id": "src\\transformers\\data\\__init__.py"
      },
      {
        "type": "file",
        "size": 21.368,
        "file_type": "py",
        "inclusive_size": 21.368,
        "id": "src\\transformers\\data\\processors\\glue.py"
      },
      {
        "type": "file",
        "size": 28.895,
        "file_type": "py",
        "inclusive_size": 28.895,
        "id": "src\\transformers\\data\\processors\\squad.py"
      },
      {
        "type": "file",
        "size": 12.829,
        "file_type": "py",
        "inclusive_size": 12.829,
        "id": "src\\transformers\\data\\processors\\utils.py"
      },
      {
        "type": "file",
        "size": 3.466,
        "file_type": "py",
        "inclusive_size": 3.466,
        "id": "src\\transformers\\data\\processors\\xnli.py"
      },
      {
        "type": "file",
        "size": 1.014,
        "file_type": "py",
        "inclusive_size": 1.014,
        "id": "src\\transformers\\data\\processors\\__init__.py"
      },
      {
        "type": "file",
        "size": 29.685,
        "file_type": "py",
        "inclusive_size": 29.685,
        "id": "src\\transformers\\data\\metrics\\squad_metrics.py"
      },
      {
        "type": "file",
        "size": 3.64,
        "file_type": "py",
        "inclusive_size": 3.64,
        "id": "src\\transformers\\data\\metrics\\__init__.py"
      },
      {
        "type": "file",
        "size": 6.086,
        "file_type": "py",
        "inclusive_size": 6.086,
        "id": "src\\transformers\\data\\datasets\\glue.py"
      },
      {
        "type": "file",
        "size": 9.145,
        "file_type": "py",
        "inclusive_size": 9.145,
        "id": "src\\transformers\\data\\datasets\\squad.py"
      },
      {
        "type": "file",
        "size": 0.724,
        "file_type": "py",
        "inclusive_size": 0.724,
        "id": "src\\transformers\\data\\datasets\\__init__.py"
      },
      {
        "type": "file",
        "size": 23.296,
        "file_type": "py",
        "inclusive_size": 23.296,
        "id": "src\\transformers\\cli\\add_fast_image_processor.py"
      },
      {
        "type": "file",
        "size": 33.031,
        "file_type": "py",
        "inclusive_size": 33.031,
        "id": "src\\transformers\\cli\\add_new_model_like.py"
      },
      {
        "type": "file",
        "size": 24.695,
        "file_type": "py",
        "inclusive_size": 24.695,
        "id": "src\\transformers\\cli\\chat.py"
      },
      {
        "type": "file",
        "size": 1.692,
        "file_type": "py",
        "inclusive_size": 1.692,
        "id": "src\\transformers\\cli\\download.py"
      },
      {
        "type": "file",
        "size": 79.828,
        "file_type": "py",
        "inclusive_size": 79.828,
        "id": "src\\transformers\\cli\\serve.py"
      },
      {
        "type": "file",
        "size": 4.738,
        "file_type": "py",
        "inclusive_size": 4.738,
        "id": "src\\transformers\\cli\\system.py"
      },
      {
        "type": "file",
        "size": 1.367,
        "file_type": "py",
        "inclusive_size": 1.367,
        "id": "src\\transformers\\cli\\transformers.py"
      },
      {
        "type": "file",
        "size": 0.606,
        "file_type": "py",
        "inclusive_size": 0.606,
        "id": "src\\transformers\\cli\\__init__.py"
      }
    ],
    "edges": [
      {
        "source": "src",
        "target": "src\\transformers"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\activations.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\audio_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\cache_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\cli"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\configuration_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\conversion_mapping.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\convert_slow_tokenizer.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\convert_slow_tokenizers_checkpoints_to_fast.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\core_model_loading.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\data"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\debug_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\dependency_versions_check.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\dependency_versions_table.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\distributed"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\dynamic_module_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\feature_extraction_sequence_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\feature_extraction_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\file_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\generation"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\hf_argparser.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\hyperparameter_search.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\image_processing_base.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\image_processing_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\image_processing_utils_fast.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\image_transforms.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\image_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\initialization.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\integrations"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\loss"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\masking_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\modelcard.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\modeling_attn_mask_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\modeling_flash_attention_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\modeling_gguf_pytorch_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\modeling_layers.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\modeling_outputs.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\modeling_rope_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\modeling_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\models"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\model_debugging_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\optimization.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\pipelines"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\processing_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\py.typed"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\pytorch_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\quantizers"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\safetensors_conversion.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\testing_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\time_series_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\tokenization_mistral_common.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\tokenization_python.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\tokenization_utils_base.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\tokenization_utils_sentencepiece.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\tokenization_utils_tokenizers.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\trainer.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\trainer_callback.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\trainer_jit_checkpoint.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\trainer_pt_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\trainer_seq2seq.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\trainer_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\training_args.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\training_args_seq2seq.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\utils"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\video_processing_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\video_utils.py"
      },
      {
        "source": "src\\transformers",
        "target": "src\\transformers\\__init__.py"
      },
      {
        "source": "src\\transformers\\cli",
        "target": "src\\transformers\\cli\\add_fast_image_processor.py"
      },
      {
        "source": "src\\transformers\\cli",
        "target": "src\\transformers\\cli\\add_new_model_like.py"
      },
      {
        "source": "src\\transformers\\cli",
        "target": "src\\transformers\\cli\\chat.py"
      },
      {
        "source": "src\\transformers\\cli",
        "target": "src\\transformers\\cli\\download.py"
      },
      {
        "source": "src\\transformers\\cli",
        "target": "src\\transformers\\cli\\serve.py"
      },
      {
        "source": "src\\transformers\\cli",
        "target": "src\\transformers\\cli\\system.py"
      },
      {
        "source": "src\\transformers\\cli",
        "target": "src\\transformers\\cli\\transformers.py"
      },
      {
        "source": "src\\transformers\\cli",
        "target": "src\\transformers\\cli\\__init__.py"
      },
      {
        "source": "src\\transformers\\data",
        "target": "src\\transformers\\data\\datasets"
      },
      {
        "source": "src\\transformers\\data",
        "target": "src\\transformers\\data\\data_collator.py"
      },
      {
        "source": "src\\transformers\\data",
        "target": "src\\transformers\\data\\metrics"
      },
      {
        "source": "src\\transformers\\data",
        "target": "src\\transformers\\data\\processors"
      },
      {
        "source": "src\\transformers\\data",
        "target": "src\\transformers\\data\\__init__.py"
      },
      {
        "source": "src\\transformers\\distributed",
        "target": "src\\transformers\\distributed\\configuration_utils.py"
      },
      {
        "source": "src\\transformers\\distributed",
        "target": "src\\transformers\\distributed\\__init__.py"
      },
      {
        "source": "src\\transformers\\generation",
        "target": "src\\transformers\\generation\\candidate_generator.py"
      },
      {
        "source": "src\\transformers\\generation",
        "target": "src\\transformers\\generation\\configuration_utils.py"
      },
      {
        "source": "src\\transformers\\generation",
        "target": "src\\transformers\\generation\\continuous_batching"
      },
      {
        "source": "src\\transformers\\generation",
        "target": "src\\transformers\\generation\\logits_process.py"
      },
      {
        "source": "src\\transformers\\generation",
        "target": "src\\transformers\\generation\\stopping_criteria.py"
      },
      {
        "source": "src\\transformers\\generation",
        "target": "src\\transformers\\generation\\streamers.py"
      },
      {
        "source": "src\\transformers\\generation",
        "target": "src\\transformers\\generation\\utils.py"
      },
      {
        "source": "src\\transformers\\generation",
        "target": "src\\transformers\\generation\\watermarking.py"
      },
      {
        "source": "src\\transformers\\generation",
        "target": "src\\transformers\\generation\\__init__.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\accelerate.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\aqlm.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\awq.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\bitnet.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\bitsandbytes.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\deepspeed.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\eager_paged.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\eetq.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\executorch.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\fbgemm_fp8.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\finegrained_fp8.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\flash_attention.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\flash_paged.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\flex_attention.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\fp_quant.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\fsdp.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\ggml.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\higgs.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\hqq.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\hub_kernels.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\integration_utils.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\mistral.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\moe.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\mxfp4.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\npu_flash_attention.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\peft.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\quanto.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\quark.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\sdpa_attention.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\sdpa_paged.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\spqr.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\tensor_parallel.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\tiktoken.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\torchao.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\tpu.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\vptq.py"
      },
      {
        "source": "src\\transformers\\integrations",
        "target": "src\\transformers\\integrations\\__init__.py"
      },
      {
        "source": "src\\transformers\\loss",
        "target": "src\\transformers\\loss\\loss_deformable_detr.py"
      },
      {
        "source": "src\\transformers\\loss",
        "target": "src\\transformers\\loss\\loss_d_fine.py"
      },
      {
        "source": "src\\transformers\\loss",
        "target": "src\\transformers\\loss\\loss_for_object_detection.py"
      },
      {
        "source": "src\\transformers\\loss",
        "target": "src\\transformers\\loss\\loss_grounding_dino.py"
      },
      {
        "source": "src\\transformers\\loss",
        "target": "src\\transformers\\loss\\loss_lw_detr.py"
      },
      {
        "source": "src\\transformers\\loss",
        "target": "src\\transformers\\loss\\loss_rt_detr.py"
      },
      {
        "source": "src\\transformers\\loss",
        "target": "src\\transformers\\loss\\loss_utils.py"
      },
      {
        "source": "src\\transformers\\loss",
        "target": "src\\transformers\\loss\\__init__.py"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\afmoe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\aimv2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\albert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\align"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\altclip"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\apertus"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\arcee"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\aria"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\audioflamingo3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\audio_spectrogram_transformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\auto"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\autoformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\aya_vision"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bamba"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bark"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bart"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\barthez"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bartpho"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\beit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bertweet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bert_generation"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bert_japanese"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bigbird_pegasus"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\big_bird"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\biogpt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bitnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\blenderbot"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\blenderbot_small"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\blip"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\blip_2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bloom"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\blt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bridgetower"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\bros"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\byt5"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\camembert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\canine"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\chameleon"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\chinese_clip"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\clap"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\clip"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\clipseg"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\clvp"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\codegen"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\code_llama"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\cohere"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\cohere2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\cohere2_vision"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\colpali"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\colqwen2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\conditional_detr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\convbert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\convnext"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\convnextv2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\cpm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\cpmant"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\csm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\ctrl"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\cvt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\cwm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dab_detr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dac"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\data2vec"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dbrx"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\deberta"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\deberta_v2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\decision_transformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\deepseek_v2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\deepseek_v3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\deepseek_vl"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\deepseek_vl_hybrid"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\deformable_detr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\deit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\deprecated"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\depth_anything"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\depth_pro"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\detr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dia"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dialogpt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\diffllama"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dinat"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dinov2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dinov2_with_registers"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dinov3_convnext"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dinov3_vit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\distilbert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\doge"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\donut"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dots1"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dpr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\dpt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\d_fine"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\edgetam"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\edgetam_video"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\efficientloftr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\efficientnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\electra"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\emu3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\encodec"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\encoder_decoder"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\eomt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\ernie"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\ernie4_5"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\ernie4_5_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\ernie4_5_vl_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\esm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\evolla"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\exaone4"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\falcon"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\falcon_h1"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\falcon_mamba"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\fastspeech2_conformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\fast_vlm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\flaubert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\flava"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\flex_olmo"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\florence2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\fnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\focalnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\fsmt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\funnel"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\fuyu"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gemma"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gemma2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gemma3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gemma3n"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\git"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glm4"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glm46v"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glm4v"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glm4v_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glm4_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glm4_moe_lite"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glmasr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glm_image"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glm_ocr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\glpn"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\got_ocr2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gpt2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gptj"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gpt_bigcode"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gpt_neo"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gpt_neox"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gpt_neox_japanese"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gpt_oss"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\gpt_sw3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\granite"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\granitemoe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\granitemoehybrid"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\granitemoeshared"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\granite_speech"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\grounding_dino"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\groupvit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\helium"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\herbert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\hgnet_v2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\hiera"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\hubert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\hunyuan_v1_dense"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\hunyuan_v1_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\ibert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\idefics"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\idefics2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\idefics3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\ijepa"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\imagegpt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\informer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\instructblip"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\instructblipvideo"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\internvl"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\jais2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\jamba"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\janus"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\jetmoe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\kosmos2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\kosmos2_5"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\kyutai_speech_to_text"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\lasr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\layoutlm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\layoutlmv2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\layoutlmv3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\layoutxlm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\led"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\levit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\lfm2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\lfm2_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\lfm2_vl"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\lightglue"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\lighton_ocr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\lilt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\llama"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\llama4"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\llava"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\llava_next"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\llava_next_video"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\llava_onevision"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\longcat_flash"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\longformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\longt5"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\luke"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\lw_detr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\lxmert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\m2m_100"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mamba"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mamba2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\marian"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\markuplm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mask2former"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\maskformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mbart"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mbart50"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\megatron_bert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\megatron_gpt2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\metaclip_2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mgp_str"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mimi"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\minimax"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\minimax_m2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\ministral"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\ministral3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mistral"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mistral3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mixtral"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mlcd"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mllama"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mluke"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mm_grounding_dino"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mobilebert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mobilenet_v1"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mobilenet_v2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mobilevit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mobilevitv2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\modernbert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\modernbert_decoder"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\moonshine"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\moshi"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mpnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mpt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mra"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mt5"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\musicgen"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\musicgen_melody"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\mvp"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\myt5"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\nanochat"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\nemotron"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\nllb"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\nllb_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\nougat"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\nystromformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\olmo"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\olmo2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\olmo3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\olmoe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\omdet_turbo"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\oneformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\openai"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\opt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\ovis2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\owlv2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\owlvit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\paddleocr_vl"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\paligemma"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\parakeet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\patchtsmixer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\patchtst"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pegasus"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pegasus_x"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\perceiver"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\perception_lm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\persimmon"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pe_audio"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pe_audio_video"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pe_video"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\phi"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\phi3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\phi4_multimodal"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\phimoe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\phobert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pix2struct"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pixio"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pixtral"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\plbart"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\poolformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pop2piano"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pp_doclayout_v3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\prompt_depth_anything"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\prophetnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pvt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\pvt_v2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen2_5_omni"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen2_5_vl"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen2_audio"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen2_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen2_vl"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen3_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen3_next"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen3_omni_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen3_vl"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\qwen3_vl_moe"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\rag"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\recurrent_gemma"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\reformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\regnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\rembert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\resnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\roberta"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\roberta_prelayernorm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\roc_bert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\roformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\rt_detr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\rt_detr_v2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\rwkv"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\sam"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\sam2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\sam2_video"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\sam3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\sam3_tracker"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\sam3_tracker_video"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\sam3_video"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\sam_hq"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\seamless_m4t"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\seamless_m4t_v2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\seed_oss"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\segformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\seggpt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\sew"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\sew_d"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\shieldgemma2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\siglip"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\siglip2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\smollm3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\smolvlm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\solar_open"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\speecht5"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\speech_encoder_decoder"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\speech_to_text"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\splinter"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\squeezebert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\stablelm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\starcoder2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\superglue"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\superpoint"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\swiftformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\swin"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\swin2sr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\swinv2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\switch_transformers"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\t5"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\t5gemma"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\t5gemma2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\table_transformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\tapas"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\textnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\timesfm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\timesformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\time_series_transformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\timm_backbone"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\timm_wrapper"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\trocr"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\tvp"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\udop"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\umt5"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\unispeech"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\unispeech_sat"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\univnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\upernet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vaultgemma"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\videomae"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\video_llama_3"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\video_llava"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vilt"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vipllava"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vision_encoder_decoder"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vision_text_dual_encoder"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\visual_bert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vitdet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vitmatte"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vitpose"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vitpose_backbone"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vits"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vit_mae"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vit_msn"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vivit"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\vjepa2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\voxtral"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\wav2vec2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\wav2vec2_bert"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\wav2vec2_conformer"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\wav2vec2_phoneme"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\wav2vec2_with_lm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\wavlm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\whisper"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\xcodec"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\xglm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\xlm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\xlm_roberta"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\xlm_roberta_xl"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\xlnet"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\xlstm"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\xmod"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\x_clip"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\yolos"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\yoso"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\youtu"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\zamba"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\zamba2"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\zoedepth"
      },
      {
        "source": "src\\transformers\\models",
        "target": "src\\transformers\\models\\__init__.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\any_to_any.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\audio_classification.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\audio_utils.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\automatic_speech_recognition.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\base.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\depth_estimation.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\document_question_answering.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\feature_extraction.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\fill_mask.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\image_classification.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\image_feature_extraction.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\image_segmentation.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\image_text_to_text.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\image_to_image.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\keypoint_matching.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\mask_generation.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\object_detection.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\pt_utils.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\question_answering.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\table_question_answering.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\text_classification.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\text_generation.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\text_to_audio.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\token_classification.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\video_classification.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\visual_question_answering.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\zero_shot_audio_classification.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\zero_shot_classification.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\zero_shot_image_classification.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\zero_shot_object_detection.py"
      },
      {
        "source": "src\\transformers\\pipelines",
        "target": "src\\transformers\\pipelines\\__init__.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\auto.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\base.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizers_utils.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_aqlm.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_auto_round.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_awq.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_bitnet.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_bnb_4bit.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_bnb_8bit.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_compressed_tensors.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_eetq.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_fbgemm_fp8.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_finegrained_fp8.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_fp_quant.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_gptq.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_higgs.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_hqq.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_mxfp4.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_quanto.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_quark.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_spqr.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_torchao.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\quantizer_vptq.py"
      },
      {
        "source": "src\\transformers\\quantizers",
        "target": "src\\transformers\\quantizers\\__init__.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\attention_visualizer.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\auto_docstring.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\backbone_utils.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\chat_parsing_utils.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\chat_template_utils.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\constants.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\deprecation.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\doc.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_detectron2_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_mistral_common_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_music_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_pt_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_sentencepiece_and_tokenizers_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_speech_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_timm_and_torchvision_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_tokenizers_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_torchaudio_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_torchvision_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\dummy_vision_objects.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\generic.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\hp_naming.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\hub.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\import_utils.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\kernel_config.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\loading_report.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\logging.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\metrics.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\notebook.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\peft_utils.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\pytest_helpers.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\quantization_config.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\sentencepiece_model_pb2.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\sentencepiece_model_pb2_new.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\type_validators.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\versions.py"
      },
      {
        "source": "src\\transformers\\utils",
        "target": "src\\transformers\\utils\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\afmoe",
        "target": "src\\transformers\\models\\afmoe\\configuration_afmoe.py"
      },
      {
        "source": "src\\transformers\\models\\afmoe",
        "target": "src\\transformers\\models\\afmoe\\modeling_afmoe.py"
      },
      {
        "source": "src\\transformers\\models\\afmoe",
        "target": "src\\transformers\\models\\afmoe\\modular_afmoe.py"
      },
      {
        "source": "src\\transformers\\models\\afmoe",
        "target": "src\\transformers\\models\\afmoe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\aimv2",
        "target": "src\\transformers\\models\\aimv2\\configuration_aimv2.py"
      },
      {
        "source": "src\\transformers\\models\\aimv2",
        "target": "src\\transformers\\models\\aimv2\\convert_aimv2_original_pytorch_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\aimv2",
        "target": "src\\transformers\\models\\aimv2\\modeling_aimv2.py"
      },
      {
        "source": "src\\transformers\\models\\aimv2",
        "target": "src\\transformers\\models\\aimv2\\modular_aimv2.py"
      },
      {
        "source": "src\\transformers\\models\\aimv2",
        "target": "src\\transformers\\models\\aimv2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\albert",
        "target": "src\\transformers\\models\\albert\\configuration_albert.py"
      },
      {
        "source": "src\\transformers\\models\\albert",
        "target": "src\\transformers\\models\\albert\\convert_albert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\albert",
        "target": "src\\transformers\\models\\albert\\modeling_albert.py"
      },
      {
        "source": "src\\transformers\\models\\albert",
        "target": "src\\transformers\\models\\albert\\tokenization_albert.py"
      },
      {
        "source": "src\\transformers\\models\\albert",
        "target": "src\\transformers\\models\\albert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\align",
        "target": "src\\transformers\\models\\align\\configuration_align.py"
      },
      {
        "source": "src\\transformers\\models\\align",
        "target": "src\\transformers\\models\\align\\convert_align_tf_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\align",
        "target": "src\\transformers\\models\\align\\modeling_align.py"
      },
      {
        "source": "src\\transformers\\models\\align",
        "target": "src\\transformers\\models\\align\\processing_align.py"
      },
      {
        "source": "src\\transformers\\models\\align",
        "target": "src\\transformers\\models\\align\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\altclip",
        "target": "src\\transformers\\models\\altclip\\configuration_altclip.py"
      },
      {
        "source": "src\\transformers\\models\\altclip",
        "target": "src\\transformers\\models\\altclip\\modeling_altclip.py"
      },
      {
        "source": "src\\transformers\\models\\altclip",
        "target": "src\\transformers\\models\\altclip\\processing_altclip.py"
      },
      {
        "source": "src\\transformers\\models\\altclip",
        "target": "src\\transformers\\models\\altclip\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\apertus",
        "target": "src\\transformers\\models\\apertus\\configuration_apertus.py"
      },
      {
        "source": "src\\transformers\\models\\apertus",
        "target": "src\\transformers\\models\\apertus\\modeling_apertus.py"
      },
      {
        "source": "src\\transformers\\models\\apertus",
        "target": "src\\transformers\\models\\apertus\\modular_apertus.py"
      },
      {
        "source": "src\\transformers\\models\\apertus",
        "target": "src\\transformers\\models\\apertus\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\arcee",
        "target": "src\\transformers\\models\\arcee\\configuration_arcee.py"
      },
      {
        "source": "src\\transformers\\models\\arcee",
        "target": "src\\transformers\\models\\arcee\\modeling_arcee.py"
      },
      {
        "source": "src\\transformers\\models\\arcee",
        "target": "src\\transformers\\models\\arcee\\modular_arcee.py"
      },
      {
        "source": "src\\transformers\\models\\arcee",
        "target": "src\\transformers\\models\\arcee\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\aria",
        "target": "src\\transformers\\models\\aria\\configuration_aria.py"
      },
      {
        "source": "src\\transformers\\models\\aria",
        "target": "src\\transformers\\models\\aria\\convert_aria_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\aria",
        "target": "src\\transformers\\models\\aria\\image_processing_aria.py"
      },
      {
        "source": "src\\transformers\\models\\aria",
        "target": "src\\transformers\\models\\aria\\modeling_aria.py"
      },
      {
        "source": "src\\transformers\\models\\aria",
        "target": "src\\transformers\\models\\aria\\modular_aria.py"
      },
      {
        "source": "src\\transformers\\models\\aria",
        "target": "src\\transformers\\models\\aria\\processing_aria.py"
      },
      {
        "source": "src\\transformers\\models\\aria",
        "target": "src\\transformers\\models\\aria\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\audioflamingo3",
        "target": "src\\transformers\\models\\audioflamingo3\\configuration_audioflamingo3.py"
      },
      {
        "source": "src\\transformers\\models\\audioflamingo3",
        "target": "src\\transformers\\models\\audioflamingo3\\convert_audioflamingo3_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\audioflamingo3",
        "target": "src\\transformers\\models\\audioflamingo3\\modeling_audioflamingo3.py"
      },
      {
        "source": "src\\transformers\\models\\audioflamingo3",
        "target": "src\\transformers\\models\\audioflamingo3\\modular_audioflamingo3.py"
      },
      {
        "source": "src\\transformers\\models\\audioflamingo3",
        "target": "src\\transformers\\models\\audioflamingo3\\processing_audioflamingo3.py"
      },
      {
        "source": "src\\transformers\\models\\audioflamingo3",
        "target": "src\\transformers\\models\\audioflamingo3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\audio_spectrogram_transformer",
        "target": "src\\transformers\\models\\audio_spectrogram_transformer\\configuration_audio_spectrogram_transformer.py"
      },
      {
        "source": "src\\transformers\\models\\audio_spectrogram_transformer",
        "target": "src\\transformers\\models\\audio_spectrogram_transformer\\convert_audio_spectrogram_transformer_original_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\audio_spectrogram_transformer",
        "target": "src\\transformers\\models\\audio_spectrogram_transformer\\feature_extraction_audio_spectrogram_transformer.py"
      },
      {
        "source": "src\\transformers\\models\\audio_spectrogram_transformer",
        "target": "src\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py"
      },
      {
        "source": "src\\transformers\\models\\audio_spectrogram_transformer",
        "target": "src\\transformers\\models\\audio_spectrogram_transformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\auto",
        "target": "src\\transformers\\models\\auto\\auto_factory.py"
      },
      {
        "source": "src\\transformers\\models\\auto",
        "target": "src\\transformers\\models\\auto\\configuration_auto.py"
      },
      {
        "source": "src\\transformers\\models\\auto",
        "target": "src\\transformers\\models\\auto\\feature_extraction_auto.py"
      },
      {
        "source": "src\\transformers\\models\\auto",
        "target": "src\\transformers\\models\\auto\\image_processing_auto.py"
      },
      {
        "source": "src\\transformers\\models\\auto",
        "target": "src\\transformers\\models\\auto\\modeling_auto.py"
      },
      {
        "source": "src\\transformers\\models\\auto",
        "target": "src\\transformers\\models\\auto\\processing_auto.py"
      },
      {
        "source": "src\\transformers\\models\\auto",
        "target": "src\\transformers\\models\\auto\\tokenization_auto.py"
      },
      {
        "source": "src\\transformers\\models\\auto",
        "target": "src\\transformers\\models\\auto\\video_processing_auto.py"
      },
      {
        "source": "src\\transformers\\models\\auto",
        "target": "src\\transformers\\models\\auto\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\autoformer",
        "target": "src\\transformers\\models\\autoformer\\configuration_autoformer.py"
      },
      {
        "source": "src\\transformers\\models\\autoformer",
        "target": "src\\transformers\\models\\autoformer\\modeling_autoformer.py"
      },
      {
        "source": "src\\transformers\\models\\autoformer",
        "target": "src\\transformers\\models\\autoformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\aya_vision",
        "target": "src\\transformers\\models\\aya_vision\\configuration_aya_vision.py"
      },
      {
        "source": "src\\transformers\\models\\aya_vision",
        "target": "src\\transformers\\models\\aya_vision\\modeling_aya_vision.py"
      },
      {
        "source": "src\\transformers\\models\\aya_vision",
        "target": "src\\transformers\\models\\aya_vision\\modular_aya_vision.py"
      },
      {
        "source": "src\\transformers\\models\\aya_vision",
        "target": "src\\transformers\\models\\aya_vision\\processing_aya_vision.py"
      },
      {
        "source": "src\\transformers\\models\\aya_vision",
        "target": "src\\transformers\\models\\aya_vision\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bamba",
        "target": "src\\transformers\\models\\bamba\\configuration_bamba.py"
      },
      {
        "source": "src\\transformers\\models\\bamba",
        "target": "src\\transformers\\models\\bamba\\convert_mamba_ssm_checkpoint.py"
      },
      {
        "source": "src\\transformers\\models\\bamba",
        "target": "src\\transformers\\models\\bamba\\modeling_bamba.py"
      },
      {
        "source": "src\\transformers\\models\\bamba",
        "target": "src\\transformers\\models\\bamba\\modular_bamba.py"
      },
      {
        "source": "src\\transformers\\models\\bamba",
        "target": "src\\transformers\\models\\bamba\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bark",
        "target": "src\\transformers\\models\\bark\\configuration_bark.py"
      },
      {
        "source": "src\\transformers\\models\\bark",
        "target": "src\\transformers\\models\\bark\\convert_suno_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\bark",
        "target": "src\\transformers\\models\\bark\\generation_configuration_bark.py"
      },
      {
        "source": "src\\transformers\\models\\bark",
        "target": "src\\transformers\\models\\bark\\modeling_bark.py"
      },
      {
        "source": "src\\transformers\\models\\bark",
        "target": "src\\transformers\\models\\bark\\processing_bark.py"
      },
      {
        "source": "src\\transformers\\models\\bark",
        "target": "src\\transformers\\models\\bark\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bart",
        "target": "src\\transformers\\models\\bart\\configuration_bart.py"
      },
      {
        "source": "src\\transformers\\models\\bart",
        "target": "src\\transformers\\models\\bart\\convert_bart_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\bart",
        "target": "src\\transformers\\models\\bart\\modeling_bart.py"
      },
      {
        "source": "src\\transformers\\models\\bart",
        "target": "src\\transformers\\models\\bart\\tokenization_bart.py"
      },
      {
        "source": "src\\transformers\\models\\bart",
        "target": "src\\transformers\\models\\bart\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\barthez",
        "target": "src\\transformers\\models\\barthez\\tokenization_barthez.py"
      },
      {
        "source": "src\\transformers\\models\\barthez",
        "target": "src\\transformers\\models\\barthez\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bartpho",
        "target": "src\\transformers\\models\\bartpho\\tokenization_bartpho.py"
      },
      {
        "source": "src\\transformers\\models\\bartpho",
        "target": "src\\transformers\\models\\bartpho\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\beit",
        "target": "src\\transformers\\models\\beit\\configuration_beit.py"
      },
      {
        "source": "src\\transformers\\models\\beit",
        "target": "src\\transformers\\models\\beit\\convert_beit_unilm_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\beit",
        "target": "src\\transformers\\models\\beit\\image_processing_beit.py"
      },
      {
        "source": "src\\transformers\\models\\beit",
        "target": "src\\transformers\\models\\beit\\image_processing_beit_fast.py"
      },
      {
        "source": "src\\transformers\\models\\beit",
        "target": "src\\transformers\\models\\beit\\modeling_beit.py"
      },
      {
        "source": "src\\transformers\\models\\beit",
        "target": "src\\transformers\\models\\beit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bert",
        "target": "src\\transformers\\models\\bert\\configuration_bert.py"
      },
      {
        "source": "src\\transformers\\models\\bert",
        "target": "src\\transformers\\models\\bert\\convert_bert_original_tf2_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\bert",
        "target": "src\\transformers\\models\\bert\\convert_bert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\bert",
        "target": "src\\transformers\\models\\bert\\convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\bert",
        "target": "src\\transformers\\models\\bert\\modeling_bert.py"
      },
      {
        "source": "src\\transformers\\models\\bert",
        "target": "src\\transformers\\models\\bert\\tokenization_bert.py"
      },
      {
        "source": "src\\transformers\\models\\bert",
        "target": "src\\transformers\\models\\bert\\tokenization_bert_legacy.py"
      },
      {
        "source": "src\\transformers\\models\\bert",
        "target": "src\\transformers\\models\\bert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bertweet",
        "target": "src\\transformers\\models\\bertweet\\tokenization_bertweet.py"
      },
      {
        "source": "src\\transformers\\models\\bertweet",
        "target": "src\\transformers\\models\\bertweet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bert_generation",
        "target": "src\\transformers\\models\\bert_generation\\configuration_bert_generation.py"
      },
      {
        "source": "src\\transformers\\models\\bert_generation",
        "target": "src\\transformers\\models\\bert_generation\\modeling_bert_generation.py"
      },
      {
        "source": "src\\transformers\\models\\bert_generation",
        "target": "src\\transformers\\models\\bert_generation\\tokenization_bert_generation.py"
      },
      {
        "source": "src\\transformers\\models\\bert_generation",
        "target": "src\\transformers\\models\\bert_generation\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bert_japanese",
        "target": "src\\transformers\\models\\bert_japanese\\tokenization_bert_japanese.py"
      },
      {
        "source": "src\\transformers\\models\\bert_japanese",
        "target": "src\\transformers\\models\\bert_japanese\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bigbird_pegasus",
        "target": "src\\transformers\\models\\bigbird_pegasus\\configuration_bigbird_pegasus.py"
      },
      {
        "source": "src\\transformers\\models\\bigbird_pegasus",
        "target": "src\\transformers\\models\\bigbird_pegasus\\convert_bigbird_pegasus_tf_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\bigbird_pegasus",
        "target": "src\\transformers\\models\\bigbird_pegasus\\modeling_bigbird_pegasus.py"
      },
      {
        "source": "src\\transformers\\models\\bigbird_pegasus",
        "target": "src\\transformers\\models\\bigbird_pegasus\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\big_bird",
        "target": "src\\transformers\\models\\big_bird\\configuration_big_bird.py"
      },
      {
        "source": "src\\transformers\\models\\big_bird",
        "target": "src\\transformers\\models\\big_bird\\convert_bigbird_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\big_bird",
        "target": "src\\transformers\\models\\big_bird\\modeling_big_bird.py"
      },
      {
        "source": "src\\transformers\\models\\big_bird",
        "target": "src\\transformers\\models\\big_bird\\tokenization_big_bird.py"
      },
      {
        "source": "src\\transformers\\models\\big_bird",
        "target": "src\\transformers\\models\\big_bird\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\biogpt",
        "target": "src\\transformers\\models\\biogpt\\configuration_biogpt.py"
      },
      {
        "source": "src\\transformers\\models\\biogpt",
        "target": "src\\transformers\\models\\biogpt\\convert_biogpt_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\biogpt",
        "target": "src\\transformers\\models\\biogpt\\modeling_biogpt.py"
      },
      {
        "source": "src\\transformers\\models\\biogpt",
        "target": "src\\transformers\\models\\biogpt\\modular_biogpt.py"
      },
      {
        "source": "src\\transformers\\models\\biogpt",
        "target": "src\\transformers\\models\\biogpt\\tokenization_biogpt.py"
      },
      {
        "source": "src\\transformers\\models\\biogpt",
        "target": "src\\transformers\\models\\biogpt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bit",
        "target": "src\\transformers\\models\\bit\\configuration_bit.py"
      },
      {
        "source": "src\\transformers\\models\\bit",
        "target": "src\\transformers\\models\\bit\\convert_bit_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\bit",
        "target": "src\\transformers\\models\\bit\\image_processing_bit.py"
      },
      {
        "source": "src\\transformers\\models\\bit",
        "target": "src\\transformers\\models\\bit\\image_processing_bit_fast.py"
      },
      {
        "source": "src\\transformers\\models\\bit",
        "target": "src\\transformers\\models\\bit\\modeling_bit.py"
      },
      {
        "source": "src\\transformers\\models\\bit",
        "target": "src\\transformers\\models\\bit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bitnet",
        "target": "src\\transformers\\models\\bitnet\\configuration_bitnet.py"
      },
      {
        "source": "src\\transformers\\models\\bitnet",
        "target": "src\\transformers\\models\\bitnet\\modeling_bitnet.py"
      },
      {
        "source": "src\\transformers\\models\\bitnet",
        "target": "src\\transformers\\models\\bitnet\\modular_bitnet.py"
      },
      {
        "source": "src\\transformers\\models\\bitnet",
        "target": "src\\transformers\\models\\bitnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\blenderbot",
        "target": "src\\transformers\\models\\blenderbot\\configuration_blenderbot.py"
      },
      {
        "source": "src\\transformers\\models\\blenderbot",
        "target": "src\\transformers\\models\\blenderbot\\convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\blenderbot",
        "target": "src\\transformers\\models\\blenderbot\\modeling_blenderbot.py"
      },
      {
        "source": "src\\transformers\\models\\blenderbot",
        "target": "src\\transformers\\models\\blenderbot\\tokenization_blenderbot.py"
      },
      {
        "source": "src\\transformers\\models\\blenderbot",
        "target": "src\\transformers\\models\\blenderbot\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\blenderbot_small",
        "target": "src\\transformers\\models\\blenderbot_small\\configuration_blenderbot_small.py"
      },
      {
        "source": "src\\transformers\\models\\blenderbot_small",
        "target": "src\\transformers\\models\\blenderbot_small\\modeling_blenderbot_small.py"
      },
      {
        "source": "src\\transformers\\models\\blenderbot_small",
        "target": "src\\transformers\\models\\blenderbot_small\\tokenization_blenderbot_small.py"
      },
      {
        "source": "src\\transformers\\models\\blenderbot_small",
        "target": "src\\transformers\\models\\blenderbot_small\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\blip",
        "target": "src\\transformers\\models\\blip\\configuration_blip.py"
      },
      {
        "source": "src\\transformers\\models\\blip",
        "target": "src\\transformers\\models\\blip\\convert_blip_original_pytorch_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\blip",
        "target": "src\\transformers\\models\\blip\\image_processing_blip.py"
      },
      {
        "source": "src\\transformers\\models\\blip",
        "target": "src\\transformers\\models\\blip\\image_processing_blip_fast.py"
      },
      {
        "source": "src\\transformers\\models\\blip",
        "target": "src\\transformers\\models\\blip\\modeling_blip.py"
      },
      {
        "source": "src\\transformers\\models\\blip",
        "target": "src\\transformers\\models\\blip\\modeling_blip_text.py"
      },
      {
        "source": "src\\transformers\\models\\blip",
        "target": "src\\transformers\\models\\blip\\processing_blip.py"
      },
      {
        "source": "src\\transformers\\models\\blip",
        "target": "src\\transformers\\models\\blip\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\blip_2",
        "target": "src\\transformers\\models\\blip_2\\configuration_blip_2.py"
      },
      {
        "source": "src\\transformers\\models\\blip_2",
        "target": "src\\transformers\\models\\blip_2\\convert_blip_2_original_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\blip_2",
        "target": "src\\transformers\\models\\blip_2\\modeling_blip_2.py"
      },
      {
        "source": "src\\transformers\\models\\blip_2",
        "target": "src\\transformers\\models\\blip_2\\processing_blip_2.py"
      },
      {
        "source": "src\\transformers\\models\\blip_2",
        "target": "src\\transformers\\models\\blip_2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bloom",
        "target": "src\\transformers\\models\\bloom\\configuration_bloom.py"
      },
      {
        "source": "src\\transformers\\models\\bloom",
        "target": "src\\transformers\\models\\bloom\\convert_bloom_original_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\bloom",
        "target": "src\\transformers\\models\\bloom\\modeling_bloom.py"
      },
      {
        "source": "src\\transformers\\models\\bloom",
        "target": "src\\transformers\\models\\bloom\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\blt",
        "target": "src\\transformers\\models\\blt\\configuration_blt.py"
      },
      {
        "source": "src\\transformers\\models\\blt",
        "target": "src\\transformers\\models\\blt\\convert_blt_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\blt",
        "target": "src\\transformers\\models\\blt\\modeling_blt.py"
      },
      {
        "source": "src\\transformers\\models\\blt",
        "target": "src\\transformers\\models\\blt\\modular_blt.py"
      },
      {
        "source": "src\\transformers\\models\\blt",
        "target": "src\\transformers\\models\\blt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bridgetower",
        "target": "src\\transformers\\models\\bridgetower\\configuration_bridgetower.py"
      },
      {
        "source": "src\\transformers\\models\\bridgetower",
        "target": "src\\transformers\\models\\bridgetower\\image_processing_bridgetower.py"
      },
      {
        "source": "src\\transformers\\models\\bridgetower",
        "target": "src\\transformers\\models\\bridgetower\\image_processing_bridgetower_fast.py"
      },
      {
        "source": "src\\transformers\\models\\bridgetower",
        "target": "src\\transformers\\models\\bridgetower\\modeling_bridgetower.py"
      },
      {
        "source": "src\\transformers\\models\\bridgetower",
        "target": "src\\transformers\\models\\bridgetower\\processing_bridgetower.py"
      },
      {
        "source": "src\\transformers\\models\\bridgetower",
        "target": "src\\transformers\\models\\bridgetower\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\bros",
        "target": "src\\transformers\\models\\bros\\configuration_bros.py"
      },
      {
        "source": "src\\transformers\\models\\bros",
        "target": "src\\transformers\\models\\bros\\convert_bros_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\bros",
        "target": "src\\transformers\\models\\bros\\modeling_bros.py"
      },
      {
        "source": "src\\transformers\\models\\bros",
        "target": "src\\transformers\\models\\bros\\processing_bros.py"
      },
      {
        "source": "src\\transformers\\models\\bros",
        "target": "src\\transformers\\models\\bros\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\byt5",
        "target": "src\\transformers\\models\\byt5\\convert_byt5_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\byt5",
        "target": "src\\transformers\\models\\byt5\\tokenization_byt5.py"
      },
      {
        "source": "src\\transformers\\models\\byt5",
        "target": "src\\transformers\\models\\byt5\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\camembert",
        "target": "src\\transformers\\models\\camembert\\configuration_camembert.py"
      },
      {
        "source": "src\\transformers\\models\\camembert",
        "target": "src\\transformers\\models\\camembert\\modeling_camembert.py"
      },
      {
        "source": "src\\transformers\\models\\camembert",
        "target": "src\\transformers\\models\\camembert\\modular_camembert.py"
      },
      {
        "source": "src\\transformers\\models\\camembert",
        "target": "src\\transformers\\models\\camembert\\tokenization_camembert.py"
      },
      {
        "source": "src\\transformers\\models\\camembert",
        "target": "src\\transformers\\models\\camembert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\canine",
        "target": "src\\transformers\\models\\canine\\configuration_canine.py"
      },
      {
        "source": "src\\transformers\\models\\canine",
        "target": "src\\transformers\\models\\canine\\convert_canine_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\canine",
        "target": "src\\transformers\\models\\canine\\modeling_canine.py"
      },
      {
        "source": "src\\transformers\\models\\canine",
        "target": "src\\transformers\\models\\canine\\tokenization_canine.py"
      },
      {
        "source": "src\\transformers\\models\\canine",
        "target": "src\\transformers\\models\\canine\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\chameleon",
        "target": "src\\transformers\\models\\chameleon\\configuration_chameleon.py"
      },
      {
        "source": "src\\transformers\\models\\chameleon",
        "target": "src\\transformers\\models\\chameleon\\convert_chameleon_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\chameleon",
        "target": "src\\transformers\\models\\chameleon\\image_processing_chameleon.py"
      },
      {
        "source": "src\\transformers\\models\\chameleon",
        "target": "src\\transformers\\models\\chameleon\\image_processing_chameleon_fast.py"
      },
      {
        "source": "src\\transformers\\models\\chameleon",
        "target": "src\\transformers\\models\\chameleon\\modeling_chameleon.py"
      },
      {
        "source": "src\\transformers\\models\\chameleon",
        "target": "src\\transformers\\models\\chameleon\\processing_chameleon.py"
      },
      {
        "source": "src\\transformers\\models\\chameleon",
        "target": "src\\transformers\\models\\chameleon\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\chinese_clip",
        "target": "src\\transformers\\models\\chinese_clip\\configuration_chinese_clip.py"
      },
      {
        "source": "src\\transformers\\models\\chinese_clip",
        "target": "src\\transformers\\models\\chinese_clip\\convert_chinese_clip_original_pytorch_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\chinese_clip",
        "target": "src\\transformers\\models\\chinese_clip\\image_processing_chinese_clip.py"
      },
      {
        "source": "src\\transformers\\models\\chinese_clip",
        "target": "src\\transformers\\models\\chinese_clip\\image_processing_chinese_clip_fast.py"
      },
      {
        "source": "src\\transformers\\models\\chinese_clip",
        "target": "src\\transformers\\models\\chinese_clip\\modeling_chinese_clip.py"
      },
      {
        "source": "src\\transformers\\models\\chinese_clip",
        "target": "src\\transformers\\models\\chinese_clip\\processing_chinese_clip.py"
      },
      {
        "source": "src\\transformers\\models\\chinese_clip",
        "target": "src\\transformers\\models\\chinese_clip\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\clap",
        "target": "src\\transformers\\models\\clap\\configuration_clap.py"
      },
      {
        "source": "src\\transformers\\models\\clap",
        "target": "src\\transformers\\models\\clap\\convert_clap_original_pytorch_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\clap",
        "target": "src\\transformers\\models\\clap\\feature_extraction_clap.py"
      },
      {
        "source": "src\\transformers\\models\\clap",
        "target": "src\\transformers\\models\\clap\\modeling_clap.py"
      },
      {
        "source": "src\\transformers\\models\\clap",
        "target": "src\\transformers\\models\\clap\\processing_clap.py"
      },
      {
        "source": "src\\transformers\\models\\clap",
        "target": "src\\transformers\\models\\clap\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\clip",
        "target": "src\\transformers\\models\\clip\\configuration_clip.py"
      },
      {
        "source": "src\\transformers\\models\\clip",
        "target": "src\\transformers\\models\\clip\\convert_clip_original_pytorch_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\clip",
        "target": "src\\transformers\\models\\clip\\image_processing_clip.py"
      },
      {
        "source": "src\\transformers\\models\\clip",
        "target": "src\\transformers\\models\\clip\\image_processing_clip_fast.py"
      },
      {
        "source": "src\\transformers\\models\\clip",
        "target": "src\\transformers\\models\\clip\\modeling_clip.py"
      },
      {
        "source": "src\\transformers\\models\\clip",
        "target": "src\\transformers\\models\\clip\\processing_clip.py"
      },
      {
        "source": "src\\transformers\\models\\clip",
        "target": "src\\transformers\\models\\clip\\tokenization_clip.py"
      },
      {
        "source": "src\\transformers\\models\\clip",
        "target": "src\\transformers\\models\\clip\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\clipseg",
        "target": "src\\transformers\\models\\clipseg\\configuration_clipseg.py"
      },
      {
        "source": "src\\transformers\\models\\clipseg",
        "target": "src\\transformers\\models\\clipseg\\convert_clipseg_original_pytorch_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\clipseg",
        "target": "src\\transformers\\models\\clipseg\\modeling_clipseg.py"
      },
      {
        "source": "src\\transformers\\models\\clipseg",
        "target": "src\\transformers\\models\\clipseg\\processing_clipseg.py"
      },
      {
        "source": "src\\transformers\\models\\clipseg",
        "target": "src\\transformers\\models\\clipseg\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\clvp",
        "target": "src\\transformers\\models\\clvp\\configuration_clvp.py"
      },
      {
        "source": "src\\transformers\\models\\clvp",
        "target": "src\\transformers\\models\\clvp\\convert_clvp_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\clvp",
        "target": "src\\transformers\\models\\clvp\\feature_extraction_clvp.py"
      },
      {
        "source": "src\\transformers\\models\\clvp",
        "target": "src\\transformers\\models\\clvp\\modeling_clvp.py"
      },
      {
        "source": "src\\transformers\\models\\clvp",
        "target": "src\\transformers\\models\\clvp\\number_normalizer.py"
      },
      {
        "source": "src\\transformers\\models\\clvp",
        "target": "src\\transformers\\models\\clvp\\processing_clvp.py"
      },
      {
        "source": "src\\transformers\\models\\clvp",
        "target": "src\\transformers\\models\\clvp\\tokenization_clvp.py"
      },
      {
        "source": "src\\transformers\\models\\clvp",
        "target": "src\\transformers\\models\\clvp\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\codegen",
        "target": "src\\transformers\\models\\codegen\\configuration_codegen.py"
      },
      {
        "source": "src\\transformers\\models\\codegen",
        "target": "src\\transformers\\models\\codegen\\modeling_codegen.py"
      },
      {
        "source": "src\\transformers\\models\\codegen",
        "target": "src\\transformers\\models\\codegen\\tokenization_codegen.py"
      },
      {
        "source": "src\\transformers\\models\\codegen",
        "target": "src\\transformers\\models\\codegen\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\code_llama",
        "target": "src\\transformers\\models\\code_llama\\tokenization_code_llama.py"
      },
      {
        "source": "src\\transformers\\models\\code_llama",
        "target": "src\\transformers\\models\\code_llama\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\cohere",
        "target": "src\\transformers\\models\\cohere\\configuration_cohere.py"
      },
      {
        "source": "src\\transformers\\models\\cohere",
        "target": "src\\transformers\\models\\cohere\\modeling_cohere.py"
      },
      {
        "source": "src\\transformers\\models\\cohere",
        "target": "src\\transformers\\models\\cohere\\modular_cohere.py"
      },
      {
        "source": "src\\transformers\\models\\cohere",
        "target": "src\\transformers\\models\\cohere\\tokenization_cohere.py"
      },
      {
        "source": "src\\transformers\\models\\cohere",
        "target": "src\\transformers\\models\\cohere\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\cohere2",
        "target": "src\\transformers\\models\\cohere2\\configuration_cohere2.py"
      },
      {
        "source": "src\\transformers\\models\\cohere2",
        "target": "src\\transformers\\models\\cohere2\\modeling_cohere2.py"
      },
      {
        "source": "src\\transformers\\models\\cohere2",
        "target": "src\\transformers\\models\\cohere2\\modular_cohere2.py"
      },
      {
        "source": "src\\transformers\\models\\cohere2",
        "target": "src\\transformers\\models\\cohere2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\cohere2_vision",
        "target": "src\\transformers\\models\\cohere2_vision\\configuration_cohere2_vision.py"
      },
      {
        "source": "src\\transformers\\models\\cohere2_vision",
        "target": "src\\transformers\\models\\cohere2_vision\\image_processing_cohere2_vision_fast.py"
      },
      {
        "source": "src\\transformers\\models\\cohere2_vision",
        "target": "src\\transformers\\models\\cohere2_vision\\modeling_cohere2_vision.py"
      },
      {
        "source": "src\\transformers\\models\\cohere2_vision",
        "target": "src\\transformers\\models\\cohere2_vision\\modular_cohere2_vision.py"
      },
      {
        "source": "src\\transformers\\models\\cohere2_vision",
        "target": "src\\transformers\\models\\cohere2_vision\\processing_cohere2_vision.py"
      },
      {
        "source": "src\\transformers\\models\\cohere2_vision",
        "target": "src\\transformers\\models\\cohere2_vision\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\colpali",
        "target": "src\\transformers\\models\\colpali\\configuration_colpali.py"
      },
      {
        "source": "src\\transformers\\models\\colpali",
        "target": "src\\transformers\\models\\colpali\\convert_colpali_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\colpali",
        "target": "src\\transformers\\models\\colpali\\modeling_colpali.py"
      },
      {
        "source": "src\\transformers\\models\\colpali",
        "target": "src\\transformers\\models\\colpali\\modular_colpali.py"
      },
      {
        "source": "src\\transformers\\models\\colpali",
        "target": "src\\transformers\\models\\colpali\\processing_colpali.py"
      },
      {
        "source": "src\\transformers\\models\\colpali",
        "target": "src\\transformers\\models\\colpali\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\colqwen2",
        "target": "src\\transformers\\models\\colqwen2\\configuration_colqwen2.py"
      },
      {
        "source": "src\\transformers\\models\\colqwen2",
        "target": "src\\transformers\\models\\colqwen2\\convert_colqwen2_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\colqwen2",
        "target": "src\\transformers\\models\\colqwen2\\modeling_colqwen2.py"
      },
      {
        "source": "src\\transformers\\models\\colqwen2",
        "target": "src\\transformers\\models\\colqwen2\\modular_colqwen2.py"
      },
      {
        "source": "src\\transformers\\models\\colqwen2",
        "target": "src\\transformers\\models\\colqwen2\\processing_colqwen2.py"
      },
      {
        "source": "src\\transformers\\models\\colqwen2",
        "target": "src\\transformers\\models\\colqwen2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\conditional_detr",
        "target": "src\\transformers\\models\\conditional_detr\\configuration_conditional_detr.py"
      },
      {
        "source": "src\\transformers\\models\\conditional_detr",
        "target": "src\\transformers\\models\\conditional_detr\\convert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\conditional_detr",
        "target": "src\\transformers\\models\\conditional_detr\\image_processing_conditional_detr.py"
      },
      {
        "source": "src\\transformers\\models\\conditional_detr",
        "target": "src\\transformers\\models\\conditional_detr\\image_processing_conditional_detr_fast.py"
      },
      {
        "source": "src\\transformers\\models\\conditional_detr",
        "target": "src\\transformers\\models\\conditional_detr\\modeling_conditional_detr.py"
      },
      {
        "source": "src\\transformers\\models\\conditional_detr",
        "target": "src\\transformers\\models\\conditional_detr\\modular_conditional_detr.py"
      },
      {
        "source": "src\\transformers\\models\\conditional_detr",
        "target": "src\\transformers\\models\\conditional_detr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\convbert",
        "target": "src\\transformers\\models\\convbert\\configuration_convbert.py"
      },
      {
        "source": "src\\transformers\\models\\convbert",
        "target": "src\\transformers\\models\\convbert\\convert_convbert_original_tf1_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\convbert",
        "target": "src\\transformers\\models\\convbert\\modeling_convbert.py"
      },
      {
        "source": "src\\transformers\\models\\convbert",
        "target": "src\\transformers\\models\\convbert\\tokenization_convbert.py"
      },
      {
        "source": "src\\transformers\\models\\convbert",
        "target": "src\\transformers\\models\\convbert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\convnext",
        "target": "src\\transformers\\models\\convnext\\configuration_convnext.py"
      },
      {
        "source": "src\\transformers\\models\\convnext",
        "target": "src\\transformers\\models\\convnext\\convert_convnext_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\convnext",
        "target": "src\\transformers\\models\\convnext\\image_processing_convnext.py"
      },
      {
        "source": "src\\transformers\\models\\convnext",
        "target": "src\\transformers\\models\\convnext\\image_processing_convnext_fast.py"
      },
      {
        "source": "src\\transformers\\models\\convnext",
        "target": "src\\transformers\\models\\convnext\\modeling_convnext.py"
      },
      {
        "source": "src\\transformers\\models\\convnext",
        "target": "src\\transformers\\models\\convnext\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\convnextv2",
        "target": "src\\transformers\\models\\convnextv2\\configuration_convnextv2.py"
      },
      {
        "source": "src\\transformers\\models\\convnextv2",
        "target": "src\\transformers\\models\\convnextv2\\convert_convnextv2_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\convnextv2",
        "target": "src\\transformers\\models\\convnextv2\\modeling_convnextv2.py"
      },
      {
        "source": "src\\transformers\\models\\convnextv2",
        "target": "src\\transformers\\models\\convnextv2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\cpm",
        "target": "src\\transformers\\models\\cpm\\tokenization_cpm.py"
      },
      {
        "source": "src\\transformers\\models\\cpm",
        "target": "src\\transformers\\models\\cpm\\tokenization_cpm_fast.py"
      },
      {
        "source": "src\\transformers\\models\\cpm",
        "target": "src\\transformers\\models\\cpm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\cpmant",
        "target": "src\\transformers\\models\\cpmant\\configuration_cpmant.py"
      },
      {
        "source": "src\\transformers\\models\\cpmant",
        "target": "src\\transformers\\models\\cpmant\\modeling_cpmant.py"
      },
      {
        "source": "src\\transformers\\models\\cpmant",
        "target": "src\\transformers\\models\\cpmant\\tokenization_cpmant.py"
      },
      {
        "source": "src\\transformers\\models\\cpmant",
        "target": "src\\transformers\\models\\cpmant\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\csm",
        "target": "src\\transformers\\models\\csm\\configuration_csm.py"
      },
      {
        "source": "src\\transformers\\models\\csm",
        "target": "src\\transformers\\models\\csm\\convert_csm.py"
      },
      {
        "source": "src\\transformers\\models\\csm",
        "target": "src\\transformers\\models\\csm\\generation_csm.py"
      },
      {
        "source": "src\\transformers\\models\\csm",
        "target": "src\\transformers\\models\\csm\\modeling_csm.py"
      },
      {
        "source": "src\\transformers\\models\\csm",
        "target": "src\\transformers\\models\\csm\\modular_csm.py"
      },
      {
        "source": "src\\transformers\\models\\csm",
        "target": "src\\transformers\\models\\csm\\processing_csm.py"
      },
      {
        "source": "src\\transformers\\models\\csm",
        "target": "src\\transformers\\models\\csm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\ctrl",
        "target": "src\\transformers\\models\\ctrl\\configuration_ctrl.py"
      },
      {
        "source": "src\\transformers\\models\\ctrl",
        "target": "src\\transformers\\models\\ctrl\\modeling_ctrl.py"
      },
      {
        "source": "src\\transformers\\models\\ctrl",
        "target": "src\\transformers\\models\\ctrl\\tokenization_ctrl.py"
      },
      {
        "source": "src\\transformers\\models\\ctrl",
        "target": "src\\transformers\\models\\ctrl\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\cvt",
        "target": "src\\transformers\\models\\cvt\\configuration_cvt.py"
      },
      {
        "source": "src\\transformers\\models\\cvt",
        "target": "src\\transformers\\models\\cvt\\convert_cvt_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\cvt",
        "target": "src\\transformers\\models\\cvt\\modeling_cvt.py"
      },
      {
        "source": "src\\transformers\\models\\cvt",
        "target": "src\\transformers\\models\\cvt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\cwm",
        "target": "src\\transformers\\models\\cwm\\configuration_cwm.py"
      },
      {
        "source": "src\\transformers\\models\\cwm",
        "target": "src\\transformers\\models\\cwm\\modeling_cwm.py"
      },
      {
        "source": "src\\transformers\\models\\cwm",
        "target": "src\\transformers\\models\\cwm\\modular_cwm.py"
      },
      {
        "source": "src\\transformers\\models\\cwm",
        "target": "src\\transformers\\models\\cwm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dab_detr",
        "target": "src\\transformers\\models\\dab_detr\\configuration_dab_detr.py"
      },
      {
        "source": "src\\transformers\\models\\dab_detr",
        "target": "src\\transformers\\models\\dab_detr\\convert_dab_detr_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\dab_detr",
        "target": "src\\transformers\\models\\dab_detr\\modeling_dab_detr.py"
      },
      {
        "source": "src\\transformers\\models\\dab_detr",
        "target": "src\\transformers\\models\\dab_detr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dac",
        "target": "src\\transformers\\models\\dac\\configuration_dac.py"
      },
      {
        "source": "src\\transformers\\models\\dac",
        "target": "src\\transformers\\models\\dac\\convert_dac_checkpoint.py"
      },
      {
        "source": "src\\transformers\\models\\dac",
        "target": "src\\transformers\\models\\dac\\feature_extraction_dac.py"
      },
      {
        "source": "src\\transformers\\models\\dac",
        "target": "src\\transformers\\models\\dac\\modeling_dac.py"
      },
      {
        "source": "src\\transformers\\models\\dac",
        "target": "src\\transformers\\models\\dac\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\configuration_data2vec_audio.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\configuration_data2vec_text.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\configuration_data2vec_vision.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\modeling_data2vec_audio.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\modeling_data2vec_text.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\modeling_data2vec_vision.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\modular_data2vec_audio.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\modular_data2vec_text.py"
      },
      {
        "source": "src\\transformers\\models\\data2vec",
        "target": "src\\transformers\\models\\data2vec\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dbrx",
        "target": "src\\transformers\\models\\dbrx\\configuration_dbrx.py"
      },
      {
        "source": "src\\transformers\\models\\dbrx",
        "target": "src\\transformers\\models\\dbrx\\modeling_dbrx.py"
      },
      {
        "source": "src\\transformers\\models\\dbrx",
        "target": "src\\transformers\\models\\dbrx\\modular_dbrx.py"
      },
      {
        "source": "src\\transformers\\models\\dbrx",
        "target": "src\\transformers\\models\\dbrx\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\deberta",
        "target": "src\\transformers\\models\\deberta\\configuration_deberta.py"
      },
      {
        "source": "src\\transformers\\models\\deberta",
        "target": "src\\transformers\\models\\deberta\\modeling_deberta.py"
      },
      {
        "source": "src\\transformers\\models\\deberta",
        "target": "src\\transformers\\models\\deberta\\tokenization_deberta.py"
      },
      {
        "source": "src\\transformers\\models\\deberta",
        "target": "src\\transformers\\models\\deberta\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\deberta_v2",
        "target": "src\\transformers\\models\\deberta_v2\\configuration_deberta_v2.py"
      },
      {
        "source": "src\\transformers\\models\\deberta_v2",
        "target": "src\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py"
      },
      {
        "source": "src\\transformers\\models\\deberta_v2",
        "target": "src\\transformers\\models\\deberta_v2\\tokenization_deberta_v2.py"
      },
      {
        "source": "src\\transformers\\models\\deberta_v2",
        "target": "src\\transformers\\models\\deberta_v2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\decision_transformer",
        "target": "src\\transformers\\models\\decision_transformer\\configuration_decision_transformer.py"
      },
      {
        "source": "src\\transformers\\models\\decision_transformer",
        "target": "src\\transformers\\models\\decision_transformer\\modeling_decision_transformer.py"
      },
      {
        "source": "src\\transformers\\models\\decision_transformer",
        "target": "src\\transformers\\models\\decision_transformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_v2",
        "target": "src\\transformers\\models\\deepseek_v2\\configuration_deepseek_v2.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_v2",
        "target": "src\\transformers\\models\\deepseek_v2\\modeling_deepseek_v2.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_v2",
        "target": "src\\transformers\\models\\deepseek_v2\\modular_deepseek_v2.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_v2",
        "target": "src\\transformers\\models\\deepseek_v2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_v3",
        "target": "src\\transformers\\models\\deepseek_v3\\configuration_deepseek_v3.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_v3",
        "target": "src\\transformers\\models\\deepseek_v3\\modeling_deepseek_v3.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_v3",
        "target": "src\\transformers\\models\\deepseek_v3\\modular_deepseek_v3.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_v3",
        "target": "src\\transformers\\models\\deepseek_v3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl",
        "target": "src\\transformers\\models\\deepseek_vl\\configuration_deepseek_vl.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl",
        "target": "src\\transformers\\models\\deepseek_vl\\convert_deepseek_vl_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl",
        "target": "src\\transformers\\models\\deepseek_vl\\image_processing_deepseek_vl.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl",
        "target": "src\\transformers\\models\\deepseek_vl\\image_processing_deepseek_vl_fast.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl",
        "target": "src\\transformers\\models\\deepseek_vl\\modeling_deepseek_vl.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl",
        "target": "src\\transformers\\models\\deepseek_vl\\modular_deepseek_vl.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl",
        "target": "src\\transformers\\models\\deepseek_vl\\processing_deepseek_vl.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl",
        "target": "src\\transformers\\models\\deepseek_vl\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl_hybrid",
        "target": "src\\transformers\\models\\deepseek_vl_hybrid\\configuration_deepseek_vl_hybrid.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl_hybrid",
        "target": "src\\transformers\\models\\deepseek_vl_hybrid\\convert_deepseek_vl_hybrid_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl_hybrid",
        "target": "src\\transformers\\models\\deepseek_vl_hybrid\\image_processing_deepseek_vl_hybrid.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl_hybrid",
        "target": "src\\transformers\\models\\deepseek_vl_hybrid\\image_processing_deepseek_vl_hybrid_fast.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl_hybrid",
        "target": "src\\transformers\\models\\deepseek_vl_hybrid\\modeling_deepseek_vl_hybrid.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl_hybrid",
        "target": "src\\transformers\\models\\deepseek_vl_hybrid\\modular_deepseek_vl_hybrid.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl_hybrid",
        "target": "src\\transformers\\models\\deepseek_vl_hybrid\\processing_deepseek_vl_hybrid.py"
      },
      {
        "source": "src\\transformers\\models\\deepseek_vl_hybrid",
        "target": "src\\transformers\\models\\deepseek_vl_hybrid\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\deformable_detr",
        "target": "src\\transformers\\models\\deformable_detr\\configuration_deformable_detr.py"
      },
      {
        "source": "src\\transformers\\models\\deformable_detr",
        "target": "src\\transformers\\models\\deformable_detr\\convert_deformable_detr_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\deformable_detr",
        "target": "src\\transformers\\models\\deformable_detr\\image_processing_deformable_detr.py"
      },
      {
        "source": "src\\transformers\\models\\deformable_detr",
        "target": "src\\transformers\\models\\deformable_detr\\image_processing_deformable_detr_fast.py"
      },
      {
        "source": "src\\transformers\\models\\deformable_detr",
        "target": "src\\transformers\\models\\deformable_detr\\modeling_deformable_detr.py"
      },
      {
        "source": "src\\transformers\\models\\deformable_detr",
        "target": "src\\transformers\\models\\deformable_detr\\modular_deformable_detr.py"
      },
      {
        "source": "src\\transformers\\models\\deformable_detr",
        "target": "src\\transformers\\models\\deformable_detr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\deit",
        "target": "src\\transformers\\models\\deit\\configuration_deit.py"
      },
      {
        "source": "src\\transformers\\models\\deit",
        "target": "src\\transformers\\models\\deit\\convert_deit_timm_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\deit",
        "target": "src\\transformers\\models\\deit\\image_processing_deit.py"
      },
      {
        "source": "src\\transformers\\models\\deit",
        "target": "src\\transformers\\models\\deit\\image_processing_deit_fast.py"
      },
      {
        "source": "src\\transformers\\models\\deit",
        "target": "src\\transformers\\models\\deit\\modeling_deit.py"
      },
      {
        "source": "src\\transformers\\models\\deit",
        "target": "src\\transformers\\models\\deit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\deprecated",
        "target": "src\\transformers\\models\\deprecated\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\depth_anything",
        "target": "src\\transformers\\models\\depth_anything\\configuration_depth_anything.py"
      },
      {
        "source": "src\\transformers\\models\\depth_anything",
        "target": "src\\transformers\\models\\depth_anything\\convert_depth_anything_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\depth_anything",
        "target": "src\\transformers\\models\\depth_anything\\convert_distill_any_depth_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\depth_anything",
        "target": "src\\transformers\\models\\depth_anything\\modeling_depth_anything.py"
      },
      {
        "source": "src\\transformers\\models\\depth_anything",
        "target": "src\\transformers\\models\\depth_anything\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\depth_pro",
        "target": "src\\transformers\\models\\depth_pro\\configuration_depth_pro.py"
      },
      {
        "source": "src\\transformers\\models\\depth_pro",
        "target": "src\\transformers\\models\\depth_pro\\convert_depth_pro_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\depth_pro",
        "target": "src\\transformers\\models\\depth_pro\\image_processing_depth_pro.py"
      },
      {
        "source": "src\\transformers\\models\\depth_pro",
        "target": "src\\transformers\\models\\depth_pro\\image_processing_depth_pro_fast.py"
      },
      {
        "source": "src\\transformers\\models\\depth_pro",
        "target": "src\\transformers\\models\\depth_pro\\modeling_depth_pro.py"
      },
      {
        "source": "src\\transformers\\models\\depth_pro",
        "target": "src\\transformers\\models\\depth_pro\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\detr",
        "target": "src\\transformers\\models\\detr\\configuration_detr.py"
      },
      {
        "source": "src\\transformers\\models\\detr",
        "target": "src\\transformers\\models\\detr\\convert_detr_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\detr",
        "target": "src\\transformers\\models\\detr\\convert_detr_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\detr",
        "target": "src\\transformers\\models\\detr\\image_processing_detr.py"
      },
      {
        "source": "src\\transformers\\models\\detr",
        "target": "src\\transformers\\models\\detr\\image_processing_detr_fast.py"
      },
      {
        "source": "src\\transformers\\models\\detr",
        "target": "src\\transformers\\models\\detr\\modeling_detr.py"
      },
      {
        "source": "src\\transformers\\models\\detr",
        "target": "src\\transformers\\models\\detr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dia",
        "target": "src\\transformers\\models\\dia\\configuration_dia.py"
      },
      {
        "source": "src\\transformers\\models\\dia",
        "target": "src\\transformers\\models\\dia\\convert_dia_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\dia",
        "target": "src\\transformers\\models\\dia\\feature_extraction_dia.py"
      },
      {
        "source": "src\\transformers\\models\\dia",
        "target": "src\\transformers\\models\\dia\\generation_dia.py"
      },
      {
        "source": "src\\transformers\\models\\dia",
        "target": "src\\transformers\\models\\dia\\modeling_dia.py"
      },
      {
        "source": "src\\transformers\\models\\dia",
        "target": "src\\transformers\\models\\dia\\modular_dia.py"
      },
      {
        "source": "src\\transformers\\models\\dia",
        "target": "src\\transformers\\models\\dia\\processing_dia.py"
      },
      {
        "source": "src\\transformers\\models\\dia",
        "target": "src\\transformers\\models\\dia\\tokenization_dia.py"
      },
      {
        "source": "src\\transformers\\models\\dia",
        "target": "src\\transformers\\models\\dia\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dialogpt",
        "target": "src\\transformers\\models\\dialogpt\\convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\dialogpt",
        "target": "src\\transformers\\models\\dialogpt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\diffllama",
        "target": "src\\transformers\\models\\diffllama\\configuration_diffllama.py"
      },
      {
        "source": "src\\transformers\\models\\diffllama",
        "target": "src\\transformers\\models\\diffllama\\modeling_diffllama.py"
      },
      {
        "source": "src\\transformers\\models\\diffllama",
        "target": "src\\transformers\\models\\diffllama\\modular_diffllama.py"
      },
      {
        "source": "src\\transformers\\models\\diffllama",
        "target": "src\\transformers\\models\\diffllama\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dinat",
        "target": "src\\transformers\\models\\dinat\\configuration_dinat.py"
      },
      {
        "source": "src\\transformers\\models\\dinat",
        "target": "src\\transformers\\models\\dinat\\modeling_dinat.py"
      },
      {
        "source": "src\\transformers\\models\\dinat",
        "target": "src\\transformers\\models\\dinat\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dinov2",
        "target": "src\\transformers\\models\\dinov2\\configuration_dinov2.py"
      },
      {
        "source": "src\\transformers\\models\\dinov2",
        "target": "src\\transformers\\models\\dinov2\\convert_dinov2_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\dinov2",
        "target": "src\\transformers\\models\\dinov2\\modeling_dinov2.py"
      },
      {
        "source": "src\\transformers\\models\\dinov2",
        "target": "src\\transformers\\models\\dinov2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dinov2_with_registers",
        "target": "src\\transformers\\models\\dinov2_with_registers\\configuration_dinov2_with_registers.py"
      },
      {
        "source": "src\\transformers\\models\\dinov2_with_registers",
        "target": "src\\transformers\\models\\dinov2_with_registers\\convert_dinov2_with_registers_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\dinov2_with_registers",
        "target": "src\\transformers\\models\\dinov2_with_registers\\modeling_dinov2_with_registers.py"
      },
      {
        "source": "src\\transformers\\models\\dinov2_with_registers",
        "target": "src\\transformers\\models\\dinov2_with_registers\\modular_dinov2_with_registers.py"
      },
      {
        "source": "src\\transformers\\models\\dinov2_with_registers",
        "target": "src\\transformers\\models\\dinov2_with_registers\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dinov3_convnext",
        "target": "src\\transformers\\models\\dinov3_convnext\\configuration_dinov3_convnext.py"
      },
      {
        "source": "src\\transformers\\models\\dinov3_convnext",
        "target": "src\\transformers\\models\\dinov3_convnext\\convert_dinov3_convnext_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\dinov3_convnext",
        "target": "src\\transformers\\models\\dinov3_convnext\\modeling_dinov3_convnext.py"
      },
      {
        "source": "src\\transformers\\models\\dinov3_convnext",
        "target": "src\\transformers\\models\\dinov3_convnext\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dinov3_vit",
        "target": "src\\transformers\\models\\dinov3_vit\\configuration_dinov3_vit.py"
      },
      {
        "source": "src\\transformers\\models\\dinov3_vit",
        "target": "src\\transformers\\models\\dinov3_vit\\convert_dinov3_vit_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\dinov3_vit",
        "target": "src\\transformers\\models\\dinov3_vit\\image_processing_dinov3_vit_fast.py"
      },
      {
        "source": "src\\transformers\\models\\dinov3_vit",
        "target": "src\\transformers\\models\\dinov3_vit\\modeling_dinov3_vit.py"
      },
      {
        "source": "src\\transformers\\models\\dinov3_vit",
        "target": "src\\transformers\\models\\dinov3_vit\\modular_dinov3_vit.py"
      },
      {
        "source": "src\\transformers\\models\\dinov3_vit",
        "target": "src\\transformers\\models\\dinov3_vit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\distilbert",
        "target": "src\\transformers\\models\\distilbert\\configuration_distilbert.py"
      },
      {
        "source": "src\\transformers\\models\\distilbert",
        "target": "src\\transformers\\models\\distilbert\\modeling_distilbert.py"
      },
      {
        "source": "src\\transformers\\models\\distilbert",
        "target": "src\\transformers\\models\\distilbert\\tokenization_distilbert.py"
      },
      {
        "source": "src\\transformers\\models\\distilbert",
        "target": "src\\transformers\\models\\distilbert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dit",
        "target": "src\\transformers\\models\\dit\\convert_dit_unilm_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\dit",
        "target": "src\\transformers\\models\\dit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\doge",
        "target": "src\\transformers\\models\\doge\\configuration_doge.py"
      },
      {
        "source": "src\\transformers\\models\\doge",
        "target": "src\\transformers\\models\\doge\\convert_doge_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\doge",
        "target": "src\\transformers\\models\\doge\\modeling_doge.py"
      },
      {
        "source": "src\\transformers\\models\\doge",
        "target": "src\\transformers\\models\\doge\\modular_doge.py"
      },
      {
        "source": "src\\transformers\\models\\doge",
        "target": "src\\transformers\\models\\doge\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\donut",
        "target": "src\\transformers\\models\\donut\\configuration_donut_swin.py"
      },
      {
        "source": "src\\transformers\\models\\donut",
        "target": "src\\transformers\\models\\donut\\convert_donut_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\donut",
        "target": "src\\transformers\\models\\donut\\image_processing_donut.py"
      },
      {
        "source": "src\\transformers\\models\\donut",
        "target": "src\\transformers\\models\\donut\\image_processing_donut_fast.py"
      },
      {
        "source": "src\\transformers\\models\\donut",
        "target": "src\\transformers\\models\\donut\\modeling_donut_swin.py"
      },
      {
        "source": "src\\transformers\\models\\donut",
        "target": "src\\transformers\\models\\donut\\processing_donut.py"
      },
      {
        "source": "src\\transformers\\models\\donut",
        "target": "src\\transformers\\models\\donut\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dots1",
        "target": "src\\transformers\\models\\dots1\\configuration_dots1.py"
      },
      {
        "source": "src\\transformers\\models\\dots1",
        "target": "src\\transformers\\models\\dots1\\modeling_dots1.py"
      },
      {
        "source": "src\\transformers\\models\\dots1",
        "target": "src\\transformers\\models\\dots1\\modular_dots1.py"
      },
      {
        "source": "src\\transformers\\models\\dots1",
        "target": "src\\transformers\\models\\dots1\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dpr",
        "target": "src\\transformers\\models\\dpr\\configuration_dpr.py"
      },
      {
        "source": "src\\transformers\\models\\dpr",
        "target": "src\\transformers\\models\\dpr\\convert_dpr_original_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\dpr",
        "target": "src\\transformers\\models\\dpr\\modeling_dpr.py"
      },
      {
        "source": "src\\transformers\\models\\dpr",
        "target": "src\\transformers\\models\\dpr\\tokenization_dpr.py"
      },
      {
        "source": "src\\transformers\\models\\dpr",
        "target": "src\\transformers\\models\\dpr\\tokenization_dpr_fast.py"
      },
      {
        "source": "src\\transformers\\models\\dpr",
        "target": "src\\transformers\\models\\dpr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\configuration_dpt.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\convert_dinov2_depth_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\convert_dpt_beit_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\convert_dpt_hybrid_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\convert_dpt_swinv2_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\convert_dpt_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\image_processing_dpt.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\image_processing_dpt_fast.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\modeling_dpt.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\modular_dpt.py"
      },
      {
        "source": "src\\transformers\\models\\dpt",
        "target": "src\\transformers\\models\\dpt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\d_fine",
        "target": "src\\transformers\\models\\d_fine\\configuration_d_fine.py"
      },
      {
        "source": "src\\transformers\\models\\d_fine",
        "target": "src\\transformers\\models\\d_fine\\convert_d_fine_original_pytorch_checkpoint_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\d_fine",
        "target": "src\\transformers\\models\\d_fine\\modeling_d_fine.py"
      },
      {
        "source": "src\\transformers\\models\\d_fine",
        "target": "src\\transformers\\models\\d_fine\\modular_d_fine.py"
      },
      {
        "source": "src\\transformers\\models\\d_fine",
        "target": "src\\transformers\\models\\d_fine\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\edgetam",
        "target": "src\\transformers\\models\\edgetam\\configuration_edgetam.py"
      },
      {
        "source": "src\\transformers\\models\\edgetam",
        "target": "src\\transformers\\models\\edgetam\\convert_edgetam_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\edgetam",
        "target": "src\\transformers\\models\\edgetam\\modeling_edgetam.py"
      },
      {
        "source": "src\\transformers\\models\\edgetam",
        "target": "src\\transformers\\models\\edgetam\\modular_edgetam.py"
      },
      {
        "source": "src\\transformers\\models\\edgetam",
        "target": "src\\transformers\\models\\edgetam\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\edgetam_video",
        "target": "src\\transformers\\models\\edgetam_video\\configuration_edgetam_video.py"
      },
      {
        "source": "src\\transformers\\models\\edgetam_video",
        "target": "src\\transformers\\models\\edgetam_video\\convert_edgetam_video_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\edgetam_video",
        "target": "src\\transformers\\models\\edgetam_video\\modeling_edgetam_video.py"
      },
      {
        "source": "src\\transformers\\models\\edgetam_video",
        "target": "src\\transformers\\models\\edgetam_video\\modular_edgetam_video.py"
      },
      {
        "source": "src\\transformers\\models\\edgetam_video",
        "target": "src\\transformers\\models\\edgetam_video\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\efficientloftr",
        "target": "src\\transformers\\models\\efficientloftr\\configuration_efficientloftr.py"
      },
      {
        "source": "src\\transformers\\models\\efficientloftr",
        "target": "src\\transformers\\models\\efficientloftr\\convert_efficientloftr_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\efficientloftr",
        "target": "src\\transformers\\models\\efficientloftr\\image_processing_efficientloftr.py"
      },
      {
        "source": "src\\transformers\\models\\efficientloftr",
        "target": "src\\transformers\\models\\efficientloftr\\image_processing_efficientloftr_fast.py"
      },
      {
        "source": "src\\transformers\\models\\efficientloftr",
        "target": "src\\transformers\\models\\efficientloftr\\modeling_efficientloftr.py"
      },
      {
        "source": "src\\transformers\\models\\efficientloftr",
        "target": "src\\transformers\\models\\efficientloftr\\modular_efficientloftr.py"
      },
      {
        "source": "src\\transformers\\models\\efficientloftr",
        "target": "src\\transformers\\models\\efficientloftr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\efficientnet",
        "target": "src\\transformers\\models\\efficientnet\\configuration_efficientnet.py"
      },
      {
        "source": "src\\transformers\\models\\efficientnet",
        "target": "src\\transformers\\models\\efficientnet\\convert_efficientnet_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\efficientnet",
        "target": "src\\transformers\\models\\efficientnet\\image_processing_efficientnet.py"
      },
      {
        "source": "src\\transformers\\models\\efficientnet",
        "target": "src\\transformers\\models\\efficientnet\\image_processing_efficientnet_fast.py"
      },
      {
        "source": "src\\transformers\\models\\efficientnet",
        "target": "src\\transformers\\models\\efficientnet\\modeling_efficientnet.py"
      },
      {
        "source": "src\\transformers\\models\\efficientnet",
        "target": "src\\transformers\\models\\efficientnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\electra",
        "target": "src\\transformers\\models\\electra\\configuration_electra.py"
      },
      {
        "source": "src\\transformers\\models\\electra",
        "target": "src\\transformers\\models\\electra\\convert_electra_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\electra",
        "target": "src\\transformers\\models\\electra\\modeling_electra.py"
      },
      {
        "source": "src\\transformers\\models\\electra",
        "target": "src\\transformers\\models\\electra\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\emu3",
        "target": "src\\transformers\\models\\emu3\\configuration_emu3.py"
      },
      {
        "source": "src\\transformers\\models\\emu3",
        "target": "src\\transformers\\models\\emu3\\convert_emu3_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\emu3",
        "target": "src\\transformers\\models\\emu3\\image_processing_emu3.py"
      },
      {
        "source": "src\\transformers\\models\\emu3",
        "target": "src\\transformers\\models\\emu3\\modeling_emu3.py"
      },
      {
        "source": "src\\transformers\\models\\emu3",
        "target": "src\\transformers\\models\\emu3\\modular_emu3.py"
      },
      {
        "source": "src\\transformers\\models\\emu3",
        "target": "src\\transformers\\models\\emu3\\processing_emu3.py"
      },
      {
        "source": "src\\transformers\\models\\emu3",
        "target": "src\\transformers\\models\\emu3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\encodec",
        "target": "src\\transformers\\models\\encodec\\configuration_encodec.py"
      },
      {
        "source": "src\\transformers\\models\\encodec",
        "target": "src\\transformers\\models\\encodec\\convert_encodec_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\encodec",
        "target": "src\\transformers\\models\\encodec\\feature_extraction_encodec.py"
      },
      {
        "source": "src\\transformers\\models\\encodec",
        "target": "src\\transformers\\models\\encodec\\modeling_encodec.py"
      },
      {
        "source": "src\\transformers\\models\\encodec",
        "target": "src\\transformers\\models\\encodec\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\encoder_decoder",
        "target": "src\\transformers\\models\\encoder_decoder\\configuration_encoder_decoder.py"
      },
      {
        "source": "src\\transformers\\models\\encoder_decoder",
        "target": "src\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py"
      },
      {
        "source": "src\\transformers\\models\\encoder_decoder",
        "target": "src\\transformers\\models\\encoder_decoder\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\eomt",
        "target": "src\\transformers\\models\\eomt\\configuration_eomt.py"
      },
      {
        "source": "src\\transformers\\models\\eomt",
        "target": "src\\transformers\\models\\eomt\\convert_eomt_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\eomt",
        "target": "src\\transformers\\models\\eomt\\image_processing_eomt.py"
      },
      {
        "source": "src\\transformers\\models\\eomt",
        "target": "src\\transformers\\models\\eomt\\image_processing_eomt_fast.py"
      },
      {
        "source": "src\\transformers\\models\\eomt",
        "target": "src\\transformers\\models\\eomt\\modeling_eomt.py"
      },
      {
        "source": "src\\transformers\\models\\eomt",
        "target": "src\\transformers\\models\\eomt\\modular_eomt.py"
      },
      {
        "source": "src\\transformers\\models\\eomt",
        "target": "src\\transformers\\models\\eomt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\ernie",
        "target": "src\\transformers\\models\\ernie\\configuration_ernie.py"
      },
      {
        "source": "src\\transformers\\models\\ernie",
        "target": "src\\transformers\\models\\ernie\\modeling_ernie.py"
      },
      {
        "source": "src\\transformers\\models\\ernie",
        "target": "src\\transformers\\models\\ernie\\modular_ernie.py"
      },
      {
        "source": "src\\transformers\\models\\ernie",
        "target": "src\\transformers\\models\\ernie\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5",
        "target": "src\\transformers\\models\\ernie4_5\\configuration_ernie4_5.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5",
        "target": "src\\transformers\\models\\ernie4_5\\convert_ernie4_5_tokenizer.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5",
        "target": "src\\transformers\\models\\ernie4_5\\modeling_ernie4_5.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5",
        "target": "src\\transformers\\models\\ernie4_5\\modular_ernie4_5.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5",
        "target": "src\\transformers\\models\\ernie4_5\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_moe",
        "target": "src\\transformers\\models\\ernie4_5_moe\\configuration_ernie4_5_moe.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_moe",
        "target": "src\\transformers\\models\\ernie4_5_moe\\modeling_ernie4_5_moe.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_moe",
        "target": "src\\transformers\\models\\ernie4_5_moe\\modular_ernie4_5_moe.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_moe",
        "target": "src\\transformers\\models\\ernie4_5_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_vl_moe",
        "target": "src\\transformers\\models\\ernie4_5_vl_moe\\configuration_ernie4_5_vl_moe.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_vl_moe",
        "target": "src\\transformers\\models\\ernie4_5_vl_moe\\convert_ernie4_5_vl_moe_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_vl_moe",
        "target": "src\\transformers\\models\\ernie4_5_vl_moe\\image_processing_ernie4_5_vl_moe.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_vl_moe",
        "target": "src\\transformers\\models\\ernie4_5_vl_moe\\image_processing_ernie4_5_vl_moe_fast.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_vl_moe",
        "target": "src\\transformers\\models\\ernie4_5_vl_moe\\modeling_ernie4_5_vl_moe.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_vl_moe",
        "target": "src\\transformers\\models\\ernie4_5_vl_moe\\modular_ernie4_5_vl_moe.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_vl_moe",
        "target": "src\\transformers\\models\\ernie4_5_vl_moe\\processing_ernie4_5_vl_moe.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_vl_moe",
        "target": "src\\transformers\\models\\ernie4_5_vl_moe\\video_processing_ernie4_5_vl_moe.py"
      },
      {
        "source": "src\\transformers\\models\\ernie4_5_vl_moe",
        "target": "src\\transformers\\models\\ernie4_5_vl_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\esm",
        "target": "src\\transformers\\models\\esm\\configuration_esm.py"
      },
      {
        "source": "src\\transformers\\models\\esm",
        "target": "src\\transformers\\models\\esm\\convert_esm.py"
      },
      {
        "source": "src\\transformers\\models\\esm",
        "target": "src\\transformers\\models\\esm\\modeling_esm.py"
      },
      {
        "source": "src\\transformers\\models\\esm",
        "target": "src\\transformers\\models\\esm\\modeling_esmfold.py"
      },
      {
        "source": "src\\transformers\\models\\esm",
        "target": "src\\transformers\\models\\esm\\openfold_utils"
      },
      {
        "source": "src\\transformers\\models\\esm",
        "target": "src\\transformers\\models\\esm\\tokenization_esm.py"
      },
      {
        "source": "src\\transformers\\models\\esm",
        "target": "src\\transformers\\models\\esm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\evolla",
        "target": "src\\transformers\\models\\evolla\\configuration_evolla.py"
      },
      {
        "source": "src\\transformers\\models\\evolla",
        "target": "src\\transformers\\models\\evolla\\modeling_evolla.py"
      },
      {
        "source": "src\\transformers\\models\\evolla",
        "target": "src\\transformers\\models\\evolla\\modular_evolla.py"
      },
      {
        "source": "src\\transformers\\models\\evolla",
        "target": "src\\transformers\\models\\evolla\\processing_evolla.py"
      },
      {
        "source": "src\\transformers\\models\\evolla",
        "target": "src\\transformers\\models\\evolla\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\exaone4",
        "target": "src\\transformers\\models\\exaone4\\configuration_exaone4.py"
      },
      {
        "source": "src\\transformers\\models\\exaone4",
        "target": "src\\transformers\\models\\exaone4\\modeling_exaone4.py"
      },
      {
        "source": "src\\transformers\\models\\exaone4",
        "target": "src\\transformers\\models\\exaone4\\modular_exaone4.py"
      },
      {
        "source": "src\\transformers\\models\\exaone4",
        "target": "src\\transformers\\models\\exaone4\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\falcon",
        "target": "src\\transformers\\models\\falcon\\configuration_falcon.py"
      },
      {
        "source": "src\\transformers\\models\\falcon",
        "target": "src\\transformers\\models\\falcon\\convert_custom_code_checkpoint.py"
      },
      {
        "source": "src\\transformers\\models\\falcon",
        "target": "src\\transformers\\models\\falcon\\modeling_falcon.py"
      },
      {
        "source": "src\\transformers\\models\\falcon",
        "target": "src\\transformers\\models\\falcon\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\falcon_h1",
        "target": "src\\transformers\\models\\falcon_h1\\configuration_falcon_h1.py"
      },
      {
        "source": "src\\transformers\\models\\falcon_h1",
        "target": "src\\transformers\\models\\falcon_h1\\convert_mamba_ssm_checkpoint.py"
      },
      {
        "source": "src\\transformers\\models\\falcon_h1",
        "target": "src\\transformers\\models\\falcon_h1\\modeling_falcon_h1.py"
      },
      {
        "source": "src\\transformers\\models\\falcon_h1",
        "target": "src\\transformers\\models\\falcon_h1\\modular_falcon_h1.py"
      },
      {
        "source": "src\\transformers\\models\\falcon_h1",
        "target": "src\\transformers\\models\\falcon_h1\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\falcon_mamba",
        "target": "src\\transformers\\models\\falcon_mamba\\configuration_falcon_mamba.py"
      },
      {
        "source": "src\\transformers\\models\\falcon_mamba",
        "target": "src\\transformers\\models\\falcon_mamba\\modeling_falcon_mamba.py"
      },
      {
        "source": "src\\transformers\\models\\falcon_mamba",
        "target": "src\\transformers\\models\\falcon_mamba\\modular_falcon_mamba.py"
      },
      {
        "source": "src\\transformers\\models\\falcon_mamba",
        "target": "src\\transformers\\models\\falcon_mamba\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\fastspeech2_conformer",
        "target": "src\\transformers\\models\\fastspeech2_conformer\\configuration_fastspeech2_conformer.py"
      },
      {
        "source": "src\\transformers\\models\\fastspeech2_conformer",
        "target": "src\\transformers\\models\\fastspeech2_conformer\\convert_fastspeech2_conformer_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\fastspeech2_conformer",
        "target": "src\\transformers\\models\\fastspeech2_conformer\\convert_hifigan.py"
      },
      {
        "source": "src\\transformers\\models\\fastspeech2_conformer",
        "target": "src\\transformers\\models\\fastspeech2_conformer\\convert_model_with_hifigan.py"
      },
      {
        "source": "src\\transformers\\models\\fastspeech2_conformer",
        "target": "src\\transformers\\models\\fastspeech2_conformer\\modeling_fastspeech2_conformer.py"
      },
      {
        "source": "src\\transformers\\models\\fastspeech2_conformer",
        "target": "src\\transformers\\models\\fastspeech2_conformer\\tokenization_fastspeech2_conformer.py"
      },
      {
        "source": "src\\transformers\\models\\fastspeech2_conformer",
        "target": "src\\transformers\\models\\fastspeech2_conformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\fast_vlm",
        "target": "src\\transformers\\models\\fast_vlm\\configuration_fast_vlm.py"
      },
      {
        "source": "src\\transformers\\models\\fast_vlm",
        "target": "src\\transformers\\models\\fast_vlm\\convert_fastvlm_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\fast_vlm",
        "target": "src\\transformers\\models\\fast_vlm\\modeling_fast_vlm.py"
      },
      {
        "source": "src\\transformers\\models\\fast_vlm",
        "target": "src\\transformers\\models\\fast_vlm\\modular_fast_vlm.py"
      },
      {
        "source": "src\\transformers\\models\\fast_vlm",
        "target": "src\\transformers\\models\\fast_vlm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\flaubert",
        "target": "src\\transformers\\models\\flaubert\\configuration_flaubert.py"
      },
      {
        "source": "src\\transformers\\models\\flaubert",
        "target": "src\\transformers\\models\\flaubert\\modeling_flaubert.py"
      },
      {
        "source": "src\\transformers\\models\\flaubert",
        "target": "src\\transformers\\models\\flaubert\\tokenization_flaubert.py"
      },
      {
        "source": "src\\transformers\\models\\flaubert",
        "target": "src\\transformers\\models\\flaubert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\flava",
        "target": "src\\transformers\\models\\flava\\configuration_flava.py"
      },
      {
        "source": "src\\transformers\\models\\flava",
        "target": "src\\transformers\\models\\flava\\convert_dalle_to_flava_codebook.py"
      },
      {
        "source": "src\\transformers\\models\\flava",
        "target": "src\\transformers\\models\\flava\\convert_flava_original_pytorch_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\flava",
        "target": "src\\transformers\\models\\flava\\image_processing_flava.py"
      },
      {
        "source": "src\\transformers\\models\\flava",
        "target": "src\\transformers\\models\\flava\\image_processing_flava_fast.py"
      },
      {
        "source": "src\\transformers\\models\\flava",
        "target": "src\\transformers\\models\\flava\\modeling_flava.py"
      },
      {
        "source": "src\\transformers\\models\\flava",
        "target": "src\\transformers\\models\\flava\\processing_flava.py"
      },
      {
        "source": "src\\transformers\\models\\flava",
        "target": "src\\transformers\\models\\flava\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\flex_olmo",
        "target": "src\\transformers\\models\\flex_olmo\\configuration_flex_olmo.py"
      },
      {
        "source": "src\\transformers\\models\\flex_olmo",
        "target": "src\\transformers\\models\\flex_olmo\\modeling_flex_olmo.py"
      },
      {
        "source": "src\\transformers\\models\\flex_olmo",
        "target": "src\\transformers\\models\\flex_olmo\\modular_flex_olmo.py"
      },
      {
        "source": "src\\transformers\\models\\flex_olmo",
        "target": "src\\transformers\\models\\flex_olmo\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\florence2",
        "target": "src\\transformers\\models\\florence2\\configuration_florence2.py"
      },
      {
        "source": "src\\transformers\\models\\florence2",
        "target": "src\\transformers\\models\\florence2\\convert_florence2_original_pytorch_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\florence2",
        "target": "src\\transformers\\models\\florence2\\modeling_florence2.py"
      },
      {
        "source": "src\\transformers\\models\\florence2",
        "target": "src\\transformers\\models\\florence2\\modular_florence2.py"
      },
      {
        "source": "src\\transformers\\models\\florence2",
        "target": "src\\transformers\\models\\florence2\\processing_florence2.py"
      },
      {
        "source": "src\\transformers\\models\\florence2",
        "target": "src\\transformers\\models\\florence2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\fnet",
        "target": "src\\transformers\\models\\fnet\\configuration_fnet.py"
      },
      {
        "source": "src\\transformers\\models\\fnet",
        "target": "src\\transformers\\models\\fnet\\convert_fnet_original_flax_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\fnet",
        "target": "src\\transformers\\models\\fnet\\modeling_fnet.py"
      },
      {
        "source": "src\\transformers\\models\\fnet",
        "target": "src\\transformers\\models\\fnet\\tokenization_fnet.py"
      },
      {
        "source": "src\\transformers\\models\\fnet",
        "target": "src\\transformers\\models\\fnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\focalnet",
        "target": "src\\transformers\\models\\focalnet\\configuration_focalnet.py"
      },
      {
        "source": "src\\transformers\\models\\focalnet",
        "target": "src\\transformers\\models\\focalnet\\convert_focalnet_to_hf_format.py"
      },
      {
        "source": "src\\transformers\\models\\focalnet",
        "target": "src\\transformers\\models\\focalnet\\modeling_focalnet.py"
      },
      {
        "source": "src\\transformers\\models\\focalnet",
        "target": "src\\transformers\\models\\focalnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\fsmt",
        "target": "src\\transformers\\models\\fsmt\\configuration_fsmt.py"
      },
      {
        "source": "src\\transformers\\models\\fsmt",
        "target": "src\\transformers\\models\\fsmt\\convert_fsmt_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\fsmt",
        "target": "src\\transformers\\models\\fsmt\\modeling_fsmt.py"
      },
      {
        "source": "src\\transformers\\models\\fsmt",
        "target": "src\\transformers\\models\\fsmt\\tokenization_fsmt.py"
      },
      {
        "source": "src\\transformers\\models\\fsmt",
        "target": "src\\transformers\\models\\fsmt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\funnel",
        "target": "src\\transformers\\models\\funnel\\configuration_funnel.py"
      },
      {
        "source": "src\\transformers\\models\\funnel",
        "target": "src\\transformers\\models\\funnel\\convert_funnel_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\funnel",
        "target": "src\\transformers\\models\\funnel\\modeling_funnel.py"
      },
      {
        "source": "src\\transformers\\models\\funnel",
        "target": "src\\transformers\\models\\funnel\\tokenization_funnel.py"
      },
      {
        "source": "src\\transformers\\models\\funnel",
        "target": "src\\transformers\\models\\funnel\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\fuyu",
        "target": "src\\transformers\\models\\fuyu\\configuration_fuyu.py"
      },
      {
        "source": "src\\transformers\\models\\fuyu",
        "target": "src\\transformers\\models\\fuyu\\convert_fuyu_model_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\fuyu",
        "target": "src\\transformers\\models\\fuyu\\image_processing_fuyu.py"
      },
      {
        "source": "src\\transformers\\models\\fuyu",
        "target": "src\\transformers\\models\\fuyu\\image_processing_fuyu_fast.py"
      },
      {
        "source": "src\\transformers\\models\\fuyu",
        "target": "src\\transformers\\models\\fuyu\\modeling_fuyu.py"
      },
      {
        "source": "src\\transformers\\models\\fuyu",
        "target": "src\\transformers\\models\\fuyu\\processing_fuyu.py"
      },
      {
        "source": "src\\transformers\\models\\fuyu",
        "target": "src\\transformers\\models\\fuyu\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gemma",
        "target": "src\\transformers\\models\\gemma\\configuration_gemma.py"
      },
      {
        "source": "src\\transformers\\models\\gemma",
        "target": "src\\transformers\\models\\gemma\\convert_gemma_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\gemma",
        "target": "src\\transformers\\models\\gemma\\modeling_gemma.py"
      },
      {
        "source": "src\\transformers\\models\\gemma",
        "target": "src\\transformers\\models\\gemma\\modular_gemma.py"
      },
      {
        "source": "src\\transformers\\models\\gemma",
        "target": "src\\transformers\\models\\gemma\\tokenization_gemma.py"
      },
      {
        "source": "src\\transformers\\models\\gemma",
        "target": "src\\transformers\\models\\gemma\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gemma2",
        "target": "src\\transformers\\models\\gemma2\\configuration_gemma2.py"
      },
      {
        "source": "src\\transformers\\models\\gemma2",
        "target": "src\\transformers\\models\\gemma2\\convert_gemma2_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\gemma2",
        "target": "src\\transformers\\models\\gemma2\\modeling_gemma2.py"
      },
      {
        "source": "src\\transformers\\models\\gemma2",
        "target": "src\\transformers\\models\\gemma2\\modular_gemma2.py"
      },
      {
        "source": "src\\transformers\\models\\gemma2",
        "target": "src\\transformers\\models\\gemma2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3",
        "target": "src\\transformers\\models\\gemma3\\configuration_gemma3.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3",
        "target": "src\\transformers\\models\\gemma3\\convert_gemma3_weights.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3",
        "target": "src\\transformers\\models\\gemma3\\image_processing_gemma3.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3",
        "target": "src\\transformers\\models\\gemma3\\image_processing_gemma3_fast.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3",
        "target": "src\\transformers\\models\\gemma3\\modeling_gemma3.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3",
        "target": "src\\transformers\\models\\gemma3\\modular_gemma3.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3",
        "target": "src\\transformers\\models\\gemma3\\processing_gemma3.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3",
        "target": "src\\transformers\\models\\gemma3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3n",
        "target": "src\\transformers\\models\\gemma3n\\configuration_gemma3n.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3n",
        "target": "src\\transformers\\models\\gemma3n\\convert_gemma3n_weights.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3n",
        "target": "src\\transformers\\models\\gemma3n\\feature_extraction_gemma3n.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3n",
        "target": "src\\transformers\\models\\gemma3n\\modeling_gemma3n.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3n",
        "target": "src\\transformers\\models\\gemma3n\\modular_gemma3n.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3n",
        "target": "src\\transformers\\models\\gemma3n\\processing_gemma3n.py"
      },
      {
        "source": "src\\transformers\\models\\gemma3n",
        "target": "src\\transformers\\models\\gemma3n\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\git",
        "target": "src\\transformers\\models\\git\\configuration_git.py"
      },
      {
        "source": "src\\transformers\\models\\git",
        "target": "src\\transformers\\models\\git\\convert_git_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\git",
        "target": "src\\transformers\\models\\git\\modeling_git.py"
      },
      {
        "source": "src\\transformers\\models\\git",
        "target": "src\\transformers\\models\\git\\processing_git.py"
      },
      {
        "source": "src\\transformers\\models\\git",
        "target": "src\\transformers\\models\\git\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glm",
        "target": "src\\transformers\\models\\glm\\configuration_glm.py"
      },
      {
        "source": "src\\transformers\\models\\glm",
        "target": "src\\transformers\\models\\glm\\convert_glm_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\glm",
        "target": "src\\transformers\\models\\glm\\modeling_glm.py"
      },
      {
        "source": "src\\transformers\\models\\glm",
        "target": "src\\transformers\\models\\glm\\modular_glm.py"
      },
      {
        "source": "src\\transformers\\models\\glm",
        "target": "src\\transformers\\models\\glm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glm4",
        "target": "src\\transformers\\models\\glm4\\configuration_glm4.py"
      },
      {
        "source": "src\\transformers\\models\\glm4",
        "target": "src\\transformers\\models\\glm4\\convert_glm4_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\glm4",
        "target": "src\\transformers\\models\\glm4\\modeling_glm4.py"
      },
      {
        "source": "src\\transformers\\models\\glm4",
        "target": "src\\transformers\\models\\glm4\\modular_glm4.py"
      },
      {
        "source": "src\\transformers\\models\\glm4",
        "target": "src\\transformers\\models\\glm4\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glm46v",
        "target": "src\\transformers\\models\\glm46v\\configuration_glm46v.py"
      },
      {
        "source": "src\\transformers\\models\\glm46v",
        "target": "src\\transformers\\models\\glm46v\\image_processing_glm46v.py"
      },
      {
        "source": "src\\transformers\\models\\glm46v",
        "target": "src\\transformers\\models\\glm46v\\image_processing_glm46v_fast.py"
      },
      {
        "source": "src\\transformers\\models\\glm46v",
        "target": "src\\transformers\\models\\glm46v\\modeling_glm46v.py"
      },
      {
        "source": "src\\transformers\\models\\glm46v",
        "target": "src\\transformers\\models\\glm46v\\modular_glm46v.py"
      },
      {
        "source": "src\\transformers\\models\\glm46v",
        "target": "src\\transformers\\models\\glm46v\\processing_glm46v.py"
      },
      {
        "source": "src\\transformers\\models\\glm46v",
        "target": "src\\transformers\\models\\glm46v\\video_processing_glm46v.py"
      },
      {
        "source": "src\\transformers\\models\\glm46v",
        "target": "src\\transformers\\models\\glm46v\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v",
        "target": "src\\transformers\\models\\glm4v\\configuration_glm4v.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v",
        "target": "src\\transformers\\models\\glm4v\\convert_glm4v_mgt_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v",
        "target": "src\\transformers\\models\\glm4v\\image_processing_glm4v.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v",
        "target": "src\\transformers\\models\\glm4v\\image_processing_glm4v_fast.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v",
        "target": "src\\transformers\\models\\glm4v\\modeling_glm4v.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v",
        "target": "src\\transformers\\models\\glm4v\\modular_glm4v.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v",
        "target": "src\\transformers\\models\\glm4v\\processing_glm4v.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v",
        "target": "src\\transformers\\models\\glm4v\\video_processing_glm4v.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v",
        "target": "src\\transformers\\models\\glm4v\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v_moe",
        "target": "src\\transformers\\models\\glm4v_moe\\configuration_glm4v_moe.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v_moe",
        "target": "src\\transformers\\models\\glm4v_moe\\convert_glm4v_moe_mgt_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v_moe",
        "target": "src\\transformers\\models\\glm4v_moe\\modeling_glm4v_moe.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v_moe",
        "target": "src\\transformers\\models\\glm4v_moe\\modular_glm4v_moe.py"
      },
      {
        "source": "src\\transformers\\models\\glm4v_moe",
        "target": "src\\transformers\\models\\glm4v_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glm4_moe",
        "target": "src\\transformers\\models\\glm4_moe\\configuration_glm4_moe.py"
      },
      {
        "source": "src\\transformers\\models\\glm4_moe",
        "target": "src\\transformers\\models\\glm4_moe\\modeling_glm4_moe.py"
      },
      {
        "source": "src\\transformers\\models\\glm4_moe",
        "target": "src\\transformers\\models\\glm4_moe\\modular_glm4_moe.py"
      },
      {
        "source": "src\\transformers\\models\\glm4_moe",
        "target": "src\\transformers\\models\\glm4_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glm4_moe_lite",
        "target": "src\\transformers\\models\\glm4_moe_lite\\configuration_glm4_moe_lite.py"
      },
      {
        "source": "src\\transformers\\models\\glm4_moe_lite",
        "target": "src\\transformers\\models\\glm4_moe_lite\\modeling_glm4_moe_lite.py"
      },
      {
        "source": "src\\transformers\\models\\glm4_moe_lite",
        "target": "src\\transformers\\models\\glm4_moe_lite\\modular_glm4_moe_lite.py"
      },
      {
        "source": "src\\transformers\\models\\glm4_moe_lite",
        "target": "src\\transformers\\models\\glm4_moe_lite\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glmasr",
        "target": "src\\transformers\\models\\glmasr\\configuration_glmasr.py"
      },
      {
        "source": "src\\transformers\\models\\glmasr",
        "target": "src\\transformers\\models\\glmasr\\convert_glmasr_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\glmasr",
        "target": "src\\transformers\\models\\glmasr\\modeling_glmasr.py"
      },
      {
        "source": "src\\transformers\\models\\glmasr",
        "target": "src\\transformers\\models\\glmasr\\modular_glmasr.py"
      },
      {
        "source": "src\\transformers\\models\\glmasr",
        "target": "src\\transformers\\models\\glmasr\\processing_glmasr.py"
      },
      {
        "source": "src\\transformers\\models\\glmasr",
        "target": "src\\transformers\\models\\glmasr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glm_image",
        "target": "src\\transformers\\models\\glm_image\\configuration_glm_image.py"
      },
      {
        "source": "src\\transformers\\models\\glm_image",
        "target": "src\\transformers\\models\\glm_image\\image_processing_glm_image.py"
      },
      {
        "source": "src\\transformers\\models\\glm_image",
        "target": "src\\transformers\\models\\glm_image\\image_processing_glm_image_fast.py"
      },
      {
        "source": "src\\transformers\\models\\glm_image",
        "target": "src\\transformers\\models\\glm_image\\modeling_glm_image.py"
      },
      {
        "source": "src\\transformers\\models\\glm_image",
        "target": "src\\transformers\\models\\glm_image\\modular_glm_image.py"
      },
      {
        "source": "src\\transformers\\models\\glm_image",
        "target": "src\\transformers\\models\\glm_image\\processing_glm_image.py"
      },
      {
        "source": "src\\transformers\\models\\glm_image",
        "target": "src\\transformers\\models\\glm_image\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glm_ocr",
        "target": "src\\transformers\\models\\glm_ocr\\configuration_glm_ocr.py"
      },
      {
        "source": "src\\transformers\\models\\glm_ocr",
        "target": "src\\transformers\\models\\glm_ocr\\modeling_glm_ocr.py"
      },
      {
        "source": "src\\transformers\\models\\glm_ocr",
        "target": "src\\transformers\\models\\glm_ocr\\modular_glm_ocr.py"
      },
      {
        "source": "src\\transformers\\models\\glm_ocr",
        "target": "src\\transformers\\models\\glm_ocr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\glpn",
        "target": "src\\transformers\\models\\glpn\\configuration_glpn.py"
      },
      {
        "source": "src\\transformers\\models\\glpn",
        "target": "src\\transformers\\models\\glpn\\convert_glpn_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\glpn",
        "target": "src\\transformers\\models\\glpn\\image_processing_glpn.py"
      },
      {
        "source": "src\\transformers\\models\\glpn",
        "target": "src\\transformers\\models\\glpn\\image_processing_glpn_fast.py"
      },
      {
        "source": "src\\transformers\\models\\glpn",
        "target": "src\\transformers\\models\\glpn\\modeling_glpn.py"
      },
      {
        "source": "src\\transformers\\models\\glpn",
        "target": "src\\transformers\\models\\glpn\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\got_ocr2",
        "target": "src\\transformers\\models\\got_ocr2\\configuration_got_ocr2.py"
      },
      {
        "source": "src\\transformers\\models\\got_ocr2",
        "target": "src\\transformers\\models\\got_ocr2\\convert_got_ocr2_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\got_ocr2",
        "target": "src\\transformers\\models\\got_ocr2\\image_processing_got_ocr2.py"
      },
      {
        "source": "src\\transformers\\models\\got_ocr2",
        "target": "src\\transformers\\models\\got_ocr2\\image_processing_got_ocr2_fast.py"
      },
      {
        "source": "src\\transformers\\models\\got_ocr2",
        "target": "src\\transformers\\models\\got_ocr2\\modeling_got_ocr2.py"
      },
      {
        "source": "src\\transformers\\models\\got_ocr2",
        "target": "src\\transformers\\models\\got_ocr2\\modular_got_ocr2.py"
      },
      {
        "source": "src\\transformers\\models\\got_ocr2",
        "target": "src\\transformers\\models\\got_ocr2\\processing_got_ocr2.py"
      },
      {
        "source": "src\\transformers\\models\\got_ocr2",
        "target": "src\\transformers\\models\\got_ocr2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gpt2",
        "target": "src\\transformers\\models\\gpt2\\configuration_gpt2.py"
      },
      {
        "source": "src\\transformers\\models\\gpt2",
        "target": "src\\transformers\\models\\gpt2\\CONVERSION.md"
      },
      {
        "source": "src\\transformers\\models\\gpt2",
        "target": "src\\transformers\\models\\gpt2\\convert_gpt2_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\gpt2",
        "target": "src\\transformers\\models\\gpt2\\modeling_gpt2.py"
      },
      {
        "source": "src\\transformers\\models\\gpt2",
        "target": "src\\transformers\\models\\gpt2\\tokenization_gpt2.py"
      },
      {
        "source": "src\\transformers\\models\\gpt2",
        "target": "src\\transformers\\models\\gpt2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gptj",
        "target": "src\\transformers\\models\\gptj\\configuration_gptj.py"
      },
      {
        "source": "src\\transformers\\models\\gptj",
        "target": "src\\transformers\\models\\gptj\\modeling_gptj.py"
      },
      {
        "source": "src\\transformers\\models\\gptj",
        "target": "src\\transformers\\models\\gptj\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_bigcode",
        "target": "src\\transformers\\models\\gpt_bigcode\\configuration_gpt_bigcode.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_bigcode",
        "target": "src\\transformers\\models\\gpt_bigcode\\modeling_gpt_bigcode.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_bigcode",
        "target": "src\\transformers\\models\\gpt_bigcode\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neo",
        "target": "src\\transformers\\models\\gpt_neo\\configuration_gpt_neo.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neo",
        "target": "src\\transformers\\models\\gpt_neo\\convert_gpt_neo_mesh_tf_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neo",
        "target": "src\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neo",
        "target": "src\\transformers\\models\\gpt_neo\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neox",
        "target": "src\\transformers\\models\\gpt_neox\\configuration_gpt_neox.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neox",
        "target": "src\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neox",
        "target": "src\\transformers\\models\\gpt_neox\\modular_gpt_neox.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neox",
        "target": "src\\transformers\\models\\gpt_neox\\tokenization_gpt_neox.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neox",
        "target": "src\\transformers\\models\\gpt_neox\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neox_japanese",
        "target": "src\\transformers\\models\\gpt_neox_japanese\\configuration_gpt_neox_japanese.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neox_japanese",
        "target": "src\\transformers\\models\\gpt_neox_japanese\\modeling_gpt_neox_japanese.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neox_japanese",
        "target": "src\\transformers\\models\\gpt_neox_japanese\\tokenization_gpt_neox_japanese.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_neox_japanese",
        "target": "src\\transformers\\models\\gpt_neox_japanese\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_oss",
        "target": "src\\transformers\\models\\gpt_oss\\configuration_gpt_oss.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_oss",
        "target": "src\\transformers\\models\\gpt_oss\\convert_gpt_oss_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_oss",
        "target": "src\\transformers\\models\\gpt_oss\\modeling_gpt_oss.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_oss",
        "target": "src\\transformers\\models\\gpt_oss\\modular_gpt_oss.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_oss",
        "target": "src\\transformers\\models\\gpt_oss\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_sw3",
        "target": "src\\transformers\\models\\gpt_sw3\\convert_megatron_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_sw3",
        "target": "src\\transformers\\models\\gpt_sw3\\tokenization_gpt_sw3.py"
      },
      {
        "source": "src\\transformers\\models\\gpt_sw3",
        "target": "src\\transformers\\models\\gpt_sw3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\granite",
        "target": "src\\transformers\\models\\granite\\configuration_granite.py"
      },
      {
        "source": "src\\transformers\\models\\granite",
        "target": "src\\transformers\\models\\granite\\modeling_granite.py"
      },
      {
        "source": "src\\transformers\\models\\granite",
        "target": "src\\transformers\\models\\granite\\modular_granite.py"
      },
      {
        "source": "src\\transformers\\models\\granite",
        "target": "src\\transformers\\models\\granite\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoe",
        "target": "src\\transformers\\models\\granitemoe\\configuration_granitemoe.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoe",
        "target": "src\\transformers\\models\\granitemoe\\modeling_granitemoe.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoe",
        "target": "src\\transformers\\models\\granitemoe\\modular_granitemoe.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoe",
        "target": "src\\transformers\\models\\granitemoe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoehybrid",
        "target": "src\\transformers\\models\\granitemoehybrid\\configuration_granitemoehybrid.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoehybrid",
        "target": "src\\transformers\\models\\granitemoehybrid\\modeling_granitemoehybrid.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoehybrid",
        "target": "src\\transformers\\models\\granitemoehybrid\\modular_granitemoehybrid.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoehybrid",
        "target": "src\\transformers\\models\\granitemoehybrid\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoeshared",
        "target": "src\\transformers\\models\\granitemoeshared\\configuration_granitemoeshared.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoeshared",
        "target": "src\\transformers\\models\\granitemoeshared\\modeling_granitemoeshared.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoeshared",
        "target": "src\\transformers\\models\\granitemoeshared\\modular_granitemoeshared.py"
      },
      {
        "source": "src\\transformers\\models\\granitemoeshared",
        "target": "src\\transformers\\models\\granitemoeshared\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\granite_speech",
        "target": "src\\transformers\\models\\granite_speech\\configuration_granite_speech.py"
      },
      {
        "source": "src\\transformers\\models\\granite_speech",
        "target": "src\\transformers\\models\\granite_speech\\feature_extraction_granite_speech.py"
      },
      {
        "source": "src\\transformers\\models\\granite_speech",
        "target": "src\\transformers\\models\\granite_speech\\modeling_granite_speech.py"
      },
      {
        "source": "src\\transformers\\models\\granite_speech",
        "target": "src\\transformers\\models\\granite_speech\\processing_granite_speech.py"
      },
      {
        "source": "src\\transformers\\models\\granite_speech",
        "target": "src\\transformers\\models\\granite_speech\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\grounding_dino",
        "target": "src\\transformers\\models\\grounding_dino\\configuration_grounding_dino.py"
      },
      {
        "source": "src\\transformers\\models\\grounding_dino",
        "target": "src\\transformers\\models\\grounding_dino\\convert_grounding_dino_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\grounding_dino",
        "target": "src\\transformers\\models\\grounding_dino\\image_processing_grounding_dino.py"
      },
      {
        "source": "src\\transformers\\models\\grounding_dino",
        "target": "src\\transformers\\models\\grounding_dino\\image_processing_grounding_dino_fast.py"
      },
      {
        "source": "src\\transformers\\models\\grounding_dino",
        "target": "src\\transformers\\models\\grounding_dino\\modeling_grounding_dino.py"
      },
      {
        "source": "src\\transformers\\models\\grounding_dino",
        "target": "src\\transformers\\models\\grounding_dino\\modular_grounding_dino.py"
      },
      {
        "source": "src\\transformers\\models\\grounding_dino",
        "target": "src\\transformers\\models\\grounding_dino\\processing_grounding_dino.py"
      },
      {
        "source": "src\\transformers\\models\\grounding_dino",
        "target": "src\\transformers\\models\\grounding_dino\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\groupvit",
        "target": "src\\transformers\\models\\groupvit\\configuration_groupvit.py"
      },
      {
        "source": "src\\transformers\\models\\groupvit",
        "target": "src\\transformers\\models\\groupvit\\convert_groupvit_nvlab_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\groupvit",
        "target": "src\\transformers\\models\\groupvit\\modeling_groupvit.py"
      },
      {
        "source": "src\\transformers\\models\\groupvit",
        "target": "src\\transformers\\models\\groupvit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\helium",
        "target": "src\\transformers\\models\\helium\\configuration_helium.py"
      },
      {
        "source": "src\\transformers\\models\\helium",
        "target": "src\\transformers\\models\\helium\\modeling_helium.py"
      },
      {
        "source": "src\\transformers\\models\\helium",
        "target": "src\\transformers\\models\\helium\\modular_helium.py"
      },
      {
        "source": "src\\transformers\\models\\helium",
        "target": "src\\transformers\\models\\helium\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\herbert",
        "target": "src\\transformers\\models\\herbert\\tokenization_herbert.py"
      },
      {
        "source": "src\\transformers\\models\\herbert",
        "target": "src\\transformers\\models\\herbert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\hgnet_v2",
        "target": "src\\transformers\\models\\hgnet_v2\\configuration_hgnet_v2.py"
      },
      {
        "source": "src\\transformers\\models\\hgnet_v2",
        "target": "src\\transformers\\models\\hgnet_v2\\modeling_hgnet_v2.py"
      },
      {
        "source": "src\\transformers\\models\\hgnet_v2",
        "target": "src\\transformers\\models\\hgnet_v2\\modular_hgnet_v2.py"
      },
      {
        "source": "src\\transformers\\models\\hgnet_v2",
        "target": "src\\transformers\\models\\hgnet_v2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\hiera",
        "target": "src\\transformers\\models\\hiera\\configuration_hiera.py"
      },
      {
        "source": "src\\transformers\\models\\hiera",
        "target": "src\\transformers\\models\\hiera\\convert_hiera_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\hiera",
        "target": "src\\transformers\\models\\hiera\\modeling_hiera.py"
      },
      {
        "source": "src\\transformers\\models\\hiera",
        "target": "src\\transformers\\models\\hiera\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\hubert",
        "target": "src\\transformers\\models\\hubert\\configuration_hubert.py"
      },
      {
        "source": "src\\transformers\\models\\hubert",
        "target": "src\\transformers\\models\\hubert\\convert_distilhubert_original_s3prl_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\hubert",
        "target": "src\\transformers\\models\\hubert\\convert_hubert_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\hubert",
        "target": "src\\transformers\\models\\hubert\\convert_hubert_original_s3prl_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\hubert",
        "target": "src\\transformers\\models\\hubert\\modeling_hubert.py"
      },
      {
        "source": "src\\transformers\\models\\hubert",
        "target": "src\\transformers\\models\\hubert\\modular_hubert.py"
      },
      {
        "source": "src\\transformers\\models\\hubert",
        "target": "src\\transformers\\models\\hubert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\hunyuan_v1_dense",
        "target": "src\\transformers\\models\\hunyuan_v1_dense\\configuration_hunyuan_v1_dense.py"
      },
      {
        "source": "src\\transformers\\models\\hunyuan_v1_dense",
        "target": "src\\transformers\\models\\hunyuan_v1_dense\\modeling_hunyuan_v1_dense.py"
      },
      {
        "source": "src\\transformers\\models\\hunyuan_v1_dense",
        "target": "src\\transformers\\models\\hunyuan_v1_dense\\modular_hunyuan_v1_dense.py"
      },
      {
        "source": "src\\transformers\\models\\hunyuan_v1_dense",
        "target": "src\\transformers\\models\\hunyuan_v1_dense\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\hunyuan_v1_moe",
        "target": "src\\transformers\\models\\hunyuan_v1_moe\\configuration_hunyuan_v1_moe.py"
      },
      {
        "source": "src\\transformers\\models\\hunyuan_v1_moe",
        "target": "src\\transformers\\models\\hunyuan_v1_moe\\modeling_hunyuan_v1_moe.py"
      },
      {
        "source": "src\\transformers\\models\\hunyuan_v1_moe",
        "target": "src\\transformers\\models\\hunyuan_v1_moe\\modular_hunyuan_v1_moe.py"
      },
      {
        "source": "src\\transformers\\models\\hunyuan_v1_moe",
        "target": "src\\transformers\\models\\hunyuan_v1_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\ibert",
        "target": "src\\transformers\\models\\ibert\\configuration_ibert.py"
      },
      {
        "source": "src\\transformers\\models\\ibert",
        "target": "src\\transformers\\models\\ibert\\modeling_ibert.py"
      },
      {
        "source": "src\\transformers\\models\\ibert",
        "target": "src\\transformers\\models\\ibert\\quant_modules.py"
      },
      {
        "source": "src\\transformers\\models\\ibert",
        "target": "src\\transformers\\models\\ibert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\idefics",
        "target": "src\\transformers\\models\\idefics\\configuration_idefics.py"
      },
      {
        "source": "src\\transformers\\models\\idefics",
        "target": "src\\transformers\\models\\idefics\\image_processing_idefics.py"
      },
      {
        "source": "src\\transformers\\models\\idefics",
        "target": "src\\transformers\\models\\idefics\\modeling_idefics.py"
      },
      {
        "source": "src\\transformers\\models\\idefics",
        "target": "src\\transformers\\models\\idefics\\perceiver.py"
      },
      {
        "source": "src\\transformers\\models\\idefics",
        "target": "src\\transformers\\models\\idefics\\processing_idefics.py"
      },
      {
        "source": "src\\transformers\\models\\idefics",
        "target": "src\\transformers\\models\\idefics\\vision.py"
      },
      {
        "source": "src\\transformers\\models\\idefics",
        "target": "src\\transformers\\models\\idefics\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\idefics2",
        "target": "src\\transformers\\models\\idefics2\\configuration_idefics2.py"
      },
      {
        "source": "src\\transformers\\models\\idefics2",
        "target": "src\\transformers\\models\\idefics2\\convert_idefics2_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\idefics2",
        "target": "src\\transformers\\models\\idefics2\\image_processing_idefics2.py"
      },
      {
        "source": "src\\transformers\\models\\idefics2",
        "target": "src\\transformers\\models\\idefics2\\image_processing_idefics2_fast.py"
      },
      {
        "source": "src\\transformers\\models\\idefics2",
        "target": "src\\transformers\\models\\idefics2\\modeling_idefics2.py"
      },
      {
        "source": "src\\transformers\\models\\idefics2",
        "target": "src\\transformers\\models\\idefics2\\processing_idefics2.py"
      },
      {
        "source": "src\\transformers\\models\\idefics2",
        "target": "src\\transformers\\models\\idefics2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\idefics3",
        "target": "src\\transformers\\models\\idefics3\\configuration_idefics3.py"
      },
      {
        "source": "src\\transformers\\models\\idefics3",
        "target": "src\\transformers\\models\\idefics3\\convert_idefics3_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\idefics3",
        "target": "src\\transformers\\models\\idefics3\\image_processing_idefics3.py"
      },
      {
        "source": "src\\transformers\\models\\idefics3",
        "target": "src\\transformers\\models\\idefics3\\image_processing_idefics3_fast.py"
      },
      {
        "source": "src\\transformers\\models\\idefics3",
        "target": "src\\transformers\\models\\idefics3\\modeling_idefics3.py"
      },
      {
        "source": "src\\transformers\\models\\idefics3",
        "target": "src\\transformers\\models\\idefics3\\processing_idefics3.py"
      },
      {
        "source": "src\\transformers\\models\\idefics3",
        "target": "src\\transformers\\models\\idefics3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\ijepa",
        "target": "src\\transformers\\models\\ijepa\\configuration_ijepa.py"
      },
      {
        "source": "src\\transformers\\models\\ijepa",
        "target": "src\\transformers\\models\\ijepa\\convert_ijepa_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\ijepa",
        "target": "src\\transformers\\models\\ijepa\\modeling_ijepa.py"
      },
      {
        "source": "src\\transformers\\models\\ijepa",
        "target": "src\\transformers\\models\\ijepa\\modular_ijepa.py"
      },
      {
        "source": "src\\transformers\\models\\ijepa",
        "target": "src\\transformers\\models\\ijepa\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\imagegpt",
        "target": "src\\transformers\\models\\imagegpt\\configuration_imagegpt.py"
      },
      {
        "source": "src\\transformers\\models\\imagegpt",
        "target": "src\\transformers\\models\\imagegpt\\convert_imagegpt_original_tf2_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\imagegpt",
        "target": "src\\transformers\\models\\imagegpt\\image_processing_imagegpt.py"
      },
      {
        "source": "src\\transformers\\models\\imagegpt",
        "target": "src\\transformers\\models\\imagegpt\\image_processing_imagegpt_fast.py"
      },
      {
        "source": "src\\transformers\\models\\imagegpt",
        "target": "src\\transformers\\models\\imagegpt\\modeling_imagegpt.py"
      },
      {
        "source": "src\\transformers\\models\\imagegpt",
        "target": "src\\transformers\\models\\imagegpt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\informer",
        "target": "src\\transformers\\models\\informer\\configuration_informer.py"
      },
      {
        "source": "src\\transformers\\models\\informer",
        "target": "src\\transformers\\models\\informer\\modeling_informer.py"
      },
      {
        "source": "src\\transformers\\models\\informer",
        "target": "src\\transformers\\models\\informer\\modular_informer.py"
      },
      {
        "source": "src\\transformers\\models\\informer",
        "target": "src\\transformers\\models\\informer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\instructblip",
        "target": "src\\transformers\\models\\instructblip\\configuration_instructblip.py"
      },
      {
        "source": "src\\transformers\\models\\instructblip",
        "target": "src\\transformers\\models\\instructblip\\convert_instructblip_original_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\instructblip",
        "target": "src\\transformers\\models\\instructblip\\modeling_instructblip.py"
      },
      {
        "source": "src\\transformers\\models\\instructblip",
        "target": "src\\transformers\\models\\instructblip\\processing_instructblip.py"
      },
      {
        "source": "src\\transformers\\models\\instructblip",
        "target": "src\\transformers\\models\\instructblip\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\instructblipvideo",
        "target": "src\\transformers\\models\\instructblipvideo\\configuration_instructblipvideo.py"
      },
      {
        "source": "src\\transformers\\models\\instructblipvideo",
        "target": "src\\transformers\\models\\instructblipvideo\\convert_instructblipvideo_original_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\instructblipvideo",
        "target": "src\\transformers\\models\\instructblipvideo\\modeling_instructblipvideo.py"
      },
      {
        "source": "src\\transformers\\models\\instructblipvideo",
        "target": "src\\transformers\\models\\instructblipvideo\\modular_instructblipvideo.py"
      },
      {
        "source": "src\\transformers\\models\\instructblipvideo",
        "target": "src\\transformers\\models\\instructblipvideo\\processing_instructblipvideo.py"
      },
      {
        "source": "src\\transformers\\models\\instructblipvideo",
        "target": "src\\transformers\\models\\instructblipvideo\\video_processing_instructblipvideo.py"
      },
      {
        "source": "src\\transformers\\models\\instructblipvideo",
        "target": "src\\transformers\\models\\instructblipvideo\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\internvl",
        "target": "src\\transformers\\models\\internvl\\configuration_internvl.py"
      },
      {
        "source": "src\\transformers\\models\\internvl",
        "target": "src\\transformers\\models\\internvl\\convert_internvl_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\internvl",
        "target": "src\\transformers\\models\\internvl\\modeling_internvl.py"
      },
      {
        "source": "src\\transformers\\models\\internvl",
        "target": "src\\transformers\\models\\internvl\\modular_internvl.py"
      },
      {
        "source": "src\\transformers\\models\\internvl",
        "target": "src\\transformers\\models\\internvl\\processing_internvl.py"
      },
      {
        "source": "src\\transformers\\models\\internvl",
        "target": "src\\transformers\\models\\internvl\\video_processing_internvl.py"
      },
      {
        "source": "src\\transformers\\models\\internvl",
        "target": "src\\transformers\\models\\internvl\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\jais2",
        "target": "src\\transformers\\models\\jais2\\configuration_jais2.py"
      },
      {
        "source": "src\\transformers\\models\\jais2",
        "target": "src\\transformers\\models\\jais2\\modeling_jais2.py"
      },
      {
        "source": "src\\transformers\\models\\jais2",
        "target": "src\\transformers\\models\\jais2\\modular_jais2.py"
      },
      {
        "source": "src\\transformers\\models\\jais2",
        "target": "src\\transformers\\models\\jais2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\jamba",
        "target": "src\\transformers\\models\\jamba\\configuration_jamba.py"
      },
      {
        "source": "src\\transformers\\models\\jamba",
        "target": "src\\transformers\\models\\jamba\\modeling_jamba.py"
      },
      {
        "source": "src\\transformers\\models\\jamba",
        "target": "src\\transformers\\models\\jamba\\modular_jamba.py"
      },
      {
        "source": "src\\transformers\\models\\jamba",
        "target": "src\\transformers\\models\\jamba\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\janus",
        "target": "src\\transformers\\models\\janus\\configuration_janus.py"
      },
      {
        "source": "src\\transformers\\models\\janus",
        "target": "src\\transformers\\models\\janus\\convert_janus_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\janus",
        "target": "src\\transformers\\models\\janus\\image_processing_janus.py"
      },
      {
        "source": "src\\transformers\\models\\janus",
        "target": "src\\transformers\\models\\janus\\image_processing_janus_fast.py"
      },
      {
        "source": "src\\transformers\\models\\janus",
        "target": "src\\transformers\\models\\janus\\modeling_janus.py"
      },
      {
        "source": "src\\transformers\\models\\janus",
        "target": "src\\transformers\\models\\janus\\modular_janus.py"
      },
      {
        "source": "src\\transformers\\models\\janus",
        "target": "src\\transformers\\models\\janus\\processing_janus.py"
      },
      {
        "source": "src\\transformers\\models\\janus",
        "target": "src\\transformers\\models\\janus\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\jetmoe",
        "target": "src\\transformers\\models\\jetmoe\\configuration_jetmoe.py"
      },
      {
        "source": "src\\transformers\\models\\jetmoe",
        "target": "src\\transformers\\models\\jetmoe\\modeling_jetmoe.py"
      },
      {
        "source": "src\\transformers\\models\\jetmoe",
        "target": "src\\transformers\\models\\jetmoe\\modular_jetmoe.py"
      },
      {
        "source": "src\\transformers\\models\\jetmoe",
        "target": "src\\transformers\\models\\jetmoe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2",
        "target": "src\\transformers\\models\\kosmos2\\configuration_kosmos2.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2",
        "target": "src\\transformers\\models\\kosmos2\\convert_kosmos2_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2",
        "target": "src\\transformers\\models\\kosmos2\\modeling_kosmos2.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2",
        "target": "src\\transformers\\models\\kosmos2\\processing_kosmos2.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2",
        "target": "src\\transformers\\models\\kosmos2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2_5",
        "target": "src\\transformers\\models\\kosmos2_5\\configuration_kosmos2_5.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2_5",
        "target": "src\\transformers\\models\\kosmos2_5\\convert_kosmos2_5.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2_5",
        "target": "src\\transformers\\models\\kosmos2_5\\image_processing_kosmos2_5.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2_5",
        "target": "src\\transformers\\models\\kosmos2_5\\image_processing_kosmos2_5_fast.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2_5",
        "target": "src\\transformers\\models\\kosmos2_5\\modeling_kosmos2_5.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2_5",
        "target": "src\\transformers\\models\\kosmos2_5\\processing_kosmos2_5.py"
      },
      {
        "source": "src\\transformers\\models\\kosmos2_5",
        "target": "src\\transformers\\models\\kosmos2_5\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\kyutai_speech_to_text",
        "target": "src\\transformers\\models\\kyutai_speech_to_text\\configuration_kyutai_speech_to_text.py"
      },
      {
        "source": "src\\transformers\\models\\kyutai_speech_to_text",
        "target": "src\\transformers\\models\\kyutai_speech_to_text\\convert_kyutai_speech_to_text_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\kyutai_speech_to_text",
        "target": "src\\transformers\\models\\kyutai_speech_to_text\\feature_extraction_kyutai_speech_to_text.py"
      },
      {
        "source": "src\\transformers\\models\\kyutai_speech_to_text",
        "target": "src\\transformers\\models\\kyutai_speech_to_text\\modeling_kyutai_speech_to_text.py"
      },
      {
        "source": "src\\transformers\\models\\kyutai_speech_to_text",
        "target": "src\\transformers\\models\\kyutai_speech_to_text\\modular_kyutai_speech_to_text.py"
      },
      {
        "source": "src\\transformers\\models\\kyutai_speech_to_text",
        "target": "src\\transformers\\models\\kyutai_speech_to_text\\processing_kyutai_speech_to_text.py"
      },
      {
        "source": "src\\transformers\\models\\kyutai_speech_to_text",
        "target": "src\\transformers\\models\\kyutai_speech_to_text\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\lasr",
        "target": "src\\transformers\\models\\lasr\\configuration_lasr.py"
      },
      {
        "source": "src\\transformers\\models\\lasr",
        "target": "src\\transformers\\models\\lasr\\feature_extraction_lasr.py"
      },
      {
        "source": "src\\transformers\\models\\lasr",
        "target": "src\\transformers\\models\\lasr\\modeling_lasr.py"
      },
      {
        "source": "src\\transformers\\models\\lasr",
        "target": "src\\transformers\\models\\lasr\\modular_lasr.py"
      },
      {
        "source": "src\\transformers\\models\\lasr",
        "target": "src\\transformers\\models\\lasr\\processing_lasr.py"
      },
      {
        "source": "src\\transformers\\models\\lasr",
        "target": "src\\transformers\\models\\lasr\\tokenization_lasr.py"
      },
      {
        "source": "src\\transformers\\models\\lasr",
        "target": "src\\transformers\\models\\lasr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlm",
        "target": "src\\transformers\\models\\layoutlm\\configuration_layoutlm.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlm",
        "target": "src\\transformers\\models\\layoutlm\\modeling_layoutlm.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlm",
        "target": "src\\transformers\\models\\layoutlm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv2",
        "target": "src\\transformers\\models\\layoutlmv2\\configuration_layoutlmv2.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv2",
        "target": "src\\transformers\\models\\layoutlmv2\\image_processing_layoutlmv2.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv2",
        "target": "src\\transformers\\models\\layoutlmv2\\image_processing_layoutlmv2_fast.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv2",
        "target": "src\\transformers\\models\\layoutlmv2\\modeling_layoutlmv2.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv2",
        "target": "src\\transformers\\models\\layoutlmv2\\processing_layoutlmv2.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv2",
        "target": "src\\transformers\\models\\layoutlmv2\\tokenization_layoutlmv2.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv2",
        "target": "src\\transformers\\models\\layoutlmv2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv3",
        "target": "src\\transformers\\models\\layoutlmv3\\configuration_layoutlmv3.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv3",
        "target": "src\\transformers\\models\\layoutlmv3\\image_processing_layoutlmv3.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv3",
        "target": "src\\transformers\\models\\layoutlmv3\\image_processing_layoutlmv3_fast.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv3",
        "target": "src\\transformers\\models\\layoutlmv3\\modeling_layoutlmv3.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv3",
        "target": "src\\transformers\\models\\layoutlmv3\\processing_layoutlmv3.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv3",
        "target": "src\\transformers\\models\\layoutlmv3\\tokenization_layoutlmv3.py"
      },
      {
        "source": "src\\transformers\\models\\layoutlmv3",
        "target": "src\\transformers\\models\\layoutlmv3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\layoutxlm",
        "target": "src\\transformers\\models\\layoutxlm\\configuration_layoutxlm.py"
      },
      {
        "source": "src\\transformers\\models\\layoutxlm",
        "target": "src\\transformers\\models\\layoutxlm\\modular_layoutxlm.py"
      },
      {
        "source": "src\\transformers\\models\\layoutxlm",
        "target": "src\\transformers\\models\\layoutxlm\\processing_layoutxlm.py"
      },
      {
        "source": "src\\transformers\\models\\layoutxlm",
        "target": "src\\transformers\\models\\layoutxlm\\tokenization_layoutxlm.py"
      },
      {
        "source": "src\\transformers\\models\\layoutxlm",
        "target": "src\\transformers\\models\\layoutxlm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\led",
        "target": "src\\transformers\\models\\led\\configuration_led.py"
      },
      {
        "source": "src\\transformers\\models\\led",
        "target": "src\\transformers\\models\\led\\modeling_led.py"
      },
      {
        "source": "src\\transformers\\models\\led",
        "target": "src\\transformers\\models\\led\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\levit",
        "target": "src\\transformers\\models\\levit\\configuration_levit.py"
      },
      {
        "source": "src\\transformers\\models\\levit",
        "target": "src\\transformers\\models\\levit\\convert_levit_timm_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\levit",
        "target": "src\\transformers\\models\\levit\\image_processing_levit.py"
      },
      {
        "source": "src\\transformers\\models\\levit",
        "target": "src\\transformers\\models\\levit\\image_processing_levit_fast.py"
      },
      {
        "source": "src\\transformers\\models\\levit",
        "target": "src\\transformers\\models\\levit\\modeling_levit.py"
      },
      {
        "source": "src\\transformers\\models\\levit",
        "target": "src\\transformers\\models\\levit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2",
        "target": "src\\transformers\\models\\lfm2\\configuration_lfm2.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2",
        "target": "src\\transformers\\models\\lfm2\\modeling_lfm2.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2",
        "target": "src\\transformers\\models\\lfm2\\modular_lfm2.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2",
        "target": "src\\transformers\\models\\lfm2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2_moe",
        "target": "src\\transformers\\models\\lfm2_moe\\configuration_lfm2_moe.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2_moe",
        "target": "src\\transformers\\models\\lfm2_moe\\modeling_lfm2_moe.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2_moe",
        "target": "src\\transformers\\models\\lfm2_moe\\modular_lfm2_moe.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2_moe",
        "target": "src\\transformers\\models\\lfm2_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2_vl",
        "target": "src\\transformers\\models\\lfm2_vl\\configuration_lfm2_vl.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2_vl",
        "target": "src\\transformers\\models\\lfm2_vl\\image_processing_lfm2_vl_fast.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2_vl",
        "target": "src\\transformers\\models\\lfm2_vl\\modeling_lfm2_vl.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2_vl",
        "target": "src\\transformers\\models\\lfm2_vl\\modular_lfm2_vl.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2_vl",
        "target": "src\\transformers\\models\\lfm2_vl\\processing_lfm2_vl.py"
      },
      {
        "source": "src\\transformers\\models\\lfm2_vl",
        "target": "src\\transformers\\models\\lfm2_vl\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\lightglue",
        "target": "src\\transformers\\models\\lightglue\\configuration_lightglue.py"
      },
      {
        "source": "src\\transformers\\models\\lightglue",
        "target": "src\\transformers\\models\\lightglue\\convert_lightglue_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\lightglue",
        "target": "src\\transformers\\models\\lightglue\\image_processing_lightglue.py"
      },
      {
        "source": "src\\transformers\\models\\lightglue",
        "target": "src\\transformers\\models\\lightglue\\image_processing_lightglue_fast.py"
      },
      {
        "source": "src\\transformers\\models\\lightglue",
        "target": "src\\transformers\\models\\lightglue\\modeling_lightglue.py"
      },
      {
        "source": "src\\transformers\\models\\lightglue",
        "target": "src\\transformers\\models\\lightglue\\modular_lightglue.py"
      },
      {
        "source": "src\\transformers\\models\\lightglue",
        "target": "src\\transformers\\models\\lightglue\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\lighton_ocr",
        "target": "src\\transformers\\models\\lighton_ocr\\configuration_lighton_ocr.py"
      },
      {
        "source": "src\\transformers\\models\\lighton_ocr",
        "target": "src\\transformers\\models\\lighton_ocr\\modeling_lighton_ocr.py"
      },
      {
        "source": "src\\transformers\\models\\lighton_ocr",
        "target": "src\\transformers\\models\\lighton_ocr\\modular_lighton_ocr.py"
      },
      {
        "source": "src\\transformers\\models\\lighton_ocr",
        "target": "src\\transformers\\models\\lighton_ocr\\processing_lighton_ocr.py"
      },
      {
        "source": "src\\transformers\\models\\lighton_ocr",
        "target": "src\\transformers\\models\\lighton_ocr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\lilt",
        "target": "src\\transformers\\models\\lilt\\configuration_lilt.py"
      },
      {
        "source": "src\\transformers\\models\\lilt",
        "target": "src\\transformers\\models\\lilt\\modeling_lilt.py"
      },
      {
        "source": "src\\transformers\\models\\lilt",
        "target": "src\\transformers\\models\\lilt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\llama",
        "target": "src\\transformers\\models\\llama\\configuration_llama.py"
      },
      {
        "source": "src\\transformers\\models\\llama",
        "target": "src\\transformers\\models\\llama\\convert_llama_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\llama",
        "target": "src\\transformers\\models\\llama\\modeling_llama.py"
      },
      {
        "source": "src\\transformers\\models\\llama",
        "target": "src\\transformers\\models\\llama\\tokenization_llama.py"
      },
      {
        "source": "src\\transformers\\models\\llama",
        "target": "src\\transformers\\models\\llama\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\llama4",
        "target": "src\\transformers\\models\\llama4\\configuration_llama4.py"
      },
      {
        "source": "src\\transformers\\models\\llama4",
        "target": "src\\transformers\\models\\llama4\\convert_llama4_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\llama4",
        "target": "src\\transformers\\models\\llama4\\image_processing_llama4_fast.py"
      },
      {
        "source": "src\\transformers\\models\\llama4",
        "target": "src\\transformers\\models\\llama4\\modeling_llama4.py"
      },
      {
        "source": "src\\transformers\\models\\llama4",
        "target": "src\\transformers\\models\\llama4\\processing_llama4.py"
      },
      {
        "source": "src\\transformers\\models\\llama4",
        "target": "src\\transformers\\models\\llama4\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\llava",
        "target": "src\\transformers\\models\\llava\\configuration_llava.py"
      },
      {
        "source": "src\\transformers\\models\\llava",
        "target": "src\\transformers\\models\\llava\\convert_llava_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\llava",
        "target": "src\\transformers\\models\\llava\\image_processing_llava.py"
      },
      {
        "source": "src\\transformers\\models\\llava",
        "target": "src\\transformers\\models\\llava\\image_processing_llava_fast.py"
      },
      {
        "source": "src\\transformers\\models\\llava",
        "target": "src\\transformers\\models\\llava\\modeling_llava.py"
      },
      {
        "source": "src\\transformers\\models\\llava",
        "target": "src\\transformers\\models\\llava\\processing_llava.py"
      },
      {
        "source": "src\\transformers\\models\\llava",
        "target": "src\\transformers\\models\\llava\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next",
        "target": "src\\transformers\\models\\llava_next\\configuration_llava_next.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next",
        "target": "src\\transformers\\models\\llava_next\\convert_llava_next_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next",
        "target": "src\\transformers\\models\\llava_next\\image_processing_llava_next.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next",
        "target": "src\\transformers\\models\\llava_next\\image_processing_llava_next_fast.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next",
        "target": "src\\transformers\\models\\llava_next\\modeling_llava_next.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next",
        "target": "src\\transformers\\models\\llava_next\\processing_llava_next.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next",
        "target": "src\\transformers\\models\\llava_next\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next_video",
        "target": "src\\transformers\\models\\llava_next_video\\configuration_llava_next_video.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next_video",
        "target": "src\\transformers\\models\\llava_next_video\\convert_llava_next_video_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next_video",
        "target": "src\\transformers\\models\\llava_next_video\\modeling_llava_next_video.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next_video",
        "target": "src\\transformers\\models\\llava_next_video\\modular_llava_next_video.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next_video",
        "target": "src\\transformers\\models\\llava_next_video\\processing_llava_next_video.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next_video",
        "target": "src\\transformers\\models\\llava_next_video\\video_processing_llava_next_video.py"
      },
      {
        "source": "src\\transformers\\models\\llava_next_video",
        "target": "src\\transformers\\models\\llava_next_video\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\llava_onevision",
        "target": "src\\transformers\\models\\llava_onevision\\configuration_llava_onevision.py"
      },
      {
        "source": "src\\transformers\\models\\llava_onevision",
        "target": "src\\transformers\\models\\llava_onevision\\convert_llava_onevision_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\llava_onevision",
        "target": "src\\transformers\\models\\llava_onevision\\image_processing_llava_onevision.py"
      },
      {
        "source": "src\\transformers\\models\\llava_onevision",
        "target": "src\\transformers\\models\\llava_onevision\\image_processing_llava_onevision_fast.py"
      },
      {
        "source": "src\\transformers\\models\\llava_onevision",
        "target": "src\\transformers\\models\\llava_onevision\\modeling_llava_onevision.py"
      },
      {
        "source": "src\\transformers\\models\\llava_onevision",
        "target": "src\\transformers\\models\\llava_onevision\\modular_llava_onevision.py"
      },
      {
        "source": "src\\transformers\\models\\llava_onevision",
        "target": "src\\transformers\\models\\llava_onevision\\processing_llava_onevision.py"
      },
      {
        "source": "src\\transformers\\models\\llava_onevision",
        "target": "src\\transformers\\models\\llava_onevision\\video_processing_llava_onevision.py"
      },
      {
        "source": "src\\transformers\\models\\llava_onevision",
        "target": "src\\transformers\\models\\llava_onevision\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\longcat_flash",
        "target": "src\\transformers\\models\\longcat_flash\\configuration_longcat_flash.py"
      },
      {
        "source": "src\\transformers\\models\\longcat_flash",
        "target": "src\\transformers\\models\\longcat_flash\\modeling_longcat_flash.py"
      },
      {
        "source": "src\\transformers\\models\\longcat_flash",
        "target": "src\\transformers\\models\\longcat_flash\\modular_longcat_flash.py"
      },
      {
        "source": "src\\transformers\\models\\longcat_flash",
        "target": "src\\transformers\\models\\longcat_flash\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\longformer",
        "target": "src\\transformers\\models\\longformer\\configuration_longformer.py"
      },
      {
        "source": "src\\transformers\\models\\longformer",
        "target": "src\\transformers\\models\\longformer\\convert_longformer_original_pytorch_lightning_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\longformer",
        "target": "src\\transformers\\models\\longformer\\modeling_longformer.py"
      },
      {
        "source": "src\\transformers\\models\\longformer",
        "target": "src\\transformers\\models\\longformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\longt5",
        "target": "src\\transformers\\models\\longt5\\configuration_longt5.py"
      },
      {
        "source": "src\\transformers\\models\\longt5",
        "target": "src\\transformers\\models\\longt5\\modeling_longt5.py"
      },
      {
        "source": "src\\transformers\\models\\longt5",
        "target": "src\\transformers\\models\\longt5\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\luke",
        "target": "src\\transformers\\models\\luke\\configuration_luke.py"
      },
      {
        "source": "src\\transformers\\models\\luke",
        "target": "src\\transformers\\models\\luke\\convert_luke_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\luke",
        "target": "src\\transformers\\models\\luke\\modeling_luke.py"
      },
      {
        "source": "src\\transformers\\models\\luke",
        "target": "src\\transformers\\models\\luke\\tokenization_luke.py"
      },
      {
        "source": "src\\transformers\\models\\luke",
        "target": "src\\transformers\\models\\luke\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\lw_detr",
        "target": "src\\transformers\\models\\lw_detr\\configuration_lw_detr.py"
      },
      {
        "source": "src\\transformers\\models\\lw_detr",
        "target": "src\\transformers\\models\\lw_detr\\convert_lw_detr_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\lw_detr",
        "target": "src\\transformers\\models\\lw_detr\\modeling_lw_detr.py"
      },
      {
        "source": "src\\transformers\\models\\lw_detr",
        "target": "src\\transformers\\models\\lw_detr\\modular_lw_detr.py"
      },
      {
        "source": "src\\transformers\\models\\lw_detr",
        "target": "src\\transformers\\models\\lw_detr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\lxmert",
        "target": "src\\transformers\\models\\lxmert\\configuration_lxmert.py"
      },
      {
        "source": "src\\transformers\\models\\lxmert",
        "target": "src\\transformers\\models\\lxmert\\convert_lxmert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\lxmert",
        "target": "src\\transformers\\models\\lxmert\\modeling_lxmert.py"
      },
      {
        "source": "src\\transformers\\models\\lxmert",
        "target": "src\\transformers\\models\\lxmert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\m2m_100",
        "target": "src\\transformers\\models\\m2m_100\\configuration_m2m_100.py"
      },
      {
        "source": "src\\transformers\\models\\m2m_100",
        "target": "src\\transformers\\models\\m2m_100\\convert_m2m100_original_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\m2m_100",
        "target": "src\\transformers\\models\\m2m_100\\modeling_m2m_100.py"
      },
      {
        "source": "src\\transformers\\models\\m2m_100",
        "target": "src\\transformers\\models\\m2m_100\\tokenization_m2m_100.py"
      },
      {
        "source": "src\\transformers\\models\\m2m_100",
        "target": "src\\transformers\\models\\m2m_100\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mamba",
        "target": "src\\transformers\\models\\mamba\\configuration_mamba.py"
      },
      {
        "source": "src\\transformers\\models\\mamba",
        "target": "src\\transformers\\models\\mamba\\convert_mamba_ssm_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mamba",
        "target": "src\\transformers\\models\\mamba\\modeling_mamba.py"
      },
      {
        "source": "src\\transformers\\models\\mamba",
        "target": "src\\transformers\\models\\mamba\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mamba2",
        "target": "src\\transformers\\models\\mamba2\\configuration_mamba2.py"
      },
      {
        "source": "src\\transformers\\models\\mamba2",
        "target": "src\\transformers\\models\\mamba2\\convert_mamba2_ssm_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mamba2",
        "target": "src\\transformers\\models\\mamba2\\modeling_mamba2.py"
      },
      {
        "source": "src\\transformers\\models\\mamba2",
        "target": "src\\transformers\\models\\mamba2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\marian",
        "target": "src\\transformers\\models\\marian\\configuration_marian.py"
      },
      {
        "source": "src\\transformers\\models\\marian",
        "target": "src\\transformers\\models\\marian\\convert_marian_tatoeba_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\marian",
        "target": "src\\transformers\\models\\marian\\convert_marian_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\marian",
        "target": "src\\transformers\\models\\marian\\modeling_marian.py"
      },
      {
        "source": "src\\transformers\\models\\marian",
        "target": "src\\transformers\\models\\marian\\tokenization_marian.py"
      },
      {
        "source": "src\\transformers\\models\\marian",
        "target": "src\\transformers\\models\\marian\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\markuplm",
        "target": "src\\transformers\\models\\markuplm\\configuration_markuplm.py"
      },
      {
        "source": "src\\transformers\\models\\markuplm",
        "target": "src\\transformers\\models\\markuplm\\feature_extraction_markuplm.py"
      },
      {
        "source": "src\\transformers\\models\\markuplm",
        "target": "src\\transformers\\models\\markuplm\\modeling_markuplm.py"
      },
      {
        "source": "src\\transformers\\models\\markuplm",
        "target": "src\\transformers\\models\\markuplm\\processing_markuplm.py"
      },
      {
        "source": "src\\transformers\\models\\markuplm",
        "target": "src\\transformers\\models\\markuplm\\tokenization_markuplm.py"
      },
      {
        "source": "src\\transformers\\models\\markuplm",
        "target": "src\\transformers\\models\\markuplm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mask2former",
        "target": "src\\transformers\\models\\mask2former\\configuration_mask2former.py"
      },
      {
        "source": "src\\transformers\\models\\mask2former",
        "target": "src\\transformers\\models\\mask2former\\convert_mask2former_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mask2former",
        "target": "src\\transformers\\models\\mask2former\\image_processing_mask2former.py"
      },
      {
        "source": "src\\transformers\\models\\mask2former",
        "target": "src\\transformers\\models\\mask2former\\image_processing_mask2former_fast.py"
      },
      {
        "source": "src\\transformers\\models\\mask2former",
        "target": "src\\transformers\\models\\mask2former\\modeling_mask2former.py"
      },
      {
        "source": "src\\transformers\\models\\mask2former",
        "target": "src\\transformers\\models\\mask2former\\modular_mask2former.py"
      },
      {
        "source": "src\\transformers\\models\\mask2former",
        "target": "src\\transformers\\models\\mask2former\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\maskformer",
        "target": "src\\transformers\\models\\maskformer\\configuration_maskformer.py"
      },
      {
        "source": "src\\transformers\\models\\maskformer",
        "target": "src\\transformers\\models\\maskformer\\configuration_maskformer_swin.py"
      },
      {
        "source": "src\\transformers\\models\\maskformer",
        "target": "src\\transformers\\models\\maskformer\\convert_maskformer_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\maskformer",
        "target": "src\\transformers\\models\\maskformer\\convert_maskformer_resnet_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\maskformer",
        "target": "src\\transformers\\models\\maskformer\\convert_maskformer_swin_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\maskformer",
        "target": "src\\transformers\\models\\maskformer\\image_processing_maskformer.py"
      },
      {
        "source": "src\\transformers\\models\\maskformer",
        "target": "src\\transformers\\models\\maskformer\\image_processing_maskformer_fast.py"
      },
      {
        "source": "src\\transformers\\models\\maskformer",
        "target": "src\\transformers\\models\\maskformer\\modeling_maskformer.py"
      },
      {
        "source": "src\\transformers\\models\\maskformer",
        "target": "src\\transformers\\models\\maskformer\\modeling_maskformer_swin.py"
      },
      {
        "source": "src\\transformers\\models\\maskformer",
        "target": "src\\transformers\\models\\maskformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mbart",
        "target": "src\\transformers\\models\\mbart\\configuration_mbart.py"
      },
      {
        "source": "src\\transformers\\models\\mbart",
        "target": "src\\transformers\\models\\mbart\\convert_mbart_original_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mbart",
        "target": "src\\transformers\\models\\mbart\\modeling_mbart.py"
      },
      {
        "source": "src\\transformers\\models\\mbart",
        "target": "src\\transformers\\models\\mbart\\tokenization_mbart.py"
      },
      {
        "source": "src\\transformers\\models\\mbart",
        "target": "src\\transformers\\models\\mbart\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mbart50",
        "target": "src\\transformers\\models\\mbart50\\tokenization_mbart50.py"
      },
      {
        "source": "src\\transformers\\models\\mbart50",
        "target": "src\\transformers\\models\\mbart50\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\megatron_bert",
        "target": "src\\transformers\\models\\megatron_bert\\configuration_megatron_bert.py"
      },
      {
        "source": "src\\transformers\\models\\megatron_bert",
        "target": "src\\transformers\\models\\megatron_bert\\convert_megatron_bert_checkpoint.py"
      },
      {
        "source": "src\\transformers\\models\\megatron_bert",
        "target": "src\\transformers\\models\\megatron_bert\\modeling_megatron_bert.py"
      },
      {
        "source": "src\\transformers\\models\\megatron_bert",
        "target": "src\\transformers\\models\\megatron_bert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\megatron_gpt2",
        "target": "src\\transformers\\models\\megatron_gpt2\\checkpoint_reshaping_and_interoperability.py"
      },
      {
        "source": "src\\transformers\\models\\megatron_gpt2",
        "target": "src\\transformers\\models\\megatron_gpt2\\convert_megatron_gpt2_checkpoint.py"
      },
      {
        "source": "src\\transformers\\models\\megatron_gpt2",
        "target": "src\\transformers\\models\\megatron_gpt2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\metaclip_2",
        "target": "src\\transformers\\models\\metaclip_2\\configuration_metaclip_2.py"
      },
      {
        "source": "src\\transformers\\models\\metaclip_2",
        "target": "src\\transformers\\models\\metaclip_2\\convert_metaclip_2_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\metaclip_2",
        "target": "src\\transformers\\models\\metaclip_2\\modeling_metaclip_2.py"
      },
      {
        "source": "src\\transformers\\models\\metaclip_2",
        "target": "src\\transformers\\models\\metaclip_2\\modular_metaclip_2.py"
      },
      {
        "source": "src\\transformers\\models\\metaclip_2",
        "target": "src\\transformers\\models\\metaclip_2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mgp_str",
        "target": "src\\transformers\\models\\mgp_str\\configuration_mgp_str.py"
      },
      {
        "source": "src\\transformers\\models\\mgp_str",
        "target": "src\\transformers\\models\\mgp_str\\modeling_mgp_str.py"
      },
      {
        "source": "src\\transformers\\models\\mgp_str",
        "target": "src\\transformers\\models\\mgp_str\\processing_mgp_str.py"
      },
      {
        "source": "src\\transformers\\models\\mgp_str",
        "target": "src\\transformers\\models\\mgp_str\\tokenization_mgp_str.py"
      },
      {
        "source": "src\\transformers\\models\\mgp_str",
        "target": "src\\transformers\\models\\mgp_str\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mimi",
        "target": "src\\transformers\\models\\mimi\\configuration_mimi.py"
      },
      {
        "source": "src\\transformers\\models\\mimi",
        "target": "src\\transformers\\models\\mimi\\convert_mimi_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mimi",
        "target": "src\\transformers\\models\\mimi\\modeling_mimi.py"
      },
      {
        "source": "src\\transformers\\models\\mimi",
        "target": "src\\transformers\\models\\mimi\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\minimax",
        "target": "src\\transformers\\models\\minimax\\configuration_minimax.py"
      },
      {
        "source": "src\\transformers\\models\\minimax",
        "target": "src\\transformers\\models\\minimax\\modeling_minimax.py"
      },
      {
        "source": "src\\transformers\\models\\minimax",
        "target": "src\\transformers\\models\\minimax\\modular_minimax.py"
      },
      {
        "source": "src\\transformers\\models\\minimax",
        "target": "src\\transformers\\models\\minimax\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\minimax_m2",
        "target": "src\\transformers\\models\\minimax_m2\\configuration_minimax_m2.py"
      },
      {
        "source": "src\\transformers\\models\\minimax_m2",
        "target": "src\\transformers\\models\\minimax_m2\\modeling_minimax_m2.py"
      },
      {
        "source": "src\\transformers\\models\\minimax_m2",
        "target": "src\\transformers\\models\\minimax_m2\\modular_minimax_m2.py"
      },
      {
        "source": "src\\transformers\\models\\minimax_m2",
        "target": "src\\transformers\\models\\minimax_m2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\ministral",
        "target": "src\\transformers\\models\\ministral\\configuration_ministral.py"
      },
      {
        "source": "src\\transformers\\models\\ministral",
        "target": "src\\transformers\\models\\ministral\\modeling_ministral.py"
      },
      {
        "source": "src\\transformers\\models\\ministral",
        "target": "src\\transformers\\models\\ministral\\modular_ministral.py"
      },
      {
        "source": "src\\transformers\\models\\ministral",
        "target": "src\\transformers\\models\\ministral\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\ministral3",
        "target": "src\\transformers\\models\\ministral3\\configuration_ministral3.py"
      },
      {
        "source": "src\\transformers\\models\\ministral3",
        "target": "src\\transformers\\models\\ministral3\\convert_ministral3_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\ministral3",
        "target": "src\\transformers\\models\\ministral3\\modeling_ministral3.py"
      },
      {
        "source": "src\\transformers\\models\\ministral3",
        "target": "src\\transformers\\models\\ministral3\\modular_ministral3.py"
      },
      {
        "source": "src\\transformers\\models\\ministral3",
        "target": "src\\transformers\\models\\ministral3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mistral",
        "target": "src\\transformers\\models\\mistral\\configuration_mistral.py"
      },
      {
        "source": "src\\transformers\\models\\mistral",
        "target": "src\\transformers\\models\\mistral\\convert_mistral_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\mistral",
        "target": "src\\transformers\\models\\mistral\\modeling_mistral.py"
      },
      {
        "source": "src\\transformers\\models\\mistral",
        "target": "src\\transformers\\models\\mistral\\modular_mistral.py"
      },
      {
        "source": "src\\transformers\\models\\mistral",
        "target": "src\\transformers\\models\\mistral\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mistral3",
        "target": "src\\transformers\\models\\mistral3\\configuration_mistral3.py"
      },
      {
        "source": "src\\transformers\\models\\mistral3",
        "target": "src\\transformers\\models\\mistral3\\convert_mistral3_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\mistral3",
        "target": "src\\transformers\\models\\mistral3\\modeling_mistral3.py"
      },
      {
        "source": "src\\transformers\\models\\mistral3",
        "target": "src\\transformers\\models\\mistral3\\modular_mistral3.py"
      },
      {
        "source": "src\\transformers\\models\\mistral3",
        "target": "src\\transformers\\models\\mistral3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mixtral",
        "target": "src\\transformers\\models\\mixtral\\configuration_mixtral.py"
      },
      {
        "source": "src\\transformers\\models\\mixtral",
        "target": "src\\transformers\\models\\mixtral\\convert_mixtral_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\mixtral",
        "target": "src\\transformers\\models\\mixtral\\modeling_mixtral.py"
      },
      {
        "source": "src\\transformers\\models\\mixtral",
        "target": "src\\transformers\\models\\mixtral\\modular_mixtral.py"
      },
      {
        "source": "src\\transformers\\models\\mixtral",
        "target": "src\\transformers\\models\\mixtral\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mlcd",
        "target": "src\\transformers\\models\\mlcd\\configuration_mlcd.py"
      },
      {
        "source": "src\\transformers\\models\\mlcd",
        "target": "src\\transformers\\models\\mlcd\\convert_mlcd_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\mlcd",
        "target": "src\\transformers\\models\\mlcd\\modeling_mlcd.py"
      },
      {
        "source": "src\\transformers\\models\\mlcd",
        "target": "src\\transformers\\models\\mlcd\\modular_mlcd.py"
      },
      {
        "source": "src\\transformers\\models\\mlcd",
        "target": "src\\transformers\\models\\mlcd\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mllama",
        "target": "src\\transformers\\models\\mllama\\configuration_mllama.py"
      },
      {
        "source": "src\\transformers\\models\\mllama",
        "target": "src\\transformers\\models\\mllama\\convert_mllama_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\mllama",
        "target": "src\\transformers\\models\\mllama\\image_processing_mllama.py"
      },
      {
        "source": "src\\transformers\\models\\mllama",
        "target": "src\\transformers\\models\\mllama\\image_processing_mllama_fast.py"
      },
      {
        "source": "src\\transformers\\models\\mllama",
        "target": "src\\transformers\\models\\mllama\\modeling_mllama.py"
      },
      {
        "source": "src\\transformers\\models\\mllama",
        "target": "src\\transformers\\models\\mllama\\processing_mllama.py"
      },
      {
        "source": "src\\transformers\\models\\mllama",
        "target": "src\\transformers\\models\\mllama\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mluke",
        "target": "src\\transformers\\models\\mluke\\convert_mluke_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mluke",
        "target": "src\\transformers\\models\\mluke\\tokenization_mluke.py"
      },
      {
        "source": "src\\transformers\\models\\mluke",
        "target": "src\\transformers\\models\\mluke\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mm_grounding_dino",
        "target": "src\\transformers\\models\\mm_grounding_dino\\configuration_mm_grounding_dino.py"
      },
      {
        "source": "src\\transformers\\models\\mm_grounding_dino",
        "target": "src\\transformers\\models\\mm_grounding_dino\\convert_mm_grounding_dino_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\mm_grounding_dino",
        "target": "src\\transformers\\models\\mm_grounding_dino\\modeling_mm_grounding_dino.py"
      },
      {
        "source": "src\\transformers\\models\\mm_grounding_dino",
        "target": "src\\transformers\\models\\mm_grounding_dino\\modular_mm_grounding_dino.py"
      },
      {
        "source": "src\\transformers\\models\\mm_grounding_dino",
        "target": "src\\transformers\\models\\mm_grounding_dino\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mobilebert",
        "target": "src\\transformers\\models\\mobilebert\\configuration_mobilebert.py"
      },
      {
        "source": "src\\transformers\\models\\mobilebert",
        "target": "src\\transformers\\models\\mobilebert\\convert_mobilebert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mobilebert",
        "target": "src\\transformers\\models\\mobilebert\\modeling_mobilebert.py"
      },
      {
        "source": "src\\transformers\\models\\mobilebert",
        "target": "src\\transformers\\models\\mobilebert\\tokenization_mobilebert.py"
      },
      {
        "source": "src\\transformers\\models\\mobilebert",
        "target": "src\\transformers\\models\\mobilebert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v1",
        "target": "src\\transformers\\models\\mobilenet_v1\\configuration_mobilenet_v1.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v1",
        "target": "src\\transformers\\models\\mobilenet_v1\\convert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v1",
        "target": "src\\transformers\\models\\mobilenet_v1\\image_processing_mobilenet_v1.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v1",
        "target": "src\\transformers\\models\\mobilenet_v1\\image_processing_mobilenet_v1_fast.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v1",
        "target": "src\\transformers\\models\\mobilenet_v1\\modeling_mobilenet_v1.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v1",
        "target": "src\\transformers\\models\\mobilenet_v1\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v2",
        "target": "src\\transformers\\models\\mobilenet_v2\\configuration_mobilenet_v2.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v2",
        "target": "src\\transformers\\models\\mobilenet_v2\\convert_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v2",
        "target": "src\\transformers\\models\\mobilenet_v2\\image_processing_mobilenet_v2.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v2",
        "target": "src\\transformers\\models\\mobilenet_v2\\image_processing_mobilenet_v2_fast.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v2",
        "target": "src\\transformers\\models\\mobilenet_v2\\modeling_mobilenet_v2.py"
      },
      {
        "source": "src\\transformers\\models\\mobilenet_v2",
        "target": "src\\transformers\\models\\mobilenet_v2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mobilevit",
        "target": "src\\transformers\\models\\mobilevit\\configuration_mobilevit.py"
      },
      {
        "source": "src\\transformers\\models\\mobilevit",
        "target": "src\\transformers\\models\\mobilevit\\convert_mlcvnets_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mobilevit",
        "target": "src\\transformers\\models\\mobilevit\\image_processing_mobilevit.py"
      },
      {
        "source": "src\\transformers\\models\\mobilevit",
        "target": "src\\transformers\\models\\mobilevit\\image_processing_mobilevit_fast.py"
      },
      {
        "source": "src\\transformers\\models\\mobilevit",
        "target": "src\\transformers\\models\\mobilevit\\modeling_mobilevit.py"
      },
      {
        "source": "src\\transformers\\models\\mobilevit",
        "target": "src\\transformers\\models\\mobilevit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mobilevitv2",
        "target": "src\\transformers\\models\\mobilevitv2\\configuration_mobilevitv2.py"
      },
      {
        "source": "src\\transformers\\models\\mobilevitv2",
        "target": "src\\transformers\\models\\mobilevitv2\\convert_mlcvnets_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mobilevitv2",
        "target": "src\\transformers\\models\\mobilevitv2\\modeling_mobilevitv2.py"
      },
      {
        "source": "src\\transformers\\models\\mobilevitv2",
        "target": "src\\transformers\\models\\mobilevitv2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\modernbert",
        "target": "src\\transformers\\models\\modernbert\\configuration_modernbert.py"
      },
      {
        "source": "src\\transformers\\models\\modernbert",
        "target": "src\\transformers\\models\\modernbert\\modeling_modernbert.py"
      },
      {
        "source": "src\\transformers\\models\\modernbert",
        "target": "src\\transformers\\models\\modernbert\\modular_modernbert.py"
      },
      {
        "source": "src\\transformers\\models\\modernbert",
        "target": "src\\transformers\\models\\modernbert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\modernbert_decoder",
        "target": "src\\transformers\\models\\modernbert_decoder\\configuration_modernbert_decoder.py"
      },
      {
        "source": "src\\transformers\\models\\modernbert_decoder",
        "target": "src\\transformers\\models\\modernbert_decoder\\modeling_modernbert_decoder.py"
      },
      {
        "source": "src\\transformers\\models\\modernbert_decoder",
        "target": "src\\transformers\\models\\modernbert_decoder\\modular_modernbert_decoder.py"
      },
      {
        "source": "src\\transformers\\models\\modernbert_decoder",
        "target": "src\\transformers\\models\\modernbert_decoder\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\moonshine",
        "target": "src\\transformers\\models\\moonshine\\configuration_moonshine.py"
      },
      {
        "source": "src\\transformers\\models\\moonshine",
        "target": "src\\transformers\\models\\moonshine\\convert_usefulsensors_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\moonshine",
        "target": "src\\transformers\\models\\moonshine\\modeling_moonshine.py"
      },
      {
        "source": "src\\transformers\\models\\moonshine",
        "target": "src\\transformers\\models\\moonshine\\modular_moonshine.py"
      },
      {
        "source": "src\\transformers\\models\\moonshine",
        "target": "src\\transformers\\models\\moonshine\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\moshi",
        "target": "src\\transformers\\models\\moshi\\configuration_moshi.py"
      },
      {
        "source": "src\\transformers\\models\\moshi",
        "target": "src\\transformers\\models\\moshi\\convert_moshi_transformers.py"
      },
      {
        "source": "src\\transformers\\models\\moshi",
        "target": "src\\transformers\\models\\moshi\\modeling_moshi.py"
      },
      {
        "source": "src\\transformers\\models\\moshi",
        "target": "src\\transformers\\models\\moshi\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mpnet",
        "target": "src\\transformers\\models\\mpnet\\configuration_mpnet.py"
      },
      {
        "source": "src\\transformers\\models\\mpnet",
        "target": "src\\transformers\\models\\mpnet\\modeling_mpnet.py"
      },
      {
        "source": "src\\transformers\\models\\mpnet",
        "target": "src\\transformers\\models\\mpnet\\tokenization_mpnet.py"
      },
      {
        "source": "src\\transformers\\models\\mpnet",
        "target": "src\\transformers\\models\\mpnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mpt",
        "target": "src\\transformers\\models\\mpt\\configuration_mpt.py"
      },
      {
        "source": "src\\transformers\\models\\mpt",
        "target": "src\\transformers\\models\\mpt\\modeling_mpt.py"
      },
      {
        "source": "src\\transformers\\models\\mpt",
        "target": "src\\transformers\\models\\mpt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mra",
        "target": "src\\transformers\\models\\mra\\configuration_mra.py"
      },
      {
        "source": "src\\transformers\\models\\mra",
        "target": "src\\transformers\\models\\mra\\convert_mra_pytorch_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\mra",
        "target": "src\\transformers\\models\\mra\\modeling_mra.py"
      },
      {
        "source": "src\\transformers\\models\\mra",
        "target": "src\\transformers\\models\\mra\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mt5",
        "target": "src\\transformers\\models\\mt5\\configuration_mt5.py"
      },
      {
        "source": "src\\transformers\\models\\mt5",
        "target": "src\\transformers\\models\\mt5\\modeling_mt5.py"
      },
      {
        "source": "src\\transformers\\models\\mt5",
        "target": "src\\transformers\\models\\mt5\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen",
        "target": "src\\transformers\\models\\musicgen\\configuration_musicgen.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen",
        "target": "src\\transformers\\models\\musicgen\\convert_musicgen_transformers.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen",
        "target": "src\\transformers\\models\\musicgen\\modeling_musicgen.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen",
        "target": "src\\transformers\\models\\musicgen\\processing_musicgen.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen",
        "target": "src\\transformers\\models\\musicgen\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen_melody",
        "target": "src\\transformers\\models\\musicgen_melody\\configuration_musicgen_melody.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen_melody",
        "target": "src\\transformers\\models\\musicgen_melody\\convert_musicgen_melody_transformers.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen_melody",
        "target": "src\\transformers\\models\\musicgen_melody\\feature_extraction_musicgen_melody.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen_melody",
        "target": "src\\transformers\\models\\musicgen_melody\\modeling_musicgen_melody.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen_melody",
        "target": "src\\transformers\\models\\musicgen_melody\\processing_musicgen_melody.py"
      },
      {
        "source": "src\\transformers\\models\\musicgen_melody",
        "target": "src\\transformers\\models\\musicgen_melody\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\mvp",
        "target": "src\\transformers\\models\\mvp\\configuration_mvp.py"
      },
      {
        "source": "src\\transformers\\models\\mvp",
        "target": "src\\transformers\\models\\mvp\\modeling_mvp.py"
      },
      {
        "source": "src\\transformers\\models\\mvp",
        "target": "src\\transformers\\models\\mvp\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\myt5",
        "target": "src\\transformers\\models\\myt5\\convert_myt5_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\myt5",
        "target": "src\\transformers\\models\\myt5\\tokenization_myt5.py"
      },
      {
        "source": "src\\transformers\\models\\myt5",
        "target": "src\\transformers\\models\\myt5\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\nanochat",
        "target": "src\\transformers\\models\\nanochat\\configuration_nanochat.py"
      },
      {
        "source": "src\\transformers\\models\\nanochat",
        "target": "src\\transformers\\models\\nanochat\\convert_nanochat_checkpoints.py"
      },
      {
        "source": "src\\transformers\\models\\nanochat",
        "target": "src\\transformers\\models\\nanochat\\modeling_nanochat.py"
      },
      {
        "source": "src\\transformers\\models\\nanochat",
        "target": "src\\transformers\\models\\nanochat\\modular_nanochat.py"
      },
      {
        "source": "src\\transformers\\models\\nanochat",
        "target": "src\\transformers\\models\\nanochat\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\nemotron",
        "target": "src\\transformers\\models\\nemotron\\configuration_nemotron.py"
      },
      {
        "source": "src\\transformers\\models\\nemotron",
        "target": "src\\transformers\\models\\nemotron\\convert_nemotron_nemo_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\nemotron",
        "target": "src\\transformers\\models\\nemotron\\modeling_nemotron.py"
      },
      {
        "source": "src\\transformers\\models\\nemotron",
        "target": "src\\transformers\\models\\nemotron\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\nllb",
        "target": "src\\transformers\\models\\nllb\\tokenization_nllb.py"
      },
      {
        "source": "src\\transformers\\models\\nllb",
        "target": "src\\transformers\\models\\nllb\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\nllb_moe",
        "target": "src\\transformers\\models\\nllb_moe\\configuration_nllb_moe.py"
      },
      {
        "source": "src\\transformers\\models\\nllb_moe",
        "target": "src\\transformers\\models\\nllb_moe\\convert_nllb_moe_sharded_original_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\nllb_moe",
        "target": "src\\transformers\\models\\nllb_moe\\modeling_nllb_moe.py"
      },
      {
        "source": "src\\transformers\\models\\nllb_moe",
        "target": "src\\transformers\\models\\nllb_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\nougat",
        "target": "src\\transformers\\models\\nougat\\convert_nougat_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\nougat",
        "target": "src\\transformers\\models\\nougat\\image_processing_nougat.py"
      },
      {
        "source": "src\\transformers\\models\\nougat",
        "target": "src\\transformers\\models\\nougat\\image_processing_nougat_fast.py"
      },
      {
        "source": "src\\transformers\\models\\nougat",
        "target": "src\\transformers\\models\\nougat\\processing_nougat.py"
      },
      {
        "source": "src\\transformers\\models\\nougat",
        "target": "src\\transformers\\models\\nougat\\tokenization_nougat.py"
      },
      {
        "source": "src\\transformers\\models\\nougat",
        "target": "src\\transformers\\models\\nougat\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\nystromformer",
        "target": "src\\transformers\\models\\nystromformer\\configuration_nystromformer.py"
      },
      {
        "source": "src\\transformers\\models\\nystromformer",
        "target": "src\\transformers\\models\\nystromformer\\convert_nystromformer_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\nystromformer",
        "target": "src\\transformers\\models\\nystromformer\\modeling_nystromformer.py"
      },
      {
        "source": "src\\transformers\\models\\nystromformer",
        "target": "src\\transformers\\models\\nystromformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\olmo",
        "target": "src\\transformers\\models\\olmo\\configuration_olmo.py"
      },
      {
        "source": "src\\transformers\\models\\olmo",
        "target": "src\\transformers\\models\\olmo\\convert_olmo_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\olmo",
        "target": "src\\transformers\\models\\olmo\\modeling_olmo.py"
      },
      {
        "source": "src\\transformers\\models\\olmo",
        "target": "src\\transformers\\models\\olmo\\modular_olmo.py"
      },
      {
        "source": "src\\transformers\\models\\olmo",
        "target": "src\\transformers\\models\\olmo\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\olmo2",
        "target": "src\\transformers\\models\\olmo2\\configuration_olmo2.py"
      },
      {
        "source": "src\\transformers\\models\\olmo2",
        "target": "src\\transformers\\models\\olmo2\\convert_olmo2_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\olmo2",
        "target": "src\\transformers\\models\\olmo2\\modeling_olmo2.py"
      },
      {
        "source": "src\\transformers\\models\\olmo2",
        "target": "src\\transformers\\models\\olmo2\\modular_olmo2.py"
      },
      {
        "source": "src\\transformers\\models\\olmo2",
        "target": "src\\transformers\\models\\olmo2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\olmo3",
        "target": "src\\transformers\\models\\olmo3\\configuration_olmo3.py"
      },
      {
        "source": "src\\transformers\\models\\olmo3",
        "target": "src\\transformers\\models\\olmo3\\convert_olmo3_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\olmo3",
        "target": "src\\transformers\\models\\olmo3\\modeling_olmo3.py"
      },
      {
        "source": "src\\transformers\\models\\olmo3",
        "target": "src\\transformers\\models\\olmo3\\modular_olmo3.py"
      },
      {
        "source": "src\\transformers\\models\\olmo3",
        "target": "src\\transformers\\models\\olmo3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\olmoe",
        "target": "src\\transformers\\models\\olmoe\\configuration_olmoe.py"
      },
      {
        "source": "src\\transformers\\models\\olmoe",
        "target": "src\\transformers\\models\\olmoe\\convert_olmoe_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\olmoe",
        "target": "src\\transformers\\models\\olmoe\\modeling_olmoe.py"
      },
      {
        "source": "src\\transformers\\models\\olmoe",
        "target": "src\\transformers\\models\\olmoe\\modular_olmoe.py"
      },
      {
        "source": "src\\transformers\\models\\olmoe",
        "target": "src\\transformers\\models\\olmoe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\omdet_turbo",
        "target": "src\\transformers\\models\\omdet_turbo\\configuration_omdet_turbo.py"
      },
      {
        "source": "src\\transformers\\models\\omdet_turbo",
        "target": "src\\transformers\\models\\omdet_turbo\\convert_omdet_turbo_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\omdet_turbo",
        "target": "src\\transformers\\models\\omdet_turbo\\modeling_omdet_turbo.py"
      },
      {
        "source": "src\\transformers\\models\\omdet_turbo",
        "target": "src\\transformers\\models\\omdet_turbo\\processing_omdet_turbo.py"
      },
      {
        "source": "src\\transformers\\models\\omdet_turbo",
        "target": "src\\transformers\\models\\omdet_turbo\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\oneformer",
        "target": "src\\transformers\\models\\oneformer\\configuration_oneformer.py"
      },
      {
        "source": "src\\transformers\\models\\oneformer",
        "target": "src\\transformers\\models\\oneformer\\convert_to_hf_oneformer.py"
      },
      {
        "source": "src\\transformers\\models\\oneformer",
        "target": "src\\transformers\\models\\oneformer\\image_processing_oneformer.py"
      },
      {
        "source": "src\\transformers\\models\\oneformer",
        "target": "src\\transformers\\models\\oneformer\\image_processing_oneformer_fast.py"
      },
      {
        "source": "src\\transformers\\models\\oneformer",
        "target": "src\\transformers\\models\\oneformer\\modeling_oneformer.py"
      },
      {
        "source": "src\\transformers\\models\\oneformer",
        "target": "src\\transformers\\models\\oneformer\\processing_oneformer.py"
      },
      {
        "source": "src\\transformers\\models\\oneformer",
        "target": "src\\transformers\\models\\oneformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\openai",
        "target": "src\\transformers\\models\\openai\\configuration_openai.py"
      },
      {
        "source": "src\\transformers\\models\\openai",
        "target": "src\\transformers\\models\\openai\\convert_openai_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\openai",
        "target": "src\\transformers\\models\\openai\\modeling_openai.py"
      },
      {
        "source": "src\\transformers\\models\\openai",
        "target": "src\\transformers\\models\\openai\\tokenization_openai.py"
      },
      {
        "source": "src\\transformers\\models\\openai",
        "target": "src\\transformers\\models\\openai\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\opt",
        "target": "src\\transformers\\models\\opt\\configuration_opt.py"
      },
      {
        "source": "src\\transformers\\models\\opt",
        "target": "src\\transformers\\models\\opt\\convert_opt_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\opt",
        "target": "src\\transformers\\models\\opt\\modeling_opt.py"
      },
      {
        "source": "src\\transformers\\models\\opt",
        "target": "src\\transformers\\models\\opt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\ovis2",
        "target": "src\\transformers\\models\\ovis2\\configuration_ovis2.py"
      },
      {
        "source": "src\\transformers\\models\\ovis2",
        "target": "src\\transformers\\models\\ovis2\\convert_ovis2_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\ovis2",
        "target": "src\\transformers\\models\\ovis2\\image_processing_ovis2.py"
      },
      {
        "source": "src\\transformers\\models\\ovis2",
        "target": "src\\transformers\\models\\ovis2\\image_processing_ovis2_fast.py"
      },
      {
        "source": "src\\transformers\\models\\ovis2",
        "target": "src\\transformers\\models\\ovis2\\modeling_ovis2.py"
      },
      {
        "source": "src\\transformers\\models\\ovis2",
        "target": "src\\transformers\\models\\ovis2\\modular_ovis2.py"
      },
      {
        "source": "src\\transformers\\models\\ovis2",
        "target": "src\\transformers\\models\\ovis2\\processing_ovis2.py"
      },
      {
        "source": "src\\transformers\\models\\ovis2",
        "target": "src\\transformers\\models\\ovis2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\owlv2",
        "target": "src\\transformers\\models\\owlv2\\configuration_owlv2.py"
      },
      {
        "source": "src\\transformers\\models\\owlv2",
        "target": "src\\transformers\\models\\owlv2\\convert_owlv2_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\owlv2",
        "target": "src\\transformers\\models\\owlv2\\image_processing_owlv2.py"
      },
      {
        "source": "src\\transformers\\models\\owlv2",
        "target": "src\\transformers\\models\\owlv2\\image_processing_owlv2_fast.py"
      },
      {
        "source": "src\\transformers\\models\\owlv2",
        "target": "src\\transformers\\models\\owlv2\\modeling_owlv2.py"
      },
      {
        "source": "src\\transformers\\models\\owlv2",
        "target": "src\\transformers\\models\\owlv2\\modular_owlv2.py"
      },
      {
        "source": "src\\transformers\\models\\owlv2",
        "target": "src\\transformers\\models\\owlv2\\processing_owlv2.py"
      },
      {
        "source": "src\\transformers\\models\\owlv2",
        "target": "src\\transformers\\models\\owlv2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\owlvit",
        "target": "src\\transformers\\models\\owlvit\\configuration_owlvit.py"
      },
      {
        "source": "src\\transformers\\models\\owlvit",
        "target": "src\\transformers\\models\\owlvit\\convert_owlvit_original_flax_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\owlvit",
        "target": "src\\transformers\\models\\owlvit\\image_processing_owlvit.py"
      },
      {
        "source": "src\\transformers\\models\\owlvit",
        "target": "src\\transformers\\models\\owlvit\\image_processing_owlvit_fast.py"
      },
      {
        "source": "src\\transformers\\models\\owlvit",
        "target": "src\\transformers\\models\\owlvit\\modeling_owlvit.py"
      },
      {
        "source": "src\\transformers\\models\\owlvit",
        "target": "src\\transformers\\models\\owlvit\\processing_owlvit.py"
      },
      {
        "source": "src\\transformers\\models\\owlvit",
        "target": "src\\transformers\\models\\owlvit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\paddleocr_vl",
        "target": "src\\transformers\\models\\paddleocr_vl\\configuration_paddleocr_vl.py"
      },
      {
        "source": "src\\transformers\\models\\paddleocr_vl",
        "target": "src\\transformers\\models\\paddleocr_vl\\image_processing_paddleocr_vl.py"
      },
      {
        "source": "src\\transformers\\models\\paddleocr_vl",
        "target": "src\\transformers\\models\\paddleocr_vl\\image_processing_paddleocr_vl_fast.py"
      },
      {
        "source": "src\\transformers\\models\\paddleocr_vl",
        "target": "src\\transformers\\models\\paddleocr_vl\\modeling_paddleocr_vl.py"
      },
      {
        "source": "src\\transformers\\models\\paddleocr_vl",
        "target": "src\\transformers\\models\\paddleocr_vl\\modular_paddleocr_vl.py"
      },
      {
        "source": "src\\transformers\\models\\paddleocr_vl",
        "target": "src\\transformers\\models\\paddleocr_vl\\processing_paddleocr_vl.py"
      },
      {
        "source": "src\\transformers\\models\\paddleocr_vl",
        "target": "src\\transformers\\models\\paddleocr_vl\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\paligemma",
        "target": "src\\transformers\\models\\paligemma\\configuration_paligemma.py"
      },
      {
        "source": "src\\transformers\\models\\paligemma",
        "target": "src\\transformers\\models\\paligemma\\convert_paligemma2_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\paligemma",
        "target": "src\\transformers\\models\\paligemma\\convert_paligemma_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\paligemma",
        "target": "src\\transformers\\models\\paligemma\\modeling_paligemma.py"
      },
      {
        "source": "src\\transformers\\models\\paligemma",
        "target": "src\\transformers\\models\\paligemma\\processing_paligemma.py"
      },
      {
        "source": "src\\transformers\\models\\paligemma",
        "target": "src\\transformers\\models\\paligemma\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\parakeet",
        "target": "src\\transformers\\models\\parakeet\\configuration_parakeet.py"
      },
      {
        "source": "src\\transformers\\models\\parakeet",
        "target": "src\\transformers\\models\\parakeet\\convert_nemo_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\parakeet",
        "target": "src\\transformers\\models\\parakeet\\feature_extraction_parakeet.py"
      },
      {
        "source": "src\\transformers\\models\\parakeet",
        "target": "src\\transformers\\models\\parakeet\\modeling_parakeet.py"
      },
      {
        "source": "src\\transformers\\models\\parakeet",
        "target": "src\\transformers\\models\\parakeet\\modular_parakeet.py"
      },
      {
        "source": "src\\transformers\\models\\parakeet",
        "target": "src\\transformers\\models\\parakeet\\processing_parakeet.py"
      },
      {
        "source": "src\\transformers\\models\\parakeet",
        "target": "src\\transformers\\models\\parakeet\\tokenization_parakeet.py"
      },
      {
        "source": "src\\transformers\\models\\parakeet",
        "target": "src\\transformers\\models\\parakeet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\patchtsmixer",
        "target": "src\\transformers\\models\\patchtsmixer\\configuration_patchtsmixer.py"
      },
      {
        "source": "src\\transformers\\models\\patchtsmixer",
        "target": "src\\transformers\\models\\patchtsmixer\\modeling_patchtsmixer.py"
      },
      {
        "source": "src\\transformers\\models\\patchtsmixer",
        "target": "src\\transformers\\models\\patchtsmixer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\patchtst",
        "target": "src\\transformers\\models\\patchtst\\configuration_patchtst.py"
      },
      {
        "source": "src\\transformers\\models\\patchtst",
        "target": "src\\transformers\\models\\patchtst\\modeling_patchtst.py"
      },
      {
        "source": "src\\transformers\\models\\patchtst",
        "target": "src\\transformers\\models\\patchtst\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pegasus",
        "target": "src\\transformers\\models\\pegasus\\configuration_pegasus.py"
      },
      {
        "source": "src\\transformers\\models\\pegasus",
        "target": "src\\transformers\\models\\pegasus\\convert_pegasus_tf_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\pegasus",
        "target": "src\\transformers\\models\\pegasus\\modeling_pegasus.py"
      },
      {
        "source": "src\\transformers\\models\\pegasus",
        "target": "src\\transformers\\models\\pegasus\\tokenization_pegasus.py"
      },
      {
        "source": "src\\transformers\\models\\pegasus",
        "target": "src\\transformers\\models\\pegasus\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pegasus_x",
        "target": "src\\transformers\\models\\pegasus_x\\configuration_pegasus_x.py"
      },
      {
        "source": "src\\transformers\\models\\pegasus_x",
        "target": "src\\transformers\\models\\pegasus_x\\modeling_pegasus_x.py"
      },
      {
        "source": "src\\transformers\\models\\pegasus_x",
        "target": "src\\transformers\\models\\pegasus_x\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\perceiver",
        "target": "src\\transformers\\models\\perceiver\\configuration_perceiver.py"
      },
      {
        "source": "src\\transformers\\models\\perceiver",
        "target": "src\\transformers\\models\\perceiver\\convert_perceiver_haiku_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\perceiver",
        "target": "src\\transformers\\models\\perceiver\\image_processing_perceiver.py"
      },
      {
        "source": "src\\transformers\\models\\perceiver",
        "target": "src\\transformers\\models\\perceiver\\image_processing_perceiver_fast.py"
      },
      {
        "source": "src\\transformers\\models\\perceiver",
        "target": "src\\transformers\\models\\perceiver\\modeling_perceiver.py"
      },
      {
        "source": "src\\transformers\\models\\perceiver",
        "target": "src\\transformers\\models\\perceiver\\tokenization_perceiver.py"
      },
      {
        "source": "src\\transformers\\models\\perceiver",
        "target": "src\\transformers\\models\\perceiver\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\perception_lm",
        "target": "src\\transformers\\models\\perception_lm\\configuration_perception_lm.py"
      },
      {
        "source": "src\\transformers\\models\\perception_lm",
        "target": "src\\transformers\\models\\perception_lm\\convert_perception_lm_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\perception_lm",
        "target": "src\\transformers\\models\\perception_lm\\image_processing_perception_lm_fast.py"
      },
      {
        "source": "src\\transformers\\models\\perception_lm",
        "target": "src\\transformers\\models\\perception_lm\\modeling_perception_lm.py"
      },
      {
        "source": "src\\transformers\\models\\perception_lm",
        "target": "src\\transformers\\models\\perception_lm\\modular_perception_lm.py"
      },
      {
        "source": "src\\transformers\\models\\perception_lm",
        "target": "src\\transformers\\models\\perception_lm\\processing_perception_lm.py"
      },
      {
        "source": "src\\transformers\\models\\perception_lm",
        "target": "src\\transformers\\models\\perception_lm\\video_processing_perception_lm.py"
      },
      {
        "source": "src\\transformers\\models\\perception_lm",
        "target": "src\\transformers\\models\\perception_lm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\persimmon",
        "target": "src\\transformers\\models\\persimmon\\configuration_persimmon.py"
      },
      {
        "source": "src\\transformers\\models\\persimmon",
        "target": "src\\transformers\\models\\persimmon\\convert_persimmon_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\persimmon",
        "target": "src\\transformers\\models\\persimmon\\modeling_persimmon.py"
      },
      {
        "source": "src\\transformers\\models\\persimmon",
        "target": "src\\transformers\\models\\persimmon\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio",
        "target": "src\\transformers\\models\\pe_audio\\configuration_pe_audio.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio",
        "target": "src\\transformers\\models\\pe_audio\\feature_extraction_pe_audio.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio",
        "target": "src\\transformers\\models\\pe_audio\\modeling_pe_audio.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio",
        "target": "src\\transformers\\models\\pe_audio\\modular_pe_audio.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio",
        "target": "src\\transformers\\models\\pe_audio\\processing_pe_audio.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio",
        "target": "src\\transformers\\models\\pe_audio\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio_video",
        "target": "src\\transformers\\models\\pe_audio_video\\configuration_pe_audio_video.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio_video",
        "target": "src\\transformers\\models\\pe_audio_video\\convert_pe_audio_video_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio_video",
        "target": "src\\transformers\\models\\pe_audio_video\\modeling_pe_audio_video.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio_video",
        "target": "src\\transformers\\models\\pe_audio_video\\modular_pe_audio_video.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio_video",
        "target": "src\\transformers\\models\\pe_audio_video\\processing_pe_audio_video.py"
      },
      {
        "source": "src\\transformers\\models\\pe_audio_video",
        "target": "src\\transformers\\models\\pe_audio_video\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pe_video",
        "target": "src\\transformers\\models\\pe_video\\configuration_pe_video.py"
      },
      {
        "source": "src\\transformers\\models\\pe_video",
        "target": "src\\transformers\\models\\pe_video\\modeling_pe_video.py"
      },
      {
        "source": "src\\transformers\\models\\pe_video",
        "target": "src\\transformers\\models\\pe_video\\modular_pe_video.py"
      },
      {
        "source": "src\\transformers\\models\\pe_video",
        "target": "src\\transformers\\models\\pe_video\\processing_pe_video.py"
      },
      {
        "source": "src\\transformers\\models\\pe_video",
        "target": "src\\transformers\\models\\pe_video\\video_processing_pe_video.py"
      },
      {
        "source": "src\\transformers\\models\\pe_video",
        "target": "src\\transformers\\models\\pe_video\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\phi",
        "target": "src\\transformers\\models\\phi\\configuration_phi.py"
      },
      {
        "source": "src\\transformers\\models\\phi",
        "target": "src\\transformers\\models\\phi\\convert_phi_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\phi",
        "target": "src\\transformers\\models\\phi\\modeling_phi.py"
      },
      {
        "source": "src\\transformers\\models\\phi",
        "target": "src\\transformers\\models\\phi\\modular_phi.py"
      },
      {
        "source": "src\\transformers\\models\\phi",
        "target": "src\\transformers\\models\\phi\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\phi3",
        "target": "src\\transformers\\models\\phi3\\configuration_phi3.py"
      },
      {
        "source": "src\\transformers\\models\\phi3",
        "target": "src\\transformers\\models\\phi3\\modeling_phi3.py"
      },
      {
        "source": "src\\transformers\\models\\phi3",
        "target": "src\\transformers\\models\\phi3\\modular_phi3.py"
      },
      {
        "source": "src\\transformers\\models\\phi3",
        "target": "src\\transformers\\models\\phi3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\phi4_multimodal",
        "target": "src\\transformers\\models\\phi4_multimodal\\configuration_phi4_multimodal.py"
      },
      {
        "source": "src\\transformers\\models\\phi4_multimodal",
        "target": "src\\transformers\\models\\phi4_multimodal\\convert_phi4_multimodal_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\phi4_multimodal",
        "target": "src\\transformers\\models\\phi4_multimodal\\feature_extraction_phi4_multimodal.py"
      },
      {
        "source": "src\\transformers\\models\\phi4_multimodal",
        "target": "src\\transformers\\models\\phi4_multimodal\\image_processing_phi4_multimodal_fast.py"
      },
      {
        "source": "src\\transformers\\models\\phi4_multimodal",
        "target": "src\\transformers\\models\\phi4_multimodal\\modeling_phi4_multimodal.py"
      },
      {
        "source": "src\\transformers\\models\\phi4_multimodal",
        "target": "src\\transformers\\models\\phi4_multimodal\\modular_phi4_multimodal.py"
      },
      {
        "source": "src\\transformers\\models\\phi4_multimodal",
        "target": "src\\transformers\\models\\phi4_multimodal\\processing_phi4_multimodal.py"
      },
      {
        "source": "src\\transformers\\models\\phi4_multimodal",
        "target": "src\\transformers\\models\\phi4_multimodal\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\phimoe",
        "target": "src\\transformers\\models\\phimoe\\configuration_phimoe.py"
      },
      {
        "source": "src\\transformers\\models\\phimoe",
        "target": "src\\transformers\\models\\phimoe\\modeling_phimoe.py"
      },
      {
        "source": "src\\transformers\\models\\phimoe",
        "target": "src\\transformers\\models\\phimoe\\modular_phimoe.py"
      },
      {
        "source": "src\\transformers\\models\\phimoe",
        "target": "src\\transformers\\models\\phimoe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\phobert",
        "target": "src\\transformers\\models\\phobert\\tokenization_phobert.py"
      },
      {
        "source": "src\\transformers\\models\\phobert",
        "target": "src\\transformers\\models\\phobert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pix2struct",
        "target": "src\\transformers\\models\\pix2struct\\configuration_pix2struct.py"
      },
      {
        "source": "src\\transformers\\models\\pix2struct",
        "target": "src\\transformers\\models\\pix2struct\\convert_pix2struct_original_pytorch_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\pix2struct",
        "target": "src\\transformers\\models\\pix2struct\\image_processing_pix2struct.py"
      },
      {
        "source": "src\\transformers\\models\\pix2struct",
        "target": "src\\transformers\\models\\pix2struct\\image_processing_pix2struct_fast.py"
      },
      {
        "source": "src\\transformers\\models\\pix2struct",
        "target": "src\\transformers\\models\\pix2struct\\modeling_pix2struct.py"
      },
      {
        "source": "src\\transformers\\models\\pix2struct",
        "target": "src\\transformers\\models\\pix2struct\\processing_pix2struct.py"
      },
      {
        "source": "src\\transformers\\models\\pix2struct",
        "target": "src\\transformers\\models\\pix2struct\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pixio",
        "target": "src\\transformers\\models\\pixio\\configuration_pixio.py"
      },
      {
        "source": "src\\transformers\\models\\pixio",
        "target": "src\\transformers\\models\\pixio\\convert_pixio_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\pixio",
        "target": "src\\transformers\\models\\pixio\\modeling_pixio.py"
      },
      {
        "source": "src\\transformers\\models\\pixio",
        "target": "src\\transformers\\models\\pixio\\modular_pixio.py"
      },
      {
        "source": "src\\transformers\\models\\pixio",
        "target": "src\\transformers\\models\\pixio\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pixtral",
        "target": "src\\transformers\\models\\pixtral\\configuration_pixtral.py"
      },
      {
        "source": "src\\transformers\\models\\pixtral",
        "target": "src\\transformers\\models\\pixtral\\convert_pixtral_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\pixtral",
        "target": "src\\transformers\\models\\pixtral\\image_processing_pixtral.py"
      },
      {
        "source": "src\\transformers\\models\\pixtral",
        "target": "src\\transformers\\models\\pixtral\\image_processing_pixtral_fast.py"
      },
      {
        "source": "src\\transformers\\models\\pixtral",
        "target": "src\\transformers\\models\\pixtral\\modeling_pixtral.py"
      },
      {
        "source": "src\\transformers\\models\\pixtral",
        "target": "src\\transformers\\models\\pixtral\\processing_pixtral.py"
      },
      {
        "source": "src\\transformers\\models\\pixtral",
        "target": "src\\transformers\\models\\pixtral\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\plbart",
        "target": "src\\transformers\\models\\plbart\\configuration_plbart.py"
      },
      {
        "source": "src\\transformers\\models\\plbart",
        "target": "src\\transformers\\models\\plbart\\convert_plbart_original_checkpoint_to_torch.py"
      },
      {
        "source": "src\\transformers\\models\\plbart",
        "target": "src\\transformers\\models\\plbart\\modeling_plbart.py"
      },
      {
        "source": "src\\transformers\\models\\plbart",
        "target": "src\\transformers\\models\\plbart\\modular_plbart.py"
      },
      {
        "source": "src\\transformers\\models\\plbart",
        "target": "src\\transformers\\models\\plbart\\tokenization_plbart.py"
      },
      {
        "source": "src\\transformers\\models\\plbart",
        "target": "src\\transformers\\models\\plbart\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\poolformer",
        "target": "src\\transformers\\models\\poolformer\\configuration_poolformer.py"
      },
      {
        "source": "src\\transformers\\models\\poolformer",
        "target": "src\\transformers\\models\\poolformer\\convert_poolformer_original_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\poolformer",
        "target": "src\\transformers\\models\\poolformer\\image_processing_poolformer.py"
      },
      {
        "source": "src\\transformers\\models\\poolformer",
        "target": "src\\transformers\\models\\poolformer\\image_processing_poolformer_fast.py"
      },
      {
        "source": "src\\transformers\\models\\poolformer",
        "target": "src\\transformers\\models\\poolformer\\modeling_poolformer.py"
      },
      {
        "source": "src\\transformers\\models\\poolformer",
        "target": "src\\transformers\\models\\poolformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pop2piano",
        "target": "src\\transformers\\models\\pop2piano\\configuration_pop2piano.py"
      },
      {
        "source": "src\\transformers\\models\\pop2piano",
        "target": "src\\transformers\\models\\pop2piano\\convert_pop2piano_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\pop2piano",
        "target": "src\\transformers\\models\\pop2piano\\feature_extraction_pop2piano.py"
      },
      {
        "source": "src\\transformers\\models\\pop2piano",
        "target": "src\\transformers\\models\\pop2piano\\modeling_pop2piano.py"
      },
      {
        "source": "src\\transformers\\models\\pop2piano",
        "target": "src\\transformers\\models\\pop2piano\\processing_pop2piano.py"
      },
      {
        "source": "src\\transformers\\models\\pop2piano",
        "target": "src\\transformers\\models\\pop2piano\\tokenization_pop2piano.py"
      },
      {
        "source": "src\\transformers\\models\\pop2piano",
        "target": "src\\transformers\\models\\pop2piano\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pp_doclayout_v3",
        "target": "src\\transformers\\models\\pp_doclayout_v3\\configuration_pp_doclayout_v3.py"
      },
      {
        "source": "src\\transformers\\models\\pp_doclayout_v3",
        "target": "src\\transformers\\models\\pp_doclayout_v3\\image_processing_pp_doclayout_v3_fast.py"
      },
      {
        "source": "src\\transformers\\models\\pp_doclayout_v3",
        "target": "src\\transformers\\models\\pp_doclayout_v3\\modeling_pp_doclayout_v3.py"
      },
      {
        "source": "src\\transformers\\models\\pp_doclayout_v3",
        "target": "src\\transformers\\models\\pp_doclayout_v3\\modular_pp_doclayout_v3.py"
      },
      {
        "source": "src\\transformers\\models\\pp_doclayout_v3",
        "target": "src\\transformers\\models\\pp_doclayout_v3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\prompt_depth_anything",
        "target": "src\\transformers\\models\\prompt_depth_anything\\configuration_prompt_depth_anything.py"
      },
      {
        "source": "src\\transformers\\models\\prompt_depth_anything",
        "target": "src\\transformers\\models\\prompt_depth_anything\\convert_prompt_depth_anything_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\prompt_depth_anything",
        "target": "src\\transformers\\models\\prompt_depth_anything\\image_processing_prompt_depth_anything.py"
      },
      {
        "source": "src\\transformers\\models\\prompt_depth_anything",
        "target": "src\\transformers\\models\\prompt_depth_anything\\image_processing_prompt_depth_anything_fast.py"
      },
      {
        "source": "src\\transformers\\models\\prompt_depth_anything",
        "target": "src\\transformers\\models\\prompt_depth_anything\\modeling_prompt_depth_anything.py"
      },
      {
        "source": "src\\transformers\\models\\prompt_depth_anything",
        "target": "src\\transformers\\models\\prompt_depth_anything\\modular_prompt_depth_anything.py"
      },
      {
        "source": "src\\transformers\\models\\prompt_depth_anything",
        "target": "src\\transformers\\models\\prompt_depth_anything\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\prophetnet",
        "target": "src\\transformers\\models\\prophetnet\\configuration_prophetnet.py"
      },
      {
        "source": "src\\transformers\\models\\prophetnet",
        "target": "src\\transformers\\models\\prophetnet\\convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\prophetnet",
        "target": "src\\transformers\\models\\prophetnet\\modeling_prophetnet.py"
      },
      {
        "source": "src\\transformers\\models\\prophetnet",
        "target": "src\\transformers\\models\\prophetnet\\tokenization_prophetnet.py"
      },
      {
        "source": "src\\transformers\\models\\prophetnet",
        "target": "src\\transformers\\models\\prophetnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pvt",
        "target": "src\\transformers\\models\\pvt\\configuration_pvt.py"
      },
      {
        "source": "src\\transformers\\models\\pvt",
        "target": "src\\transformers\\models\\pvt\\convert_pvt_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\pvt",
        "target": "src\\transformers\\models\\pvt\\image_processing_pvt.py"
      },
      {
        "source": "src\\transformers\\models\\pvt",
        "target": "src\\transformers\\models\\pvt\\image_processing_pvt_fast.py"
      },
      {
        "source": "src\\transformers\\models\\pvt",
        "target": "src\\transformers\\models\\pvt\\modeling_pvt.py"
      },
      {
        "source": "src\\transformers\\models\\pvt",
        "target": "src\\transformers\\models\\pvt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\pvt_v2",
        "target": "src\\transformers\\models\\pvt_v2\\configuration_pvt_v2.py"
      },
      {
        "source": "src\\transformers\\models\\pvt_v2",
        "target": "src\\transformers\\models\\pvt_v2\\convert_pvt_v2_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\pvt_v2",
        "target": "src\\transformers\\models\\pvt_v2\\modeling_pvt_v2.py"
      },
      {
        "source": "src\\transformers\\models\\pvt_v2",
        "target": "src\\transformers\\models\\pvt_v2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2",
        "target": "src\\transformers\\models\\qwen2\\configuration_qwen2.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2",
        "target": "src\\transformers\\models\\qwen2\\modeling_qwen2.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2",
        "target": "src\\transformers\\models\\qwen2\\modular_qwen2.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2",
        "target": "src\\transformers\\models\\qwen2\\tokenization_qwen2.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2",
        "target": "src\\transformers\\models\\qwen2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_5_omni",
        "target": "src\\transformers\\models\\qwen2_5_omni\\configuration_qwen2_5_omni.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_5_omni",
        "target": "src\\transformers\\models\\qwen2_5_omni\\modeling_qwen2_5_omni.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_5_omni",
        "target": "src\\transformers\\models\\qwen2_5_omni\\modular_qwen2_5_omni.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_5_omni",
        "target": "src\\transformers\\models\\qwen2_5_omni\\processing_qwen2_5_omni.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_5_omni",
        "target": "src\\transformers\\models\\qwen2_5_omni\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_5_vl",
        "target": "src\\transformers\\models\\qwen2_5_vl\\configuration_qwen2_5_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_5_vl",
        "target": "src\\transformers\\models\\qwen2_5_vl\\modeling_qwen2_5_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_5_vl",
        "target": "src\\transformers\\models\\qwen2_5_vl\\modular_qwen2_5_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_5_vl",
        "target": "src\\transformers\\models\\qwen2_5_vl\\processing_qwen2_5_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_5_vl",
        "target": "src\\transformers\\models\\qwen2_5_vl\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_audio",
        "target": "src\\transformers\\models\\qwen2_audio\\configuration_qwen2_audio.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_audio",
        "target": "src\\transformers\\models\\qwen2_audio\\modeling_qwen2_audio.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_audio",
        "target": "src\\transformers\\models\\qwen2_audio\\processing_qwen2_audio.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_audio",
        "target": "src\\transformers\\models\\qwen2_audio\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_moe",
        "target": "src\\transformers\\models\\qwen2_moe\\configuration_qwen2_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_moe",
        "target": "src\\transformers\\models\\qwen2_moe\\modeling_qwen2_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_moe",
        "target": "src\\transformers\\models\\qwen2_moe\\modular_qwen2_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_moe",
        "target": "src\\transformers\\models\\qwen2_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_vl",
        "target": "src\\transformers\\models\\qwen2_vl\\configuration_qwen2_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_vl",
        "target": "src\\transformers\\models\\qwen2_vl\\image_processing_qwen2_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_vl",
        "target": "src\\transformers\\models\\qwen2_vl\\image_processing_qwen2_vl_fast.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_vl",
        "target": "src\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_vl",
        "target": "src\\transformers\\models\\qwen2_vl\\processing_qwen2_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_vl",
        "target": "src\\transformers\\models\\qwen2_vl\\video_processing_qwen2_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen2_vl",
        "target": "src\\transformers\\models\\qwen2_vl\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3",
        "target": "src\\transformers\\models\\qwen3\\configuration_qwen3.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3",
        "target": "src\\transformers\\models\\qwen3\\modeling_qwen3.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3",
        "target": "src\\transformers\\models\\qwen3\\modular_qwen3.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3",
        "target": "src\\transformers\\models\\qwen3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_moe",
        "target": "src\\transformers\\models\\qwen3_moe\\configuration_qwen3_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_moe",
        "target": "src\\transformers\\models\\qwen3_moe\\modeling_qwen3_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_moe",
        "target": "src\\transformers\\models\\qwen3_moe\\modular_qwen3_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_moe",
        "target": "src\\transformers\\models\\qwen3_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_next",
        "target": "src\\transformers\\models\\qwen3_next\\configuration_qwen3_next.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_next",
        "target": "src\\transformers\\models\\qwen3_next\\modeling_qwen3_next.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_next",
        "target": "src\\transformers\\models\\qwen3_next\\modular_qwen3_next.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_next",
        "target": "src\\transformers\\models\\qwen3_next\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_omni_moe",
        "target": "src\\transformers\\models\\qwen3_omni_moe\\configuration_qwen3_omni_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_omni_moe",
        "target": "src\\transformers\\models\\qwen3_omni_moe\\modeling_qwen3_omni_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_omni_moe",
        "target": "src\\transformers\\models\\qwen3_omni_moe\\modular_qwen3_omni_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_omni_moe",
        "target": "src\\transformers\\models\\qwen3_omni_moe\\processing_qwen3_omni_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_omni_moe",
        "target": "src\\transformers\\models\\qwen3_omni_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_vl",
        "target": "src\\transformers\\models\\qwen3_vl\\configuration_qwen3_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_vl",
        "target": "src\\transformers\\models\\qwen3_vl\\modeling_qwen3_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_vl",
        "target": "src\\transformers\\models\\qwen3_vl\\modular_qwen3_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_vl",
        "target": "src\\transformers\\models\\qwen3_vl\\processing_qwen3_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_vl",
        "target": "src\\transformers\\models\\qwen3_vl\\video_processing_qwen3_vl.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_vl",
        "target": "src\\transformers\\models\\qwen3_vl\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_vl_moe",
        "target": "src\\transformers\\models\\qwen3_vl_moe\\configuration_qwen3_vl_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_vl_moe",
        "target": "src\\transformers\\models\\qwen3_vl_moe\\modeling_qwen3_vl_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_vl_moe",
        "target": "src\\transformers\\models\\qwen3_vl_moe\\modular_qwen3_vl_moe.py"
      },
      {
        "source": "src\\transformers\\models\\qwen3_vl_moe",
        "target": "src\\transformers\\models\\qwen3_vl_moe\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\rag",
        "target": "src\\transformers\\models\\rag\\configuration_rag.py"
      },
      {
        "source": "src\\transformers\\models\\rag",
        "target": "src\\transformers\\models\\rag\\modeling_rag.py"
      },
      {
        "source": "src\\transformers\\models\\rag",
        "target": "src\\transformers\\models\\rag\\retrieval_rag.py"
      },
      {
        "source": "src\\transformers\\models\\rag",
        "target": "src\\transformers\\models\\rag\\tokenization_rag.py"
      },
      {
        "source": "src\\transformers\\models\\rag",
        "target": "src\\transformers\\models\\rag\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\recurrent_gemma",
        "target": "src\\transformers\\models\\recurrent_gemma\\configuration_recurrent_gemma.py"
      },
      {
        "source": "src\\transformers\\models\\recurrent_gemma",
        "target": "src\\transformers\\models\\recurrent_gemma\\convert_recurrent_gemma_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\recurrent_gemma",
        "target": "src\\transformers\\models\\recurrent_gemma\\modeling_recurrent_gemma.py"
      },
      {
        "source": "src\\transformers\\models\\recurrent_gemma",
        "target": "src\\transformers\\models\\recurrent_gemma\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\reformer",
        "target": "src\\transformers\\models\\reformer\\configuration_reformer.py"
      },
      {
        "source": "src\\transformers\\models\\reformer",
        "target": "src\\transformers\\models\\reformer\\convert_reformer_trax_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\reformer",
        "target": "src\\transformers\\models\\reformer\\modeling_reformer.py"
      },
      {
        "source": "src\\transformers\\models\\reformer",
        "target": "src\\transformers\\models\\reformer\\tokenization_reformer.py"
      },
      {
        "source": "src\\transformers\\models\\reformer",
        "target": "src\\transformers\\models\\reformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\regnet",
        "target": "src\\transformers\\models\\regnet\\configuration_regnet.py"
      },
      {
        "source": "src\\transformers\\models\\regnet",
        "target": "src\\transformers\\models\\regnet\\convert_regnet_seer_10b_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\regnet",
        "target": "src\\transformers\\models\\regnet\\convert_regnet_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\regnet",
        "target": "src\\transformers\\models\\regnet\\modeling_regnet.py"
      },
      {
        "source": "src\\transformers\\models\\regnet",
        "target": "src\\transformers\\models\\regnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\rembert",
        "target": "src\\transformers\\models\\rembert\\configuration_rembert.py"
      },
      {
        "source": "src\\transformers\\models\\rembert",
        "target": "src\\transformers\\models\\rembert\\convert_rembert_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\rembert",
        "target": "src\\transformers\\models\\rembert\\modeling_rembert.py"
      },
      {
        "source": "src\\transformers\\models\\rembert",
        "target": "src\\transformers\\models\\rembert\\tokenization_rembert.py"
      },
      {
        "source": "src\\transformers\\models\\rembert",
        "target": "src\\transformers\\models\\rembert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\resnet",
        "target": "src\\transformers\\models\\resnet\\configuration_resnet.py"
      },
      {
        "source": "src\\transformers\\models\\resnet",
        "target": "src\\transformers\\models\\resnet\\convert_resnet_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\resnet",
        "target": "src\\transformers\\models\\resnet\\modeling_resnet.py"
      },
      {
        "source": "src\\transformers\\models\\resnet",
        "target": "src\\transformers\\models\\resnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\roberta",
        "target": "src\\transformers\\models\\roberta\\configuration_roberta.py"
      },
      {
        "source": "src\\transformers\\models\\roberta",
        "target": "src\\transformers\\models\\roberta\\convert_roberta_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\roberta",
        "target": "src\\transformers\\models\\roberta\\modeling_roberta.py"
      },
      {
        "source": "src\\transformers\\models\\roberta",
        "target": "src\\transformers\\models\\roberta\\modular_roberta.py"
      },
      {
        "source": "src\\transformers\\models\\roberta",
        "target": "src\\transformers\\models\\roberta\\tokenization_roberta.py"
      },
      {
        "source": "src\\transformers\\models\\roberta",
        "target": "src\\transformers\\models\\roberta\\tokenization_roberta_old.py"
      },
      {
        "source": "src\\transformers\\models\\roberta",
        "target": "src\\transformers\\models\\roberta\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\roberta_prelayernorm",
        "target": "src\\transformers\\models\\roberta_prelayernorm\\configuration_roberta_prelayernorm.py"
      },
      {
        "source": "src\\transformers\\models\\roberta_prelayernorm",
        "target": "src\\transformers\\models\\roberta_prelayernorm\\convert_roberta_prelayernorm_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\roberta_prelayernorm",
        "target": "src\\transformers\\models\\roberta_prelayernorm\\modeling_roberta_prelayernorm.py"
      },
      {
        "source": "src\\transformers\\models\\roberta_prelayernorm",
        "target": "src\\transformers\\models\\roberta_prelayernorm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\roc_bert",
        "target": "src\\transformers\\models\\roc_bert\\configuration_roc_bert.py"
      },
      {
        "source": "src\\transformers\\models\\roc_bert",
        "target": "src\\transformers\\models\\roc_bert\\modeling_roc_bert.py"
      },
      {
        "source": "src\\transformers\\models\\roc_bert",
        "target": "src\\transformers\\models\\roc_bert\\tokenization_roc_bert.py"
      },
      {
        "source": "src\\transformers\\models\\roc_bert",
        "target": "src\\transformers\\models\\roc_bert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\roformer",
        "target": "src\\transformers\\models\\roformer\\configuration_roformer.py"
      },
      {
        "source": "src\\transformers\\models\\roformer",
        "target": "src\\transformers\\models\\roformer\\convert_roformer_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\roformer",
        "target": "src\\transformers\\models\\roformer\\modeling_roformer.py"
      },
      {
        "source": "src\\transformers\\models\\roformer",
        "target": "src\\transformers\\models\\roformer\\tokenization_roformer.py"
      },
      {
        "source": "src\\transformers\\models\\roformer",
        "target": "src\\transformers\\models\\roformer\\tokenization_utils.py"
      },
      {
        "source": "src\\transformers\\models\\roformer",
        "target": "src\\transformers\\models\\roformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr",
        "target": "src\\transformers\\models\\rt_detr\\configuration_rt_detr.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr",
        "target": "src\\transformers\\models\\rt_detr\\configuration_rt_detr_resnet.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr",
        "target": "src\\transformers\\models\\rt_detr\\convert_rt_detr_original_pytorch_checkpoint_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr",
        "target": "src\\transformers\\models\\rt_detr\\image_processing_rt_detr.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr",
        "target": "src\\transformers\\models\\rt_detr\\image_processing_rt_detr_fast.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr",
        "target": "src\\transformers\\models\\rt_detr\\modeling_rt_detr.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr",
        "target": "src\\transformers\\models\\rt_detr\\modeling_rt_detr_resnet.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr",
        "target": "src\\transformers\\models\\rt_detr\\modular_rt_detr.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr",
        "target": "src\\transformers\\models\\rt_detr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr_v2",
        "target": "src\\transformers\\models\\rt_detr_v2\\configuration_rt_detr_v2.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr_v2",
        "target": "src\\transformers\\models\\rt_detr_v2\\convert_rt_detr_v2_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr_v2",
        "target": "src\\transformers\\models\\rt_detr_v2\\modeling_rt_detr_v2.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr_v2",
        "target": "src\\transformers\\models\\rt_detr_v2\\modular_rt_detr_v2.py"
      },
      {
        "source": "src\\transformers\\models\\rt_detr_v2",
        "target": "src\\transformers\\models\\rt_detr_v2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\rwkv",
        "target": "src\\transformers\\models\\rwkv\\configuration_rwkv.py"
      },
      {
        "source": "src\\transformers\\models\\rwkv",
        "target": "src\\transformers\\models\\rwkv\\convert_rwkv_checkpoint_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\rwkv",
        "target": "src\\transformers\\models\\rwkv\\modeling_rwkv.py"
      },
      {
        "source": "src\\transformers\\models\\rwkv",
        "target": "src\\transformers\\models\\rwkv\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\sam",
        "target": "src\\transformers\\models\\sam\\configuration_sam.py"
      },
      {
        "source": "src\\transformers\\models\\sam",
        "target": "src\\transformers\\models\\sam\\convert_sam_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\sam",
        "target": "src\\transformers\\models\\sam\\image_processing_sam.py"
      },
      {
        "source": "src\\transformers\\models\\sam",
        "target": "src\\transformers\\models\\sam\\image_processing_sam_fast.py"
      },
      {
        "source": "src\\transformers\\models\\sam",
        "target": "src\\transformers\\models\\sam\\modeling_sam.py"
      },
      {
        "source": "src\\transformers\\models\\sam",
        "target": "src\\transformers\\models\\sam\\processing_sam.py"
      },
      {
        "source": "src\\transformers\\models\\sam",
        "target": "src\\transformers\\models\\sam\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\sam2",
        "target": "src\\transformers\\models\\sam2\\configuration_sam2.py"
      },
      {
        "source": "src\\transformers\\models\\sam2",
        "target": "src\\transformers\\models\\sam2\\convert_sam2_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\sam2",
        "target": "src\\transformers\\models\\sam2\\image_processing_sam2_fast.py"
      },
      {
        "source": "src\\transformers\\models\\sam2",
        "target": "src\\transformers\\models\\sam2\\modeling_sam2.py"
      },
      {
        "source": "src\\transformers\\models\\sam2",
        "target": "src\\transformers\\models\\sam2\\modular_sam2.py"
      },
      {
        "source": "src\\transformers\\models\\sam2",
        "target": "src\\transformers\\models\\sam2\\processing_sam2.py"
      },
      {
        "source": "src\\transformers\\models\\sam2",
        "target": "src\\transformers\\models\\sam2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\sam2_video",
        "target": "src\\transformers\\models\\sam2_video\\configuration_sam2_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam2_video",
        "target": "src\\transformers\\models\\sam2_video\\convert_sam2_video_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\sam2_video",
        "target": "src\\transformers\\models\\sam2_video\\modeling_sam2_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam2_video",
        "target": "src\\transformers\\models\\sam2_video\\modular_sam2_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam2_video",
        "target": "src\\transformers\\models\\sam2_video\\processing_sam2_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam2_video",
        "target": "src\\transformers\\models\\sam2_video\\video_processing_sam2_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam2_video",
        "target": "src\\transformers\\models\\sam2_video\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\sam3",
        "target": "src\\transformers\\models\\sam3\\configuration_sam3.py"
      },
      {
        "source": "src\\transformers\\models\\sam3",
        "target": "src\\transformers\\models\\sam3\\convert_sam3_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\sam3",
        "target": "src\\transformers\\models\\sam3\\image_processing_sam3_fast.py"
      },
      {
        "source": "src\\transformers\\models\\sam3",
        "target": "src\\transformers\\models\\sam3\\modeling_sam3.py"
      },
      {
        "source": "src\\transformers\\models\\sam3",
        "target": "src\\transformers\\models\\sam3\\modular_sam3.py"
      },
      {
        "source": "src\\transformers\\models\\sam3",
        "target": "src\\transformers\\models\\sam3\\processing_sam3.py"
      },
      {
        "source": "src\\transformers\\models\\sam3",
        "target": "src\\transformers\\models\\sam3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_tracker",
        "target": "src\\transformers\\models\\sam3_tracker\\configuration_sam3_tracker.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_tracker",
        "target": "src\\transformers\\models\\sam3_tracker\\modeling_sam3_tracker.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_tracker",
        "target": "src\\transformers\\models\\sam3_tracker\\modular_sam3_tracker.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_tracker",
        "target": "src\\transformers\\models\\sam3_tracker\\processing_sam3_tracker.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_tracker",
        "target": "src\\transformers\\models\\sam3_tracker\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_tracker_video",
        "target": "src\\transformers\\models\\sam3_tracker_video\\configuration_sam3_tracker_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_tracker_video",
        "target": "src\\transformers\\models\\sam3_tracker_video\\modeling_sam3_tracker_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_tracker_video",
        "target": "src\\transformers\\models\\sam3_tracker_video\\modular_sam3_tracker_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_tracker_video",
        "target": "src\\transformers\\models\\sam3_tracker_video\\processing_sam3_tracker_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_tracker_video",
        "target": "src\\transformers\\models\\sam3_tracker_video\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_video",
        "target": "src\\transformers\\models\\sam3_video\\configuration_sam3_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_video",
        "target": "src\\transformers\\models\\sam3_video\\convert_sam3_video_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_video",
        "target": "src\\transformers\\models\\sam3_video\\modeling_sam3_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_video",
        "target": "src\\transformers\\models\\sam3_video\\processing_sam3_video.py"
      },
      {
        "source": "src\\transformers\\models\\sam3_video",
        "target": "src\\transformers\\models\\sam3_video\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\sam_hq",
        "target": "src\\transformers\\models\\sam_hq\\configuration_sam_hq.py"
      },
      {
        "source": "src\\transformers\\models\\sam_hq",
        "target": "src\\transformers\\models\\sam_hq\\convert_samhq_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\sam_hq",
        "target": "src\\transformers\\models\\sam_hq\\modeling_sam_hq.py"
      },
      {
        "source": "src\\transformers\\models\\sam_hq",
        "target": "src\\transformers\\models\\sam_hq\\modular_sam_hq.py"
      },
      {
        "source": "src\\transformers\\models\\sam_hq",
        "target": "src\\transformers\\models\\sam_hq\\processing_sam_hq.py"
      },
      {
        "source": "src\\transformers\\models\\sam_hq",
        "target": "src\\transformers\\models\\sam_hq\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t",
        "target": "src\\transformers\\models\\seamless_m4t\\configuration_seamless_m4t.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t",
        "target": "src\\transformers\\models\\seamless_m4t\\convert_fairseq2_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t",
        "target": "src\\transformers\\models\\seamless_m4t\\feature_extraction_seamless_m4t.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t",
        "target": "src\\transformers\\models\\seamless_m4t\\modeling_seamless_m4t.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t",
        "target": "src\\transformers\\models\\seamless_m4t\\processing_seamless_m4t.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t",
        "target": "src\\transformers\\models\\seamless_m4t\\tokenization_seamless_m4t.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t",
        "target": "src\\transformers\\models\\seamless_m4t\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t_v2",
        "target": "src\\transformers\\models\\seamless_m4t_v2\\configuration_seamless_m4t_v2.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t_v2",
        "target": "src\\transformers\\models\\seamless_m4t_v2\\convert_fairseq2_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t_v2",
        "target": "src\\transformers\\models\\seamless_m4t_v2\\modeling_seamless_m4t_v2.py"
      },
      {
        "source": "src\\transformers\\models\\seamless_m4t_v2",
        "target": "src\\transformers\\models\\seamless_m4t_v2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\seed_oss",
        "target": "src\\transformers\\models\\seed_oss\\configuration_seed_oss.py"
      },
      {
        "source": "src\\transformers\\models\\seed_oss",
        "target": "src\\transformers\\models\\seed_oss\\modeling_seed_oss.py"
      },
      {
        "source": "src\\transformers\\models\\seed_oss",
        "target": "src\\transformers\\models\\seed_oss\\modular_seed_oss.py"
      },
      {
        "source": "src\\transformers\\models\\seed_oss",
        "target": "src\\transformers\\models\\seed_oss\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\segformer",
        "target": "src\\transformers\\models\\segformer\\configuration_segformer.py"
      },
      {
        "source": "src\\transformers\\models\\segformer",
        "target": "src\\transformers\\models\\segformer\\convert_segformer_original_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\segformer",
        "target": "src\\transformers\\models\\segformer\\image_processing_segformer.py"
      },
      {
        "source": "src\\transformers\\models\\segformer",
        "target": "src\\transformers\\models\\segformer\\image_processing_segformer_fast.py"
      },
      {
        "source": "src\\transformers\\models\\segformer",
        "target": "src\\transformers\\models\\segformer\\modeling_segformer.py"
      },
      {
        "source": "src\\transformers\\models\\segformer",
        "target": "src\\transformers\\models\\segformer\\modular_segformer.py"
      },
      {
        "source": "src\\transformers\\models\\segformer",
        "target": "src\\transformers\\models\\segformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\seggpt",
        "target": "src\\transformers\\models\\seggpt\\configuration_seggpt.py"
      },
      {
        "source": "src\\transformers\\models\\seggpt",
        "target": "src\\transformers\\models\\seggpt\\convert_seggpt_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\seggpt",
        "target": "src\\transformers\\models\\seggpt\\image_processing_seggpt.py"
      },
      {
        "source": "src\\transformers\\models\\seggpt",
        "target": "src\\transformers\\models\\seggpt\\modeling_seggpt.py"
      },
      {
        "source": "src\\transformers\\models\\seggpt",
        "target": "src\\transformers\\models\\seggpt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\sew",
        "target": "src\\transformers\\models\\sew\\configuration_sew.py"
      },
      {
        "source": "src\\transformers\\models\\sew",
        "target": "src\\transformers\\models\\sew\\convert_sew_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\sew",
        "target": "src\\transformers\\models\\sew\\modeling_sew.py"
      },
      {
        "source": "src\\transformers\\models\\sew",
        "target": "src\\transformers\\models\\sew\\modular_sew.py"
      },
      {
        "source": "src\\transformers\\models\\sew",
        "target": "src\\transformers\\models\\sew\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\sew_d",
        "target": "src\\transformers\\models\\sew_d\\configuration_sew_d.py"
      },
      {
        "source": "src\\transformers\\models\\sew_d",
        "target": "src\\transformers\\models\\sew_d\\convert_sew_d_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\sew_d",
        "target": "src\\transformers\\models\\sew_d\\modeling_sew_d.py"
      },
      {
        "source": "src\\transformers\\models\\sew_d",
        "target": "src\\transformers\\models\\sew_d\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\shieldgemma2",
        "target": "src\\transformers\\models\\shieldgemma2\\configuration_shieldgemma2.py"
      },
      {
        "source": "src\\transformers\\models\\shieldgemma2",
        "target": "src\\transformers\\models\\shieldgemma2\\convert_shieldgemma2_weights_orbax_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\shieldgemma2",
        "target": "src\\transformers\\models\\shieldgemma2\\modeling_shieldgemma2.py"
      },
      {
        "source": "src\\transformers\\models\\shieldgemma2",
        "target": "src\\transformers\\models\\shieldgemma2\\processing_shieldgemma2.py"
      },
      {
        "source": "src\\transformers\\models\\shieldgemma2",
        "target": "src\\transformers\\models\\shieldgemma2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\siglip",
        "target": "src\\transformers\\models\\siglip\\configuration_siglip.py"
      },
      {
        "source": "src\\transformers\\models\\siglip",
        "target": "src\\transformers\\models\\siglip\\convert_siglip_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\siglip",
        "target": "src\\transformers\\models\\siglip\\image_processing_siglip.py"
      },
      {
        "source": "src\\transformers\\models\\siglip",
        "target": "src\\transformers\\models\\siglip\\image_processing_siglip_fast.py"
      },
      {
        "source": "src\\transformers\\models\\siglip",
        "target": "src\\transformers\\models\\siglip\\modeling_siglip.py"
      },
      {
        "source": "src\\transformers\\models\\siglip",
        "target": "src\\transformers\\models\\siglip\\processing_siglip.py"
      },
      {
        "source": "src\\transformers\\models\\siglip",
        "target": "src\\transformers\\models\\siglip\\tokenization_siglip.py"
      },
      {
        "source": "src\\transformers\\models\\siglip",
        "target": "src\\transformers\\models\\siglip\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\siglip2",
        "target": "src\\transformers\\models\\siglip2\\configuration_siglip2.py"
      },
      {
        "source": "src\\transformers\\models\\siglip2",
        "target": "src\\transformers\\models\\siglip2\\convert_siglip2_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\siglip2",
        "target": "src\\transformers\\models\\siglip2\\image_processing_siglip2.py"
      },
      {
        "source": "src\\transformers\\models\\siglip2",
        "target": "src\\transformers\\models\\siglip2\\image_processing_siglip2_fast.py"
      },
      {
        "source": "src\\transformers\\models\\siglip2",
        "target": "src\\transformers\\models\\siglip2\\modeling_siglip2.py"
      },
      {
        "source": "src\\transformers\\models\\siglip2",
        "target": "src\\transformers\\models\\siglip2\\modular_siglip2.py"
      },
      {
        "source": "src\\transformers\\models\\siglip2",
        "target": "src\\transformers\\models\\siglip2\\processing_siglip2.py"
      },
      {
        "source": "src\\transformers\\models\\siglip2",
        "target": "src\\transformers\\models\\siglip2\\tokenization_siglip2.py"
      },
      {
        "source": "src\\transformers\\models\\siglip2",
        "target": "src\\transformers\\models\\siglip2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\smollm3",
        "target": "src\\transformers\\models\\smollm3\\configuration_smollm3.py"
      },
      {
        "source": "src\\transformers\\models\\smollm3",
        "target": "src\\transformers\\models\\smollm3\\modeling_smollm3.py"
      },
      {
        "source": "src\\transformers\\models\\smollm3",
        "target": "src\\transformers\\models\\smollm3\\modular_smollm3.py"
      },
      {
        "source": "src\\transformers\\models\\smollm3",
        "target": "src\\transformers\\models\\smollm3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\smolvlm",
        "target": "src\\transformers\\models\\smolvlm\\configuration_smolvlm.py"
      },
      {
        "source": "src\\transformers\\models\\smolvlm",
        "target": "src\\transformers\\models\\smolvlm\\image_processing_smolvlm.py"
      },
      {
        "source": "src\\transformers\\models\\smolvlm",
        "target": "src\\transformers\\models\\smolvlm\\image_processing_smolvlm_fast.py"
      },
      {
        "source": "src\\transformers\\models\\smolvlm",
        "target": "src\\transformers\\models\\smolvlm\\modeling_smolvlm.py"
      },
      {
        "source": "src\\transformers\\models\\smolvlm",
        "target": "src\\transformers\\models\\smolvlm\\modular_smolvlm.py"
      },
      {
        "source": "src\\transformers\\models\\smolvlm",
        "target": "src\\transformers\\models\\smolvlm\\processing_smolvlm.py"
      },
      {
        "source": "src\\transformers\\models\\smolvlm",
        "target": "src\\transformers\\models\\smolvlm\\video_processing_smolvlm.py"
      },
      {
        "source": "src\\transformers\\models\\smolvlm",
        "target": "src\\transformers\\models\\smolvlm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\solar_open",
        "target": "src\\transformers\\models\\solar_open\\configuration_solar_open.py"
      },
      {
        "source": "src\\transformers\\models\\solar_open",
        "target": "src\\transformers\\models\\solar_open\\modeling_solar_open.py"
      },
      {
        "source": "src\\transformers\\models\\solar_open",
        "target": "src\\transformers\\models\\solar_open\\modular_solar_open.py"
      },
      {
        "source": "src\\transformers\\models\\solar_open",
        "target": "src\\transformers\\models\\solar_open\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\speecht5",
        "target": "src\\transformers\\models\\speecht5\\configuration_speecht5.py"
      },
      {
        "source": "src\\transformers\\models\\speecht5",
        "target": "src\\transformers\\models\\speecht5\\convert_hifigan.py"
      },
      {
        "source": "src\\transformers\\models\\speecht5",
        "target": "src\\transformers\\models\\speecht5\\convert_speecht5_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\speecht5",
        "target": "src\\transformers\\models\\speecht5\\feature_extraction_speecht5.py"
      },
      {
        "source": "src\\transformers\\models\\speecht5",
        "target": "src\\transformers\\models\\speecht5\\modeling_speecht5.py"
      },
      {
        "source": "src\\transformers\\models\\speecht5",
        "target": "src\\transformers\\models\\speecht5\\number_normalizer.py"
      },
      {
        "source": "src\\transformers\\models\\speecht5",
        "target": "src\\transformers\\models\\speecht5\\processing_speecht5.py"
      },
      {
        "source": "src\\transformers\\models\\speecht5",
        "target": "src\\transformers\\models\\speecht5\\tokenization_speecht5.py"
      },
      {
        "source": "src\\transformers\\models\\speecht5",
        "target": "src\\transformers\\models\\speecht5\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\speech_encoder_decoder",
        "target": "src\\transformers\\models\\speech_encoder_decoder\\configuration_speech_encoder_decoder.py"
      },
      {
        "source": "src\\transformers\\models\\speech_encoder_decoder",
        "target": "src\\transformers\\models\\speech_encoder_decoder\\convert_mbart_wav2vec2_seq2seq_original_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\speech_encoder_decoder",
        "target": "src\\transformers\\models\\speech_encoder_decoder\\convert_speech_to_text_wav2vec2_seq2seq_original_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\speech_encoder_decoder",
        "target": "src\\transformers\\models\\speech_encoder_decoder\\modeling_speech_encoder_decoder.py"
      },
      {
        "source": "src\\transformers\\models\\speech_encoder_decoder",
        "target": "src\\transformers\\models\\speech_encoder_decoder\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\speech_to_text",
        "target": "src\\transformers\\models\\speech_to_text\\configuration_speech_to_text.py"
      },
      {
        "source": "src\\transformers\\models\\speech_to_text",
        "target": "src\\transformers\\models\\speech_to_text\\convert_s2t_fairseq_to_tfms.py"
      },
      {
        "source": "src\\transformers\\models\\speech_to_text",
        "target": "src\\transformers\\models\\speech_to_text\\feature_extraction_speech_to_text.py"
      },
      {
        "source": "src\\transformers\\models\\speech_to_text",
        "target": "src\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py"
      },
      {
        "source": "src\\transformers\\models\\speech_to_text",
        "target": "src\\transformers\\models\\speech_to_text\\processing_speech_to_text.py"
      },
      {
        "source": "src\\transformers\\models\\speech_to_text",
        "target": "src\\transformers\\models\\speech_to_text\\tokenization_speech_to_text.py"
      },
      {
        "source": "src\\transformers\\models\\speech_to_text",
        "target": "src\\transformers\\models\\speech_to_text\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\splinter",
        "target": "src\\transformers\\models\\splinter\\configuration_splinter.py"
      },
      {
        "source": "src\\transformers\\models\\splinter",
        "target": "src\\transformers\\models\\splinter\\modeling_splinter.py"
      },
      {
        "source": "src\\transformers\\models\\splinter",
        "target": "src\\transformers\\models\\splinter\\tokenization_splinter.py"
      },
      {
        "source": "src\\transformers\\models\\splinter",
        "target": "src\\transformers\\models\\splinter\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\squeezebert",
        "target": "src\\transformers\\models\\squeezebert\\configuration_squeezebert.py"
      },
      {
        "source": "src\\transformers\\models\\squeezebert",
        "target": "src\\transformers\\models\\squeezebert\\modeling_squeezebert.py"
      },
      {
        "source": "src\\transformers\\models\\squeezebert",
        "target": "src\\transformers\\models\\squeezebert\\tokenization_squeezebert.py"
      },
      {
        "source": "src\\transformers\\models\\squeezebert",
        "target": "src\\transformers\\models\\squeezebert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\stablelm",
        "target": "src\\transformers\\models\\stablelm\\configuration_stablelm.py"
      },
      {
        "source": "src\\transformers\\models\\stablelm",
        "target": "src\\transformers\\models\\stablelm\\modeling_stablelm.py"
      },
      {
        "source": "src\\transformers\\models\\stablelm",
        "target": "src\\transformers\\models\\stablelm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\starcoder2",
        "target": "src\\transformers\\models\\starcoder2\\configuration_starcoder2.py"
      },
      {
        "source": "src\\transformers\\models\\starcoder2",
        "target": "src\\transformers\\models\\starcoder2\\modeling_starcoder2.py"
      },
      {
        "source": "src\\transformers\\models\\starcoder2",
        "target": "src\\transformers\\models\\starcoder2\\modular_starcoder2.py"
      },
      {
        "source": "src\\transformers\\models\\starcoder2",
        "target": "src\\transformers\\models\\starcoder2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\superglue",
        "target": "src\\transformers\\models\\superglue\\configuration_superglue.py"
      },
      {
        "source": "src\\transformers\\models\\superglue",
        "target": "src\\transformers\\models\\superglue\\convert_superglue_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\superglue",
        "target": "src\\transformers\\models\\superglue\\image_processing_superglue.py"
      },
      {
        "source": "src\\transformers\\models\\superglue",
        "target": "src\\transformers\\models\\superglue\\image_processing_superglue_fast.py"
      },
      {
        "source": "src\\transformers\\models\\superglue",
        "target": "src\\transformers\\models\\superglue\\modeling_superglue.py"
      },
      {
        "source": "src\\transformers\\models\\superglue",
        "target": "src\\transformers\\models\\superglue\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\superpoint",
        "target": "src\\transformers\\models\\superpoint\\configuration_superpoint.py"
      },
      {
        "source": "src\\transformers\\models\\superpoint",
        "target": "src\\transformers\\models\\superpoint\\convert_superpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\superpoint",
        "target": "src\\transformers\\models\\superpoint\\image_processing_superpoint.py"
      },
      {
        "source": "src\\transformers\\models\\superpoint",
        "target": "src\\transformers\\models\\superpoint\\image_processing_superpoint_fast.py"
      },
      {
        "source": "src\\transformers\\models\\superpoint",
        "target": "src\\transformers\\models\\superpoint\\modeling_superpoint.py"
      },
      {
        "source": "src\\transformers\\models\\superpoint",
        "target": "src\\transformers\\models\\superpoint\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\swiftformer",
        "target": "src\\transformers\\models\\swiftformer\\configuration_swiftformer.py"
      },
      {
        "source": "src\\transformers\\models\\swiftformer",
        "target": "src\\transformers\\models\\swiftformer\\convert_swiftformer_original_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\swiftformer",
        "target": "src\\transformers\\models\\swiftformer\\modeling_swiftformer.py"
      },
      {
        "source": "src\\transformers\\models\\swiftformer",
        "target": "src\\transformers\\models\\swiftformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\swin",
        "target": "src\\transformers\\models\\swin\\configuration_swin.py"
      },
      {
        "source": "src\\transformers\\models\\swin",
        "target": "src\\transformers\\models\\swin\\convert_swin_simmim_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\swin",
        "target": "src\\transformers\\models\\swin\\convert_swin_timm_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\swin",
        "target": "src\\transformers\\models\\swin\\modeling_swin.py"
      },
      {
        "source": "src\\transformers\\models\\swin",
        "target": "src\\transformers\\models\\swin\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\swin2sr",
        "target": "src\\transformers\\models\\swin2sr\\configuration_swin2sr.py"
      },
      {
        "source": "src\\transformers\\models\\swin2sr",
        "target": "src\\transformers\\models\\swin2sr\\convert_swin2sr_original_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\swin2sr",
        "target": "src\\transformers\\models\\swin2sr\\image_processing_swin2sr.py"
      },
      {
        "source": "src\\transformers\\models\\swin2sr",
        "target": "src\\transformers\\models\\swin2sr\\image_processing_swin2sr_fast.py"
      },
      {
        "source": "src\\transformers\\models\\swin2sr",
        "target": "src\\transformers\\models\\swin2sr\\modeling_swin2sr.py"
      },
      {
        "source": "src\\transformers\\models\\swin2sr",
        "target": "src\\transformers\\models\\swin2sr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\swinv2",
        "target": "src\\transformers\\models\\swinv2\\configuration_swinv2.py"
      },
      {
        "source": "src\\transformers\\models\\swinv2",
        "target": "src\\transformers\\models\\swinv2\\convert_swinv2_timm_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\swinv2",
        "target": "src\\transformers\\models\\swinv2\\modeling_swinv2.py"
      },
      {
        "source": "src\\transformers\\models\\swinv2",
        "target": "src\\transformers\\models\\swinv2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\switch_transformers",
        "target": "src\\transformers\\models\\switch_transformers\\configuration_switch_transformers.py"
      },
      {
        "source": "src\\transformers\\models\\switch_transformers",
        "target": "src\\transformers\\models\\switch_transformers\\convert_big_switch.py"
      },
      {
        "source": "src\\transformers\\models\\switch_transformers",
        "target": "src\\transformers\\models\\switch_transformers\\convert_switch_transformers_original_flax_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\switch_transformers",
        "target": "src\\transformers\\models\\switch_transformers\\modeling_switch_transformers.py"
      },
      {
        "source": "src\\transformers\\models\\switch_transformers",
        "target": "src\\transformers\\models\\switch_transformers\\modular_switch_transformers.py"
      },
      {
        "source": "src\\transformers\\models\\switch_transformers",
        "target": "src\\transformers\\models\\switch_transformers\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\t5",
        "target": "src\\transformers\\models\\t5\\configuration_t5.py"
      },
      {
        "source": "src\\transformers\\models\\t5",
        "target": "src\\transformers\\models\\t5\\convert_t5x_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\t5",
        "target": "src\\transformers\\models\\t5\\convert_t5_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\t5",
        "target": "src\\transformers\\models\\t5\\download_from_gcp.sh"
      },
      {
        "source": "src\\transformers\\models\\t5",
        "target": "src\\transformers\\models\\t5\\modeling_t5.py"
      },
      {
        "source": "src\\transformers\\models\\t5",
        "target": "src\\transformers\\models\\t5\\tokenization_t5.py"
      },
      {
        "source": "src\\transformers\\models\\t5",
        "target": "src\\transformers\\models\\t5\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\t5gemma",
        "target": "src\\transformers\\models\\t5gemma\\configuration_t5gemma.py"
      },
      {
        "source": "src\\transformers\\models\\t5gemma",
        "target": "src\\transformers\\models\\t5gemma\\modeling_t5gemma.py"
      },
      {
        "source": "src\\transformers\\models\\t5gemma",
        "target": "src\\transformers\\models\\t5gemma\\modular_t5gemma.py"
      },
      {
        "source": "src\\transformers\\models\\t5gemma",
        "target": "src\\transformers\\models\\t5gemma\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\t5gemma2",
        "target": "src\\transformers\\models\\t5gemma2\\configuration_t5gemma2.py"
      },
      {
        "source": "src\\transformers\\models\\t5gemma2",
        "target": "src\\transformers\\models\\t5gemma2\\modeling_t5gemma2.py"
      },
      {
        "source": "src\\transformers\\models\\t5gemma2",
        "target": "src\\transformers\\models\\t5gemma2\\modular_t5gemma2.py"
      },
      {
        "source": "src\\transformers\\models\\t5gemma2",
        "target": "src\\transformers\\models\\t5gemma2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\table_transformer",
        "target": "src\\transformers\\models\\table_transformer\\configuration_table_transformer.py"
      },
      {
        "source": "src\\transformers\\models\\table_transformer",
        "target": "src\\transformers\\models\\table_transformer\\convert_table_transformer_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\table_transformer",
        "target": "src\\transformers\\models\\table_transformer\\convert_table_transformer_to_hf_no_timm.py"
      },
      {
        "source": "src\\transformers\\models\\table_transformer",
        "target": "src\\transformers\\models\\table_transformer\\modeling_table_transformer.py"
      },
      {
        "source": "src\\transformers\\models\\table_transformer",
        "target": "src\\transformers\\models\\table_transformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\tapas",
        "target": "src\\transformers\\models\\tapas\\configuration_tapas.py"
      },
      {
        "source": "src\\transformers\\models\\tapas",
        "target": "src\\transformers\\models\\tapas\\convert_tapas_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\tapas",
        "target": "src\\transformers\\models\\tapas\\modeling_tapas.py"
      },
      {
        "source": "src\\transformers\\models\\tapas",
        "target": "src\\transformers\\models\\tapas\\tokenization_tapas.py"
      },
      {
        "source": "src\\transformers\\models\\tapas",
        "target": "src\\transformers\\models\\tapas\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\textnet",
        "target": "src\\transformers\\models\\textnet\\configuration_textnet.py"
      },
      {
        "source": "src\\transformers\\models\\textnet",
        "target": "src\\transformers\\models\\textnet\\convert_textnet_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\textnet",
        "target": "src\\transformers\\models\\textnet\\image_processing_textnet.py"
      },
      {
        "source": "src\\transformers\\models\\textnet",
        "target": "src\\transformers\\models\\textnet\\image_processing_textnet_fast.py"
      },
      {
        "source": "src\\transformers\\models\\textnet",
        "target": "src\\transformers\\models\\textnet\\modeling_textnet.py"
      },
      {
        "source": "src\\transformers\\models\\textnet",
        "target": "src\\transformers\\models\\textnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\timesfm",
        "target": "src\\transformers\\models\\timesfm\\configuration_timesfm.py"
      },
      {
        "source": "src\\transformers\\models\\timesfm",
        "target": "src\\transformers\\models\\timesfm\\convert_timesfm_orignal_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\timesfm",
        "target": "src\\transformers\\models\\timesfm\\modeling_timesfm.py"
      },
      {
        "source": "src\\transformers\\models\\timesfm",
        "target": "src\\transformers\\models\\timesfm\\modular_timesfm.py"
      },
      {
        "source": "src\\transformers\\models\\timesfm",
        "target": "src\\transformers\\models\\timesfm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\timesformer",
        "target": "src\\transformers\\models\\timesformer\\configuration_timesformer.py"
      },
      {
        "source": "src\\transformers\\models\\timesformer",
        "target": "src\\transformers\\models\\timesformer\\convert_timesformer_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\timesformer",
        "target": "src\\transformers\\models\\timesformer\\modeling_timesformer.py"
      },
      {
        "source": "src\\transformers\\models\\timesformer",
        "target": "src\\transformers\\models\\timesformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\time_series_transformer",
        "target": "src\\transformers\\models\\time_series_transformer\\configuration_time_series_transformer.py"
      },
      {
        "source": "src\\transformers\\models\\time_series_transformer",
        "target": "src\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py"
      },
      {
        "source": "src\\transformers\\models\\time_series_transformer",
        "target": "src\\transformers\\models\\time_series_transformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\timm_backbone",
        "target": "src\\transformers\\models\\timm_backbone\\configuration_timm_backbone.py"
      },
      {
        "source": "src\\transformers\\models\\timm_backbone",
        "target": "src\\transformers\\models\\timm_backbone\\modeling_timm_backbone.py"
      },
      {
        "source": "src\\transformers\\models\\timm_backbone",
        "target": "src\\transformers\\models\\timm_backbone\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\timm_wrapper",
        "target": "src\\transformers\\models\\timm_wrapper\\configuration_timm_wrapper.py"
      },
      {
        "source": "src\\transformers\\models\\timm_wrapper",
        "target": "src\\transformers\\models\\timm_wrapper\\image_processing_timm_wrapper.py"
      },
      {
        "source": "src\\transformers\\models\\timm_wrapper",
        "target": "src\\transformers\\models\\timm_wrapper\\modeling_timm_wrapper.py"
      },
      {
        "source": "src\\transformers\\models\\timm_wrapper",
        "target": "src\\transformers\\models\\timm_wrapper\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\trocr",
        "target": "src\\transformers\\models\\trocr\\configuration_trocr.py"
      },
      {
        "source": "src\\transformers\\models\\trocr",
        "target": "src\\transformers\\models\\trocr\\convert_trocr_unilm_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\trocr",
        "target": "src\\transformers\\models\\trocr\\modeling_trocr.py"
      },
      {
        "source": "src\\transformers\\models\\trocr",
        "target": "src\\transformers\\models\\trocr\\processing_trocr.py"
      },
      {
        "source": "src\\transformers\\models\\trocr",
        "target": "src\\transformers\\models\\trocr\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\tvp",
        "target": "src\\transformers\\models\\tvp\\configuration_tvp.py"
      },
      {
        "source": "src\\transformers\\models\\tvp",
        "target": "src\\transformers\\models\\tvp\\image_processing_tvp.py"
      },
      {
        "source": "src\\transformers\\models\\tvp",
        "target": "src\\transformers\\models\\tvp\\image_processing_tvp_fast.py"
      },
      {
        "source": "src\\transformers\\models\\tvp",
        "target": "src\\transformers\\models\\tvp\\modeling_tvp.py"
      },
      {
        "source": "src\\transformers\\models\\tvp",
        "target": "src\\transformers\\models\\tvp\\processing_tvp.py"
      },
      {
        "source": "src\\transformers\\models\\tvp",
        "target": "src\\transformers\\models\\tvp\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\udop",
        "target": "src\\transformers\\models\\udop\\configuration_udop.py"
      },
      {
        "source": "src\\transformers\\models\\udop",
        "target": "src\\transformers\\models\\udop\\convert_udop_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\udop",
        "target": "src\\transformers\\models\\udop\\modeling_udop.py"
      },
      {
        "source": "src\\transformers\\models\\udop",
        "target": "src\\transformers\\models\\udop\\processing_udop.py"
      },
      {
        "source": "src\\transformers\\models\\udop",
        "target": "src\\transformers\\models\\udop\\tokenization_udop.py"
      },
      {
        "source": "src\\transformers\\models\\udop",
        "target": "src\\transformers\\models\\udop\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\umt5",
        "target": "src\\transformers\\models\\umt5\\configuration_umt5.py"
      },
      {
        "source": "src\\transformers\\models\\umt5",
        "target": "src\\transformers\\models\\umt5\\convert_umt5_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\umt5",
        "target": "src\\transformers\\models\\umt5\\modeling_umt5.py"
      },
      {
        "source": "src\\transformers\\models\\umt5",
        "target": "src\\transformers\\models\\umt5\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech",
        "target": "src\\transformers\\models\\unispeech\\configuration_unispeech.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech",
        "target": "src\\transformers\\models\\unispeech\\convert_unispeech_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech",
        "target": "src\\transformers\\models\\unispeech\\modeling_unispeech.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech",
        "target": "src\\transformers\\models\\unispeech\\modular_unispeech.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech",
        "target": "src\\transformers\\models\\unispeech\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech_sat",
        "target": "src\\transformers\\models\\unispeech_sat\\configuration_unispeech_sat.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech_sat",
        "target": "src\\transformers\\models\\unispeech_sat\\convert_unispeech_original_s3prl_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech_sat",
        "target": "src\\transformers\\models\\unispeech_sat\\convert_unispeech_sat_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech_sat",
        "target": "src\\transformers\\models\\unispeech_sat\\modeling_unispeech_sat.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech_sat",
        "target": "src\\transformers\\models\\unispeech_sat\\modular_unispeech_sat.py"
      },
      {
        "source": "src\\transformers\\models\\unispeech_sat",
        "target": "src\\transformers\\models\\unispeech_sat\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\univnet",
        "target": "src\\transformers\\models\\univnet\\configuration_univnet.py"
      },
      {
        "source": "src\\transformers\\models\\univnet",
        "target": "src\\transformers\\models\\univnet\\convert_univnet.py"
      },
      {
        "source": "src\\transformers\\models\\univnet",
        "target": "src\\transformers\\models\\univnet\\feature_extraction_univnet.py"
      },
      {
        "source": "src\\transformers\\models\\univnet",
        "target": "src\\transformers\\models\\univnet\\modeling_univnet.py"
      },
      {
        "source": "src\\transformers\\models\\univnet",
        "target": "src\\transformers\\models\\univnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\upernet",
        "target": "src\\transformers\\models\\upernet\\configuration_upernet.py"
      },
      {
        "source": "src\\transformers\\models\\upernet",
        "target": "src\\transformers\\models\\upernet\\convert_convnext_upernet_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\upernet",
        "target": "src\\transformers\\models\\upernet\\convert_swin_upernet_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\upernet",
        "target": "src\\transformers\\models\\upernet\\modeling_upernet.py"
      },
      {
        "source": "src\\transformers\\models\\upernet",
        "target": "src\\transformers\\models\\upernet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vaultgemma",
        "target": "src\\transformers\\models\\vaultgemma\\configuration_vaultgemma.py"
      },
      {
        "source": "src\\transformers\\models\\vaultgemma",
        "target": "src\\transformers\\models\\vaultgemma\\modeling_vaultgemma.py"
      },
      {
        "source": "src\\transformers\\models\\vaultgemma",
        "target": "src\\transformers\\models\\vaultgemma\\modular_vaultgemma.py"
      },
      {
        "source": "src\\transformers\\models\\vaultgemma",
        "target": "src\\transformers\\models\\vaultgemma\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\videomae",
        "target": "src\\transformers\\models\\videomae\\configuration_videomae.py"
      },
      {
        "source": "src\\transformers\\models\\videomae",
        "target": "src\\transformers\\models\\videomae\\convert_videomae_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\videomae",
        "target": "src\\transformers\\models\\videomae\\image_processing_videomae.py"
      },
      {
        "source": "src\\transformers\\models\\videomae",
        "target": "src\\transformers\\models\\videomae\\modeling_videomae.py"
      },
      {
        "source": "src\\transformers\\models\\videomae",
        "target": "src\\transformers\\models\\videomae\\video_processing_videomae.py"
      },
      {
        "source": "src\\transformers\\models\\videomae",
        "target": "src\\transformers\\models\\videomae\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\video_llama_3",
        "target": "src\\transformers\\models\\video_llama_3\\configuration_video_llama_3.py"
      },
      {
        "source": "src\\transformers\\models\\video_llama_3",
        "target": "src\\transformers\\models\\video_llama_3\\image_processing_video_llama_3.py"
      },
      {
        "source": "src\\transformers\\models\\video_llama_3",
        "target": "src\\transformers\\models\\video_llama_3\\image_processing_video_llama_3_fast.py"
      },
      {
        "source": "src\\transformers\\models\\video_llama_3",
        "target": "src\\transformers\\models\\video_llama_3\\modeling_video_llama_3.py"
      },
      {
        "source": "src\\transformers\\models\\video_llama_3",
        "target": "src\\transformers\\models\\video_llama_3\\modular_video_llama_3.py"
      },
      {
        "source": "src\\transformers\\models\\video_llama_3",
        "target": "src\\transformers\\models\\video_llama_3\\processing_video_llama_3.py"
      },
      {
        "source": "src\\transformers\\models\\video_llama_3",
        "target": "src\\transformers\\models\\video_llama_3\\video_processing_video_llama_3.py"
      },
      {
        "source": "src\\transformers\\models\\video_llama_3",
        "target": "src\\transformers\\models\\video_llama_3\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\video_llava",
        "target": "src\\transformers\\models\\video_llava\\configuration_video_llava.py"
      },
      {
        "source": "src\\transformers\\models\\video_llava",
        "target": "src\\transformers\\models\\video_llava\\convert_video_llava_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\video_llava",
        "target": "src\\transformers\\models\\video_llava\\image_processing_video_llava.py"
      },
      {
        "source": "src\\transformers\\models\\video_llava",
        "target": "src\\transformers\\models\\video_llava\\modeling_video_llava.py"
      },
      {
        "source": "src\\transformers\\models\\video_llava",
        "target": "src\\transformers\\models\\video_llava\\processing_video_llava.py"
      },
      {
        "source": "src\\transformers\\models\\video_llava",
        "target": "src\\transformers\\models\\video_llava\\video_processing_video_llava.py"
      },
      {
        "source": "src\\transformers\\models\\video_llava",
        "target": "src\\transformers\\models\\video_llava\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vilt",
        "target": "src\\transformers\\models\\vilt\\configuration_vilt.py"
      },
      {
        "source": "src\\transformers\\models\\vilt",
        "target": "src\\transformers\\models\\vilt\\convert_vilt_original_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\vilt",
        "target": "src\\transformers\\models\\vilt\\image_processing_vilt.py"
      },
      {
        "source": "src\\transformers\\models\\vilt",
        "target": "src\\transformers\\models\\vilt\\image_processing_vilt_fast.py"
      },
      {
        "source": "src\\transformers\\models\\vilt",
        "target": "src\\transformers\\models\\vilt\\modeling_vilt.py"
      },
      {
        "source": "src\\transformers\\models\\vilt",
        "target": "src\\transformers\\models\\vilt\\processing_vilt.py"
      },
      {
        "source": "src\\transformers\\models\\vilt",
        "target": "src\\transformers\\models\\vilt\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vipllava",
        "target": "src\\transformers\\models\\vipllava\\configuration_vipllava.py"
      },
      {
        "source": "src\\transformers\\models\\vipllava",
        "target": "src\\transformers\\models\\vipllava\\convert_vipllava_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\vipllava",
        "target": "src\\transformers\\models\\vipllava\\modeling_vipllava.py"
      },
      {
        "source": "src\\transformers\\models\\vipllava",
        "target": "src\\transformers\\models\\vipllava\\modular_vipllava.py"
      },
      {
        "source": "src\\transformers\\models\\vipllava",
        "target": "src\\transformers\\models\\vipllava\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vision_encoder_decoder",
        "target": "src\\transformers\\models\\vision_encoder_decoder\\configuration_vision_encoder_decoder.py"
      },
      {
        "source": "src\\transformers\\models\\vision_encoder_decoder",
        "target": "src\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py"
      },
      {
        "source": "src\\transformers\\models\\vision_encoder_decoder",
        "target": "src\\transformers\\models\\vision_encoder_decoder\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vision_text_dual_encoder",
        "target": "src\\transformers\\models\\vision_text_dual_encoder\\configuration_vision_text_dual_encoder.py"
      },
      {
        "source": "src\\transformers\\models\\vision_text_dual_encoder",
        "target": "src\\transformers\\models\\vision_text_dual_encoder\\modeling_vision_text_dual_encoder.py"
      },
      {
        "source": "src\\transformers\\models\\vision_text_dual_encoder",
        "target": "src\\transformers\\models\\vision_text_dual_encoder\\processing_vision_text_dual_encoder.py"
      },
      {
        "source": "src\\transformers\\models\\vision_text_dual_encoder",
        "target": "src\\transformers\\models\\vision_text_dual_encoder\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\visual_bert",
        "target": "src\\transformers\\models\\visual_bert\\configuration_visual_bert.py"
      },
      {
        "source": "src\\transformers\\models\\visual_bert",
        "target": "src\\transformers\\models\\visual_bert\\convert_visual_bert_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\visual_bert",
        "target": "src\\transformers\\models\\visual_bert\\modeling_visual_bert.py"
      },
      {
        "source": "src\\transformers\\models\\visual_bert",
        "target": "src\\transformers\\models\\visual_bert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vit",
        "target": "src\\transformers\\models\\vit\\configuration_vit.py"
      },
      {
        "source": "src\\transformers\\models\\vit",
        "target": "src\\transformers\\models\\vit\\convert_dino_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\vit",
        "target": "src\\transformers\\models\\vit\\convert_vit_timm_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\vit",
        "target": "src\\transformers\\models\\vit\\image_processing_vit.py"
      },
      {
        "source": "src\\transformers\\models\\vit",
        "target": "src\\transformers\\models\\vit\\image_processing_vit_fast.py"
      },
      {
        "source": "src\\transformers\\models\\vit",
        "target": "src\\transformers\\models\\vit\\modeling_vit.py"
      },
      {
        "source": "src\\transformers\\models\\vit",
        "target": "src\\transformers\\models\\vit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vitdet",
        "target": "src\\transformers\\models\\vitdet\\configuration_vitdet.py"
      },
      {
        "source": "src\\transformers\\models\\vitdet",
        "target": "src\\transformers\\models\\vitdet\\modeling_vitdet.py"
      },
      {
        "source": "src\\transformers\\models\\vitdet",
        "target": "src\\transformers\\models\\vitdet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vitmatte",
        "target": "src\\transformers\\models\\vitmatte\\configuration_vitmatte.py"
      },
      {
        "source": "src\\transformers\\models\\vitmatte",
        "target": "src\\transformers\\models\\vitmatte\\convert_vitmatte_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\vitmatte",
        "target": "src\\transformers\\models\\vitmatte\\image_processing_vitmatte.py"
      },
      {
        "source": "src\\transformers\\models\\vitmatte",
        "target": "src\\transformers\\models\\vitmatte\\image_processing_vitmatte_fast.py"
      },
      {
        "source": "src\\transformers\\models\\vitmatte",
        "target": "src\\transformers\\models\\vitmatte\\modeling_vitmatte.py"
      },
      {
        "source": "src\\transformers\\models\\vitmatte",
        "target": "src\\transformers\\models\\vitmatte\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vitpose",
        "target": "src\\transformers\\models\\vitpose\\configuration_vitpose.py"
      },
      {
        "source": "src\\transformers\\models\\vitpose",
        "target": "src\\transformers\\models\\vitpose\\convert_vitpose_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\vitpose",
        "target": "src\\transformers\\models\\vitpose\\image_processing_vitpose.py"
      },
      {
        "source": "src\\transformers\\models\\vitpose",
        "target": "src\\transformers\\models\\vitpose\\image_processing_vitpose_fast.py"
      },
      {
        "source": "src\\transformers\\models\\vitpose",
        "target": "src\\transformers\\models\\vitpose\\modeling_vitpose.py"
      },
      {
        "source": "src\\transformers\\models\\vitpose",
        "target": "src\\transformers\\models\\vitpose\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vitpose_backbone",
        "target": "src\\transformers\\models\\vitpose_backbone\\configuration_vitpose_backbone.py"
      },
      {
        "source": "src\\transformers\\models\\vitpose_backbone",
        "target": "src\\transformers\\models\\vitpose_backbone\\modeling_vitpose_backbone.py"
      },
      {
        "source": "src\\transformers\\models\\vitpose_backbone",
        "target": "src\\transformers\\models\\vitpose_backbone\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vits",
        "target": "src\\transformers\\models\\vits\\configuration_vits.py"
      },
      {
        "source": "src\\transformers\\models\\vits",
        "target": "src\\transformers\\models\\vits\\convert_original_checkpoint.py"
      },
      {
        "source": "src\\transformers\\models\\vits",
        "target": "src\\transformers\\models\\vits\\modeling_vits.py"
      },
      {
        "source": "src\\transformers\\models\\vits",
        "target": "src\\transformers\\models\\vits\\tokenization_vits.py"
      },
      {
        "source": "src\\transformers\\models\\vits",
        "target": "src\\transformers\\models\\vits\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vit_mae",
        "target": "src\\transformers\\models\\vit_mae\\configuration_vit_mae.py"
      },
      {
        "source": "src\\transformers\\models\\vit_mae",
        "target": "src\\transformers\\models\\vit_mae\\convert_vit_mae_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\vit_mae",
        "target": "src\\transformers\\models\\vit_mae\\modeling_vit_mae.py"
      },
      {
        "source": "src\\transformers\\models\\vit_mae",
        "target": "src\\transformers\\models\\vit_mae\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vit_msn",
        "target": "src\\transformers\\models\\vit_msn\\configuration_vit_msn.py"
      },
      {
        "source": "src\\transformers\\models\\vit_msn",
        "target": "src\\transformers\\models\\vit_msn\\convert_msn_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\vit_msn",
        "target": "src\\transformers\\models\\vit_msn\\modeling_vit_msn.py"
      },
      {
        "source": "src\\transformers\\models\\vit_msn",
        "target": "src\\transformers\\models\\vit_msn\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vivit",
        "target": "src\\transformers\\models\\vivit\\configuration_vivit.py"
      },
      {
        "source": "src\\transformers\\models\\vivit",
        "target": "src\\transformers\\models\\vivit\\convert_vivit_flax_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\vivit",
        "target": "src\\transformers\\models\\vivit\\image_processing_vivit.py"
      },
      {
        "source": "src\\transformers\\models\\vivit",
        "target": "src\\transformers\\models\\vivit\\modeling_vivit.py"
      },
      {
        "source": "src\\transformers\\models\\vivit",
        "target": "src\\transformers\\models\\vivit\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\vjepa2",
        "target": "src\\transformers\\models\\vjepa2\\configuration_vjepa2.py"
      },
      {
        "source": "src\\transformers\\models\\vjepa2",
        "target": "src\\transformers\\models\\vjepa2\\convert_vjepa2_classifier_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\vjepa2",
        "target": "src\\transformers\\models\\vjepa2\\convert_vjepa2_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\vjepa2",
        "target": "src\\transformers\\models\\vjepa2\\modeling_vjepa2.py"
      },
      {
        "source": "src\\transformers\\models\\vjepa2",
        "target": "src\\transformers\\models\\vjepa2\\video_processing_vjepa2.py"
      },
      {
        "source": "src\\transformers\\models\\vjepa2",
        "target": "src\\transformers\\models\\vjepa2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\voxtral",
        "target": "src\\transformers\\models\\voxtral\\configuration_voxtral.py"
      },
      {
        "source": "src\\transformers\\models\\voxtral",
        "target": "src\\transformers\\models\\voxtral\\convert_voxtral_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\voxtral",
        "target": "src\\transformers\\models\\voxtral\\modeling_voxtral.py"
      },
      {
        "source": "src\\transformers\\models\\voxtral",
        "target": "src\\transformers\\models\\voxtral\\modular_voxtral.py"
      },
      {
        "source": "src\\transformers\\models\\voxtral",
        "target": "src\\transformers\\models\\voxtral\\processing_voxtral.py"
      },
      {
        "source": "src\\transformers\\models\\voxtral",
        "target": "src\\transformers\\models\\voxtral\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2",
        "target": "src\\transformers\\models\\wav2vec2\\configuration_wav2vec2.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2",
        "target": "src\\transformers\\models\\wav2vec2\\convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2",
        "target": "src\\transformers\\models\\wav2vec2\\convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2",
        "target": "src\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2",
        "target": "src\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2",
        "target": "src\\transformers\\models\\wav2vec2\\processing_wav2vec2.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2",
        "target": "src\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2",
        "target": "src\\transformers\\models\\wav2vec2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_bert",
        "target": "src\\transformers\\models\\wav2vec2_bert\\configuration_wav2vec2_bert.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_bert",
        "target": "src\\transformers\\models\\wav2vec2_bert\\convert_wav2vec2_seamless_checkpoint.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_bert",
        "target": "src\\transformers\\models\\wav2vec2_bert\\modeling_wav2vec2_bert.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_bert",
        "target": "src\\transformers\\models\\wav2vec2_bert\\modular_wav2vec2_bert.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_bert",
        "target": "src\\transformers\\models\\wav2vec2_bert\\processing_wav2vec2_bert.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_bert",
        "target": "src\\transformers\\models\\wav2vec2_bert\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_conformer",
        "target": "src\\transformers\\models\\wav2vec2_conformer\\configuration_wav2vec2_conformer.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_conformer",
        "target": "src\\transformers\\models\\wav2vec2_conformer\\convert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_conformer",
        "target": "src\\transformers\\models\\wav2vec2_conformer\\modeling_wav2vec2_conformer.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_conformer",
        "target": "src\\transformers\\models\\wav2vec2_conformer\\modular_wav2vec2_conformer.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_conformer",
        "target": "src\\transformers\\models\\wav2vec2_conformer\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_phoneme",
        "target": "src\\transformers\\models\\wav2vec2_phoneme\\tokenization_wav2vec2_phoneme.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_phoneme",
        "target": "src\\transformers\\models\\wav2vec2_phoneme\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_with_lm",
        "target": "src\\transformers\\models\\wav2vec2_with_lm\\processing_wav2vec2_with_lm.py"
      },
      {
        "source": "src\\transformers\\models\\wav2vec2_with_lm",
        "target": "src\\transformers\\models\\wav2vec2_with_lm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\wavlm",
        "target": "src\\transformers\\models\\wavlm\\configuration_wavlm.py"
      },
      {
        "source": "src\\transformers\\models\\wavlm",
        "target": "src\\transformers\\models\\wavlm\\convert_wavlm_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\wavlm",
        "target": "src\\transformers\\models\\wavlm\\convert_wavlm_original_s3prl_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\wavlm",
        "target": "src\\transformers\\models\\wavlm\\modeling_wavlm.py"
      },
      {
        "source": "src\\transformers\\models\\wavlm",
        "target": "src\\transformers\\models\\wavlm\\modular_wavlm.py"
      },
      {
        "source": "src\\transformers\\models\\wavlm",
        "target": "src\\transformers\\models\\wavlm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\whisper",
        "target": "src\\transformers\\models\\whisper\\configuration_whisper.py"
      },
      {
        "source": "src\\transformers\\models\\whisper",
        "target": "src\\transformers\\models\\whisper\\convert_openai_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\whisper",
        "target": "src\\transformers\\models\\whisper\\english_normalizer.py"
      },
      {
        "source": "src\\transformers\\models\\whisper",
        "target": "src\\transformers\\models\\whisper\\feature_extraction_whisper.py"
      },
      {
        "source": "src\\transformers\\models\\whisper",
        "target": "src\\transformers\\models\\whisper\\generation_whisper.py"
      },
      {
        "source": "src\\transformers\\models\\whisper",
        "target": "src\\transformers\\models\\whisper\\modeling_whisper.py"
      },
      {
        "source": "src\\transformers\\models\\whisper",
        "target": "src\\transformers\\models\\whisper\\processing_whisper.py"
      },
      {
        "source": "src\\transformers\\models\\whisper",
        "target": "src\\transformers\\models\\whisper\\tokenization_whisper.py"
      },
      {
        "source": "src\\transformers\\models\\whisper",
        "target": "src\\transformers\\models\\whisper\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\xcodec",
        "target": "src\\transformers\\models\\xcodec\\configuration_xcodec.py"
      },
      {
        "source": "src\\transformers\\models\\xcodec",
        "target": "src\\transformers\\models\\xcodec\\convert_xcodec_weights_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\xcodec",
        "target": "src\\transformers\\models\\xcodec\\modeling_xcodec.py"
      },
      {
        "source": "src\\transformers\\models\\xcodec",
        "target": "src\\transformers\\models\\xcodec\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\xglm",
        "target": "src\\transformers\\models\\xglm\\configuration_xglm.py"
      },
      {
        "source": "src\\transformers\\models\\xglm",
        "target": "src\\transformers\\models\\xglm\\convert_xglm_original_ckpt_to_trfms.py"
      },
      {
        "source": "src\\transformers\\models\\xglm",
        "target": "src\\transformers\\models\\xglm\\modeling_xglm.py"
      },
      {
        "source": "src\\transformers\\models\\xglm",
        "target": "src\\transformers\\models\\xglm\\tokenization_xglm.py"
      },
      {
        "source": "src\\transformers\\models\\xglm",
        "target": "src\\transformers\\models\\xglm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\xlm",
        "target": "src\\transformers\\models\\xlm\\configuration_xlm.py"
      },
      {
        "source": "src\\transformers\\models\\xlm",
        "target": "src\\transformers\\models\\xlm\\convert_xlm_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\xlm",
        "target": "src\\transformers\\models\\xlm\\modeling_xlm.py"
      },
      {
        "source": "src\\transformers\\models\\xlm",
        "target": "src\\transformers\\models\\xlm\\tokenization_xlm.py"
      },
      {
        "source": "src\\transformers\\models\\xlm",
        "target": "src\\transformers\\models\\xlm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\xlm_roberta",
        "target": "src\\transformers\\models\\xlm_roberta\\configuration_xlm_roberta.py"
      },
      {
        "source": "src\\transformers\\models\\xlm_roberta",
        "target": "src\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py"
      },
      {
        "source": "src\\transformers\\models\\xlm_roberta",
        "target": "src\\transformers\\models\\xlm_roberta\\modular_xlm_roberta.py"
      },
      {
        "source": "src\\transformers\\models\\xlm_roberta",
        "target": "src\\transformers\\models\\xlm_roberta\\tokenization_xlm_roberta.py"
      },
      {
        "source": "src\\transformers\\models\\xlm_roberta",
        "target": "src\\transformers\\models\\xlm_roberta\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\xlm_roberta_xl",
        "target": "src\\transformers\\models\\xlm_roberta_xl\\configuration_xlm_roberta_xl.py"
      },
      {
        "source": "src\\transformers\\models\\xlm_roberta_xl",
        "target": "src\\transformers\\models\\xlm_roberta_xl\\convert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\xlm_roberta_xl",
        "target": "src\\transformers\\models\\xlm_roberta_xl\\modeling_xlm_roberta_xl.py"
      },
      {
        "source": "src\\transformers\\models\\xlm_roberta_xl",
        "target": "src\\transformers\\models\\xlm_roberta_xl\\modular_xlm_roberta_xl.py"
      },
      {
        "source": "src\\transformers\\models\\xlm_roberta_xl",
        "target": "src\\transformers\\models\\xlm_roberta_xl\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\xlnet",
        "target": "src\\transformers\\models\\xlnet\\configuration_xlnet.py"
      },
      {
        "source": "src\\transformers\\models\\xlnet",
        "target": "src\\transformers\\models\\xlnet\\convert_xlnet_original_tf_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\xlnet",
        "target": "src\\transformers\\models\\xlnet\\modeling_xlnet.py"
      },
      {
        "source": "src\\transformers\\models\\xlnet",
        "target": "src\\transformers\\models\\xlnet\\tokenization_xlnet.py"
      },
      {
        "source": "src\\transformers\\models\\xlnet",
        "target": "src\\transformers\\models\\xlnet\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\xlstm",
        "target": "src\\transformers\\models\\xlstm\\configuration_xlstm.py"
      },
      {
        "source": "src\\transformers\\models\\xlstm",
        "target": "src\\transformers\\models\\xlstm\\modeling_xlstm.py"
      },
      {
        "source": "src\\transformers\\models\\xlstm",
        "target": "src\\transformers\\models\\xlstm\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\xmod",
        "target": "src\\transformers\\models\\xmod\\configuration_xmod.py"
      },
      {
        "source": "src\\transformers\\models\\xmod",
        "target": "src\\transformers\\models\\xmod\\convert_xmod_original_pytorch_checkpoint_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\xmod",
        "target": "src\\transformers\\models\\xmod\\modeling_xmod.py"
      },
      {
        "source": "src\\transformers\\models\\xmod",
        "target": "src\\transformers\\models\\xmod\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\x_clip",
        "target": "src\\transformers\\models\\x_clip\\configuration_x_clip.py"
      },
      {
        "source": "src\\transformers\\models\\x_clip",
        "target": "src\\transformers\\models\\x_clip\\convert_x_clip_original_pytorch_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\x_clip",
        "target": "src\\transformers\\models\\x_clip\\modeling_x_clip.py"
      },
      {
        "source": "src\\transformers\\models\\x_clip",
        "target": "src\\transformers\\models\\x_clip\\processing_x_clip.py"
      },
      {
        "source": "src\\transformers\\models\\x_clip",
        "target": "src\\transformers\\models\\x_clip\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\yolos",
        "target": "src\\transformers\\models\\yolos\\configuration_yolos.py"
      },
      {
        "source": "src\\transformers\\models\\yolos",
        "target": "src\\transformers\\models\\yolos\\convert_yolos_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\yolos",
        "target": "src\\transformers\\models\\yolos\\image_processing_yolos.py"
      },
      {
        "source": "src\\transformers\\models\\yolos",
        "target": "src\\transformers\\models\\yolos\\image_processing_yolos_fast.py"
      },
      {
        "source": "src\\transformers\\models\\yolos",
        "target": "src\\transformers\\models\\yolos\\modeling_yolos.py"
      },
      {
        "source": "src\\transformers\\models\\yolos",
        "target": "src\\transformers\\models\\yolos\\modular_yolos.py"
      },
      {
        "source": "src\\transformers\\models\\yolos",
        "target": "src\\transformers\\models\\yolos\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\yoso",
        "target": "src\\transformers\\models\\yoso\\configuration_yoso.py"
      },
      {
        "source": "src\\transformers\\models\\yoso",
        "target": "src\\transformers\\models\\yoso\\convert_yoso_pytorch_to_pytorch.py"
      },
      {
        "source": "src\\transformers\\models\\yoso",
        "target": "src\\transformers\\models\\yoso\\modeling_yoso.py"
      },
      {
        "source": "src\\transformers\\models\\yoso",
        "target": "src\\transformers\\models\\yoso\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\youtu",
        "target": "src\\transformers\\models\\youtu\\configuration_youtu.py"
      },
      {
        "source": "src\\transformers\\models\\youtu",
        "target": "src\\transformers\\models\\youtu\\modeling_youtu.py"
      },
      {
        "source": "src\\transformers\\models\\youtu",
        "target": "src\\transformers\\models\\youtu\\modular_youtu.py"
      },
      {
        "source": "src\\transformers\\models\\youtu",
        "target": "src\\transformers\\models\\youtu\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\zamba",
        "target": "src\\transformers\\models\\zamba\\configuration_zamba.py"
      },
      {
        "source": "src\\transformers\\models\\zamba",
        "target": "src\\transformers\\models\\zamba\\modeling_zamba.py"
      },
      {
        "source": "src\\transformers\\models\\zamba",
        "target": "src\\transformers\\models\\zamba\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\zamba2",
        "target": "src\\transformers\\models\\zamba2\\configuration_zamba2.py"
      },
      {
        "source": "src\\transformers\\models\\zamba2",
        "target": "src\\transformers\\models\\zamba2\\modeling_zamba2.py"
      },
      {
        "source": "src\\transformers\\models\\zamba2",
        "target": "src\\transformers\\models\\zamba2\\modular_zamba2.py"
      },
      {
        "source": "src\\transformers\\models\\zamba2",
        "target": "src\\transformers\\models\\zamba2\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\zoedepth",
        "target": "src\\transformers\\models\\zoedepth\\configuration_zoedepth.py"
      },
      {
        "source": "src\\transformers\\models\\zoedepth",
        "target": "src\\transformers\\models\\zoedepth\\convert_zoedepth_to_hf.py"
      },
      {
        "source": "src\\transformers\\models\\zoedepth",
        "target": "src\\transformers\\models\\zoedepth\\image_processing_zoedepth.py"
      },
      {
        "source": "src\\transformers\\models\\zoedepth",
        "target": "src\\transformers\\models\\zoedepth\\image_processing_zoedepth_fast.py"
      },
      {
        "source": "src\\transformers\\models\\zoedepth",
        "target": "src\\transformers\\models\\zoedepth\\modeling_zoedepth.py"
      },
      {
        "source": "src\\transformers\\models\\zoedepth",
        "target": "src\\transformers\\models\\zoedepth\\__init__.py"
      },
      {
        "source": "src\\transformers\\models\\esm\\openfold_utils",
        "target": "src\\transformers\\models\\esm\\openfold_utils\\chunk_utils.py"
      },
      {
        "source": "src\\transformers\\models\\esm\\openfold_utils",
        "target": "src\\transformers\\models\\esm\\openfold_utils\\data_transforms.py"
      },
      {
        "source": "src\\transformers\\models\\esm\\openfold_utils",
        "target": "src\\transformers\\models\\esm\\openfold_utils\\feats.py"
      },
      {
        "source": "src\\transformers\\models\\esm\\openfold_utils",
        "target": "src\\transformers\\models\\esm\\openfold_utils\\loss.py"
      },
      {
        "source": "src\\transformers\\models\\esm\\openfold_utils",
        "target": "src\\transformers\\models\\esm\\openfold_utils\\protein.py"
      },
      {
        "source": "src\\transformers\\models\\esm\\openfold_utils",
        "target": "src\\transformers\\models\\esm\\openfold_utils\\residue_constants.py"
      },
      {
        "source": "src\\transformers\\models\\esm\\openfold_utils",
        "target": "src\\transformers\\models\\esm\\openfold_utils\\rigid_utils.py"
      },
      {
        "source": "src\\transformers\\models\\esm\\openfold_utils",
        "target": "src\\transformers\\models\\esm\\openfold_utils\\tensor_utils.py"
      },
      {
        "source": "src\\transformers\\models\\esm\\openfold_utils",
        "target": "src\\transformers\\models\\esm\\openfold_utils\\__init__.py"
      },
      {
        "source": "src\\transformers\\generation\\continuous_batching",
        "target": "src\\transformers\\generation\\continuous_batching\\cache.py"
      },
      {
        "source": "src\\transformers\\generation\\continuous_batching",
        "target": "src\\transformers\\generation\\continuous_batching\\cache_manager.py"
      },
      {
        "source": "src\\transformers\\generation\\continuous_batching",
        "target": "src\\transformers\\generation\\continuous_batching\\continuous_api.py"
      },
      {
        "source": "src\\transformers\\generation\\continuous_batching",
        "target": "src\\transformers\\generation\\continuous_batching\\requests.py"
      },
      {
        "source": "src\\transformers\\generation\\continuous_batching",
        "target": "src\\transformers\\generation\\continuous_batching\\scheduler.py"
      },
      {
        "source": "src\\transformers\\generation\\continuous_batching",
        "target": "src\\transformers\\generation\\continuous_batching\\__init__.py"
      },
      {
        "source": "src\\transformers\\data\\datasets",
        "target": "src\\transformers\\data\\datasets\\glue.py"
      },
      {
        "source": "src\\transformers\\data\\datasets",
        "target": "src\\transformers\\data\\datasets\\squad.py"
      },
      {
        "source": "src\\transformers\\data\\datasets",
        "target": "src\\transformers\\data\\datasets\\__init__.py"
      },
      {
        "source": "src\\transformers\\data\\metrics",
        "target": "src\\transformers\\data\\metrics\\squad_metrics.py"
      },
      {
        "source": "src\\transformers\\data\\metrics",
        "target": "src\\transformers\\data\\metrics\\__init__.py"
      },
      {
        "source": "src\\transformers\\data\\processors",
        "target": "src\\transformers\\data\\processors\\glue.py"
      },
      {
        "source": "src\\transformers\\data\\processors",
        "target": "src\\transformers\\data\\processors\\squad.py"
      },
      {
        "source": "src\\transformers\\data\\processors",
        "target": "src\\transformers\\data\\processors\\utils.py"
      },
      {
        "source": "src\\transformers\\data\\processors",
        "target": "src\\transformers\\data\\processors\\xnli.py"
      },
      {
        "source": "src\\transformers\\data\\processors",
        "target": "src\\transformers\\data\\processors\\__init__.py"
      }
    ]
  },
  "metadata": {
    "name": "directoy tree of transformers",
    "description": "Directory tree of the Python package transformers",
    "source": "inspectapy/examples/start_map.ipynb"
  }
}